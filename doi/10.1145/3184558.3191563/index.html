<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>A3embed: Attribute Association Aware Network
  Embedding</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3184558.3191563'>https://doi.org/10.1145/3184558.3191563</a> 
originally published by ACM Press, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3191563'>https://w3id.org/oa/10.1145/3184558.3191563</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title"><em>A3embed</em>: Attribute
          Association Aware Network Embedding</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Jihwan</span> <span class=
          "surName">Lee</span>, Amazon Alexa Brain &amp; Purdue
          University Seattle, Washington 98121, <a href=
          "mailto:jihwl@amazon.com">jihwl@amazon.com</a>
        </div>
        <div class="author">
          <span class="givenName">Sunil</span> <span class=
          "surName">Prabahkar</span>, Purdue University West
          Lafayette, Indiana 47906, <a href=
          "mailto:sunil@purdue.edu">sunil@purdue.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3191563"
        target=
        "_blank">https://doi.org/10.1145/3184558.3191563</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Network embedding aims to learn low-dimensional
        vector representations for nodes in a network that preserve
        structural characteristics. It has been shown that such
        representations are helpful in several graph mining tasks
        such as node classification, link prediction, and community
        detection. Some recent works have attempted to extend the
        approach to attributed networks in which each node is
        associated with a set of attribute values. They have
        focused on homophily relationships by forcing nodes with
        similar attribute values to obtain similar vector
        representations. This is unnecessarily restrictive and
        misses the opportunity to harness other types of
        relationships revealed by patterns in attribute values of
        connected nodes for learning insightful relationships. In
        this paper, we propose a new network attributed embedding
        framework called <em>A3embed</em> that is aware of
        attribute associations. <em>A3embed</em> favors significant
        attribute associations, not merely homophily relationships,
        which contributes to its robustness to diverse attribute
        vectors and noisy links. The experimental results on
        real-world datasets demonstrate that the proposed framework
        achieves better performance on different graph mining tasks
        compared to existing models.</small></p>
      </div>
      <div class="classifications">
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Jihwan Lee and Sunil Prabahkar. 2018. <em>A3embed</em>:
          Attribute Association Aware Network Embedding. In <em>WWW
          '18 Companion: The 2018 Web Conference Companion,</em>
          <em>April 23–27, 2018,</em> <em>Lyon, France. ACM, New
          York, NY, USA</em> 9 Pages. <a href=
          "https://doi.org/10.1145/3184558.3191563" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3191563</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Data mining and machine learning tasks that aim to exract
      insightful information from real-world data must increasingly
      handle complex network data. However, it is not realistic to
      apply standard machine learning models directly to network
      data because the network itself lacks fruitful feature
      representations that can provide informative patterns among
      nodes and links. To overcome this challenge, researchers have
      proposed methods for learning new network representations
      that are usually represented by low-dimensional continuous
      vectors. Such vectors in a continuous feature space represent
      nodes in a more abstract form while preserving structural
      proximities among the nodes and thus are better suitable for
      various data mining and machine learning tasks.</p>
      <p>While recently proposed network embedding algorithms show
      acceptable performance on various tasks&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0003">3</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0013">13</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0015">15</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0018">18</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0020">20</a>], they are limited to
      networks without attributes. However, an increasing number of
      real-world objects and applications are modeled with
      attributed networks and it has become increasingly important
      to analyze the attributes together with network structure.
      For example, in social networks such as Twitter and Facebook
      where user profile information is captured using attribute
      values, many users that have similar attributes are not
      connected to each other. That is, structural proximity is not
      sufficient to explain whether nodes in a network are similar
      or dissimilar. In that case, if available, node attributes
      can bring us a huge opportunity to capture the nodes’
      underlying similarity.</p>
      <p>Alternative methods taking into account node attribute
      values in network embedding have been proposed more
      recently&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>]. They basically have the same
      motivation, where nodes with similar attribute values are
      located closely in the low-dimensional embedding space. It
      seems quite reasonable because many previous works have shown
      that nodes in a network tend to establish homophily
      relationship in terms of their attributes&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0009">9</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0016">16</a>]. However,
      there actually exist more diverse relationships in real-world
      networks&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>]. Also, even though such relationships
      may not be observed as frequently as homophily relationships,
      they can be more important for understanding various dynamics
      in complex networks and can be captured by considering
      statistical significance&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>]. Unfortunately, the notion of
      attribute associations, defined as co-occurred attribute
      values between connected nodes, along with their significance
      has been ignored by existing network embedding methods
      despite its potential impact on network embedding.</p>
      <p>Consider an attribute network where each node is
      associated with its attribute values. A pattern of node
      attribute values which co-occur between connected nodes might
      be of interest because it can reveal the type of
      relationships among nodes clearly along with their structural
      proximity. As the number of attributes increases, it is
      unlikely that homophily relationships alone are dominant in
      the entire network. For example, in a social network, people
      working at <tt>Google</tt> may establish many links to
      co-workers but they may have different alma maters (e.g.
      connections between {<tt>Google</tt>, <tt>Stanford</tt>} –
      {<tt>Google</tt>, <tt>UCLA</tt>}). Moreover, if we consider
      other attributes such as <em>nationality</em> and
      <em>major</em>, one expects to observe much more diverse
      patterns of co-occurring attribute values on the connections
      among Google employees. That is, the homophily relationship
      may not be sufficient to capture underlying similarities
      among the nodes in a network. In such cases it is clear why
      it is important to consider such patterns which are called
      attribute associations and possibly significant, as well as
      attribute similarity represented by homophily relationship
      for successful attributed network embedding. Even if a
      particular attribute association is frequently observed among
      connected nodes, the frequency itself does not tell us how
      meaningful it is. Whether it is really meaningful or not
      depends more on how many nodes hold the attribute vectors
      involved with the attribute association and how many of them
      are connected to each other. The relative frequency of
      attribute association over the number of such nodes is more
      indicative of whether two nodes with the attribute
      association should be considered similar – and therefore be
      close to each other – in a low-dimensional embedding
      space.</p>
      <p>In this paper, we study the network embedding problem,
      especially for attributed networks, and propose a new
      embedding method that exploits attribute associations in
      learning low-dimensional representations. We experimentally
      evaluate our proposed method <em>A3embed</em> using two
      real-world attributed networks including BlogCatalog and
      Flickr. We compare the performance of <em>A3embed</em> with
      state-of-the-art network embedding methods&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>]. Our
      observations from the experiments demonstrate that new
      network representations learned by <em>A3embed</em> can be
      better generalized to various prediction and visualization
      tasks. Especially, <em>A3embed</em> is superior to not only
      some baselines that use only network structure but also
      others that use augmented attribute information in addition
      to the structural information for all considered tasks.</p>
      <p>We summarize the contributions of our proposed method as
      follows:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We propose a novel network
        embedding method, called <em>A3embed</em>
        (<strong>A</strong>ttribute <strong>A</strong>ssociation
        <strong>A</strong>ware network <strong>embed</strong>ding).
        The method aims to obtain new representations of nodes in
        an attributed network by jointly modeling both structural
        and attribute information in the network while capturing
        attribute associations.<br /></li>
        <li id="list2" label="•">We show why it is important to
        consider attribute associations on the task of network
        embedding.<br /></li>
        <li id="list3" label="•">We empirically demonstrate how
        successfully <em>A3embed</em> learns new network
        representations in a low-dimensional space and how
        effective the learned representations are for downstream
        machine learning tasks on different real-world attributed
        networks.<br /></li>
      </ul>
      <p>The paper is organized as follows. In
      Section&nbsp;<a class="sec" href="#sec-4">2</a>, we introduce
      previous works related to our problem and discuss how our
      problem differs from them. In Section&nbsp;<a class="sec"
      href="#sec-5">3</a>, we define the problem of network
      embedding and provide basic background concepts, and then
      introduce a novel method to solve the problem of attributed
      network embedding. We present our experimental observations
      over different network embedding methods on real-world
      datasets in Section&nbsp;<a class="sec" href="#sec-12">4</a>.
      Finally, we conclude the paper in Section&nbsp;<a class="sec"
      href="#sec-20">5</a>.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          Work</h2>
        </div>
      </header>
      <p>The data mining and machine learning communities have been
      attracted to the problem of network embedding that aims to
      learn new representations for networks due to its practical
      importance in various applications such as node
      classification, link prediction, visualization, network
      compression, and clustering. Recently, many researchers have
      developed methods to learn network representations which are
      based on the Skip-gram model&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0010">10</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>] that aims to learn
      continuous feature representations for words in a corpus.
      <em>DeepWalk</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0015">15</a>] is the first work that established
      an analogy for networks by representing a network as a
      document. While a document includes a sequence of words,
      nodes in a network do not have any ordered sequences among
      them. The idea to obtain a sequence of nodes from a network
      is to consider a set of short truncated random walks as its
      own corpus, and the nodes as its own words. Then the same
      optimization framework as one for the Skip-gram model can be
      applied to the set of node sequences obtained from repeated
      random walks. In&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>] the authors proposed a new
      algorithmic framework called <em>node2vec</em> for learning
      continuous feature representations for nodes in networks
      using a biased random walk procedure that smoothly
      interpolate between Breadth-First Search (BFS) and
      Depth-First Search (DFS). However, those models exploit only
      network structure when learning feature representations
      without taking into account any other information such as
      node attributes.</p>
      <p>In the case of citation networks, where nodes come with
      text information, such auxiliary information can be useful
      for learning richer representations. [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0022">22</a>] proposed text-associated
      DeepWalk (<em>TADW</em>) that incorporates text features of
      nodes into network representation learning under the
      framework of matrix factorization.
      <em>TriDNR</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>], a tri-party deep network
      representation model, is based on a coupled neural network
      that exploits inter-node relationships, node-content
      correlation, and node-label correspondence in a network to
      learn an optimal representation for each node in the network.
      Even though <em>TADW</em> and <em>TriDNR</em> use rich
      information in addition to network structure for learning
      network representations, the text information is inherently
      different from node attributes in that text information
      itself includes a sequence of words so as to be easily
      exploited by neural networks based on the Skip-gram
      model.</p>
      <p>While all the methods introduced above work only for
      networks without node attributes, [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0004">4</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>] exploit node attribute
      values to get richer representations for networks if node
      attribute are available. <em>LANE</em>&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>] is a semi-supervised model
      that incorporates node labels into embedding representation
      learning for attributed networks.
      <em>AANE</em>&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0004">4</a>] also learns low-dimensional
      representations based on the decomposition of attribute
      affinity and the embedding difference between connected nodes
      in a distributed way at scale. Both of the methods jointly
      model the network structure and node attributes but they are
      limited to attribute similarity. That is, nodes have a chance
      to have similar representations only when their attribute
      values are similar. In contrast, our proposed model considers
      more diverse patterns of co-occurring attribute values.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Attribute
          Association Aware Network Embedding</h2>
        </div>
      </header>
      <p>In this section, we first define the network embedding
      problem and then introduce our proposed method that learns
      network representations for attributed networks.
      Table&nbsp;<a class="tbl" href="#tab1">1</a> presents the
      notations we use throughout the paper.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Basic notations.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;">Notation</th>
              <th style="text-align:center;">Meaning</th>
            </tr>
          </thead>
          <thead></thead>
          <tbody>
            <tr>
              <td style="text-align:center;"><em>G</em> =
              (<em>V</em>, <em>E</em>, <em>X</em>)</td>
              <td style="text-align:center;">attributed
              network</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>n</em></td>
              <td style="text-align:center;">number of nodes</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>l</em></td>
              <td style="text-align:center;">number of
              attributes</td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <em>s<sub>ij</sub></em></td>
              <td style="text-align:center;">weight of edge
              <em>e<sub>ij</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>y<sub>i</sub></strong></td>
              <td style="text-align:center;">attribute embedding of
              node <em>v<sub>i</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>z<sub>i</sub></strong></td>
              <td style="text-align:center;">structural embedding
              of node <em>v<sub>i</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;">
              <strong>h<sub>i</sub></strong></td>
              <td style="text-align:center;">joint representation
              of node <em>v<sub>i</sub></em></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$W_{1}^{(k)},
              \mathbf {b_1}^{(k)}$</span></span></td>
              <td style="text-align:center;"><em>k</em>-th layer
              weights and biases in attribute modeling</td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$W_{2}^{(k)},
              \mathbf {b_2}^{(k)}$</span></span></td>
              <td style="text-align:center;"><em>k</em>-th layer
              weights and biases in structure modeling</td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$W_{3}^{(k)},
              \mathbf {b_3}^{(k)}$</span></span></td>
              <td style="text-align:center;"><em>k</em>-th layer
              weights and biases in joint modeling</td>
            </tr>
            <tr>
              <td style="text-align:center;"><em>m</em>
              <sub>1</sub>, <em>m</em> <sub>2</sub>, <em>m</em>
              <sub>3</sub></td>
              <td style="text-align:center;">number of layers for
              each modeling component</td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Problem
            Definition</h3>
          </div>
        </header>
        <p>Consider an attributed network denoted by <em>G</em> =
        (<em>V</em>, <em>E</em>, <em>X</em>) where <em>V</em> =
        {<em>v</em> <sub>1</sub>, <em>v</em> <sub>2</sub>, ⋅⋅⋅,
        <em>v<sub>n</sub></em> } is a set of <em>n</em> number of
        nodes, <span class="inline-equation"><span class=
        "tex">$E=\lbrace e_{ij}\rbrace _{i,j=1}^{n}$</span></span>
        is a set of edges, and <em>X</em> =
        {<strong>x<sub>1</sub></strong> ,
        <strong>x<sub>2</sub></strong> , ⋅⋅⋅,
        <strong>x<sub>n</sub></strong> } is a set of attribute
        vectors, each of which is associated with a node in
        <em>V</em>. The attribute vector
        <strong>x<sub>i</sub></strong> of the node
        <em>v<sub>i</sub></em> that holds <em>l</em> different
        attributes is represented by a vector of <em>l</em>
        numerical values. An edge <em>e<sub>ij</sub></em> in
        <em>E</em> can be associated with its weight
        <em>s<sub>ij</sub></em> representing how strongly two
        individual nodes are connected to each other. If
        <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em> are not
        connected by an edge, then <em>s<sub>ij</sub></em> = 0. In
        case of unweighted networks, <em>s<sub>ij</sub></em> = 1
        for all edges <em>e<sub>ij</sub></em> .</p>
        <p>Network embedding aims to learn new representations of
        nodes in a low-dimensional feature space by finding a
        mapping function <span class="inline-equation"><span class=
        "tex">$\mathcal {F}: V \mapsto \mathbb {R}^d$</span></span>
        where <em>d</em> ≪ <em>n</em> is the number of dimensions.
        Since a raw representation of nodes is too sparse, it is
        hard to observe interesting patterns, or obtain insightful
        knowledge, by applying standard machine learning models
        directly. However, the low-dimensional representations
        learned by network embedding may contain important
        underlying information over the network nodes in an
        abstract form. The key point to achieve good
        representations for attributed networks is to preserve the
        structural and attribute proximity of nodes&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0004">4</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0005">5</a>]. However,
        the sparsity of attribute space and diversity of attribute
        vectors render attribute proximity insufficient to account
        for actual similarity of nodes. In Section&nbsp;<a class=
        "sec" href="#sec-7">3.2</a>, we will introduce the notion
        of attribute association and explain why it needs to be
        considered for the network embedding task.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Attribute
            Associations</h3>
          </div>
        </header>
        <p>We first define an attribute association as follows,</p>
        <div class="definition" id="enc1">
          <label>Definition 3.1.</label>
          <p><em>Given two nodes <em>v<sub>i</sub></em> and
          <em>v<sub>j</sub></em> , the attribute association
          between them is defined as a relationship of co-occurred
          attribute values that appear in the pair of corresponding
          attribute vectors</em> <span class=
          "inline-equation"><span class="tex">$\mathbf {x_i} =
          [x_i^1, x_i^2, \cdots x_i^l]$</span></span> <em>and</em>
          <span class="inline-equation"><span class="tex">$\mathbf
          {x_j} = [x_j^1, x_j^2, \cdots x_j^l]$</span></span> .</p>
        </div>
        <p>Every pair of nodes has its attribute association, and
        thus there are as many attribute associations as the number
        of edges in <em>E</em> if all the nodes in <em>V</em> have
        distinct attribute vectors. Note that an attribute
        association of <strong>x<sub>i</sub></strong> and
        <strong>x<sub>j</sub></strong> is associated with not only
        the nodes <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em>
        but also any pairs of nodes that have the same attribute
        vectors as <strong>x<sub>i</sub></strong> and
        <strong>x<sub>j</sub></strong> . As the number of
        attributes increases in a network, the sparsity of
        attribute vectors would be higher and it is more likely for
        nodes to have diverse attribute vectors. However, even
        though two nodes have different attribute vectors, it does
        not mean necessarily that they are not similar. That is
        because it is also possible for some different attribute
        values to share similar topics or be correlated to each
        other. For example, in a social network where each
        individual is associated with their personal profile, some
        users may have <tt>google</tt>, <tt>facebook</tt>, <tt>J.P
        Morgan</tt>, <tt>Goldman Sachs</tt>, and so on for the
        attribute <em>employer</em>. In terms of their context,
        <tt>google</tt> and <tt>facebook</tt>, Internet service
        companies, are closer to each other rather than to the
        other two finance companies, and vice versa, and thus it is
        expected that users working at <tt>google</tt> (or <tt>J.P
        Morgan</tt>) are more likely to be linked with users
        working at <tt>facebook</tt> (or <tt>Goldman Sachs</tt>)
        even if they have different values for the attribute. In
        other words, dissimilarity of node attribute values may not
        neccessarily imply dissimilarity of nodes. This motivates
        us to consider various patterns of co-occured attribute
        values that are represented by attribute associations for
        network representation learning. Similarly, it is not
        always true that two nodes with exactly the same attribute
        vectors must be similar. Even though a number of nodes are
        associated with a particular attribute vector, if only a
        few of them are connected to each other, it is hard to say
        that all the nodes with the attribute vector are similar
        and should be located closely in the embedding space. Thus,
        it is important to consider statistically significant
        attribute associations for more insightful network
        analysis&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0007">7</a>]. In this paper, we do not compute
        the actual statistical significance of attribute
        associations but introduce the basic idea of jointly
        modeling node attributes and network structure for the task
        of learning network representations. That is, for a given
        attribute association of <strong>x<sub>i</sub></strong> and
        <strong>x<sub>j</sub></strong> , we say the attribute
        association between them is more
        <strong>significant</strong> than another association of
        <strong>x<sub>m</sub></strong> and
        <strong>x<sub>n</sub></strong> if the nodes with the
        association of <strong>x<sub>i</sub></strong> and
        <strong>x<sub>j</sub></strong> are more densely connected
        to each other compared to the connections among the nodes
        with <strong>x<sub>m</sub></strong> and
        <strong>x<sub>n</sub></strong> . Such nodes with more
        significant associations should be closer to each other
        than ones with less significant associations in the
        embedding space. We explain how the notion of significance
        should be considered in <a class="sec" href=
        "#sec-9">3.3.1</a>.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            <em>A3embed</em></h3>
          </div>
        </header>
        <p>We now propose a new network embedding method called
        <em>A3embed</em> using attribute associations for
        attributed networks. <em>A3embed</em> consists of two
        parts: one is for modeling attribute associations and the
        other is for modeling network structure. The idea is
        straightforward. If two nodes share similar attribute
        values and/or the attribute association between them is
        significant, then they should be close to each other in the
        low-dimensional embedding space. Likewise, if two nodes
        share many common neighbors and therefore are structurally
        similar to each other, then they should be located closely
        as well. In this way, we can preserve both attribute
        proximity and structural similarity while keeping patterns
        of significant attribute associations in the embeddings.
        The overall framework of <em>A3embed</em> is illustrated in
        Figure&nbsp;<a class="fig" href="#fig1">1</a>.</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3200000/3191563/images/www18companion-302-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">Framework of
            <em>A3embed</em>: For each node, its attribute vector
            and one-hot vector are fed into the deep model, and
            then its attribute and structural information are
            jointly modeled to predict its neighbors.</span>
          </div>
        </figure>
        <p></p>
        <section id="sec-9">
          <p><em>3.3.1 Modeling Attribute Associations.</em> We
          basically want to not only preserve the attribute
          similarity but also employ significant attribute
          associations. First of all, in order to model the
          attribute similarity among network nodes, we apply a deep
          autoencoder&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0001">1</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0017">17</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0020">20</a>] to the set of all attribute
          vectors <em>X</em>. An autoencoder neural network,
          consisting of the encoder and decoder, is an unsupervised
          learning algorithm that applies backpropagation, setting
          the target values or outputs to be equal to the inputs.
          In other words, it tries to learn an approximation to the
          identity function, so as to output <span class=
          "inline-equation"><span class="tex">$\mathbf
          {\hat{x_i}}$</span></span> that is similar
          <strong>x<sub>i</sub></strong> , by having a non-linear
          function that encodes <strong>x<sub>i</sub></strong> to
          new representations <strong>y<sub>i</sub></strong> and
          another non-linear function that reconstructs
          <span class="inline-equation"><span class="tex">$\mathbf
          {\hat{x_i}}$</span></span> from
          <strong>y<sub>i</sub></strong> . As a result, the learned
          <strong>y<sub>i</sub></strong> in the middle of the
          autoencoder can be considered as compressed and latent
          representations of <strong>x<sub>i</sub></strong> . If we
          have multiple layers for the encoder and the decoder,
          then the latent representation
          <strong>y<sub>i</sub></strong> <sup>(<em>k</em>)</sup> is
          formulated as follows:</p>
          <div class="table-responsive" id="eq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray}
              \begin{aligned} \mathbf {y_i}^{(k)} &amp; = \sigma
              (W_1^{(k)} \mathbf {y_i}^{(k-1)} + \mathbf
              {b_1}^{(k)}) \\ &amp; = \sigma (W_1^{(k)} \sigma
              (W_1^{(k-1)} \mathbf {y_i}^{(k-2)} + \mathbf
              {b_1}^{(k-1)}) + \mathbf {b_1}^{(k)}) \\ &amp; =
              \sigma (W_1^{(k)} (\cdots \sigma (W_1^{(1)} \mathbf
              {x_i} + \mathbf {b_1}^{(1)}) \cdots) + \mathbf
              {b_1}^{(k)}) \\\end{aligned}
              \end{eqnarray}</span><br />
              <span class="equation-number">(1)</span>
            </div>
          </div>where <em>σ</em> is an element-wise activation
          function such as a sigmoid function or a rectified linear
          unit. Similarly, the reconstruction <span class=
          "inline-equation"><span class="tex">$\mathbf
          {\hat{x_i}}$</span></span> that has the same shape as
          <strong>x<sub>i</sub></strong> is mapped from
          <strong>y<sub>i</sub></strong> by stacking hidden layers
          of non-linear functions on the top of
          <strong>y<sub>i</sub></strong> in the reversed shape of
          the encoder. Then the autoencoder is trained to minimize
          reconstruction errors, represented by
          <div class="table-responsive" id="Xeq1">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \mathcal
              {L}_{sim} = \mathcal {L}(\mathbf {x}, \mathbf
              {\hat{x}}) = \displaystyle \sum _{i=1}^{n} \Vert
              \mathbf {x_i} - \mathbf {\hat{x_i}} \Vert _2^2
              \end{equation}</span><br />
              <span class="equation-number">(2)</span>
            </div>
          </div>
          <p></p>
          <p>As discussed in [<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0020">20</a>], if an input vector, which is an
          attribute vector in our setting, is very sparse, then the
          autoencoder is prone to reconstruct zero values in an
          attribute vector rather than non-zero values. We avoid
          that by imposing a higher penalty to the reconstruction
          error of non-zero values than the error of zero-values as
          follows:</p>
          <div class="table-responsive" id="Xeq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \mathcal
              {L}_{sim} = \displaystyle \sum _{i=1}^{n} \Vert
              (\mathbf {x_i} - \mathbf {\hat{x_i}}) \odot \mathbf
              {t_i} \Vert _2^2 \end{equation}</span><br />
              <span class="equation-number">(3)</span>
            </div>
          </div>where <strong>t<sub>i</sub></strong> = [<em>t</em>
          <sub><em>i</em>1</sub>, <em>t</em>
          <sub><em>i</em>2</sub>, ⋅⋅⋅, <em>t<sub>il</sub></em> ]
          and <em>t<sub>ij</sub></em> = <em>τ</em> &gt; 1 for
          <em>j</em> = 1, ⋅⋅⋅, <em>l</em> if <em>v<sub>i</sub></em>
          has a value for the <em>j</em>-th attribute, otherwise
          <em>t<sub>ij</sub></em> = 1. The ⊙ operator performs an
          element-wise multiplication between two vectors.
          <p></p>
          <p>As we put attribute vectors of nodes repeatedly into
          the autoencoder, nodes with similar attribute vectors
          must have similar latent representations
          <strong>y</strong>. However, the objective function above
          is not enough to model potentially significant attribute
          associations that exist in many real-world applications
          as well.</p>
          <p>The key idea of using significant attribute
          associations is to see, given an attribute association,
          how many nodes have the attribute vectors involved with
          the association and how frequently the association is
          observed over links among the nodes relatively. That is,
          if two nodes <em>v<sub>i</sub></em> and
          <em>v<sub>j</sub></em> are associated with a particular
          attribute association and its attribute vectors
          <strong>x<sub>i</sub></strong> and
          <strong>x<sub>j</sub></strong> appear many times on other
          connected nodes as well, then we make the corresponding
          latent representations <strong>y<sub>i</sub></strong> and
          <strong>y<sub>j</sub></strong> similar, no matter if they
          have dissimilar attribute values or not. Note that the
          frequency of attribute vectors itself may not be
          important. In contrast, if there are many nodes that hold
          either <strong>x<sub>i</sub></strong> or
          <strong>x<sub>j</sub></strong> but only few of them are
          connected, then we make <strong>y<sub>i</sub></strong>
          and <strong>y<sub>j</sub></strong> away from each other.
          The following objective function takes care of attribute
          associations with the idea above:</p>
          <div class="table-responsive" id="Xeq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{equation} \mathcal
              {L}_{ass} = \displaystyle \sum _{i,j=1}^n s_{ij}
              \cdot \Vert \mathbf {y_i} - \mathbf {y_j} \Vert _2^2
              \end{equation}</span><br />
              <span class="equation-number">(4)</span>
            </div>
          </div>where <em>s<sub>ij</sub></em> is the edge weight
          between <em>v<sub>i</sub></em> and <em>v<sub>j</sub></em>
          and <em>s<sub>ij</sub></em> = <em>δ</em> &lt; 0 if there
          is no edge between <em>v<sub>i</sub></em> and
          <em>v<sub>j</sub></em> . For nodes <em>v<sub>i</sub></em>
          and <em>v<sub>j</sub></em> that are not connected, we
          give a negative penalty <em>δ</em> to their corresponding
          latent representations <strong>y<sub>i</sub></strong> and
          <strong>y<sub>j</sub></strong> such that they are not
          close. The effect of the negative penalty term <em>δ</em>
          guarantees that 1) even two nodes with the same attribute
          vectors could be apart if such attribute association is
          very rare in the network and 2) even two nodes with
          different attribute vectors could be close if other nodes
          with such vectors are connected more densely than usual.
          The choice of <em>δ</em> also controls how aggressively
          <em>A3embed</em> models attribute associations. If
          <em>δ</em> is very low, we rarely penalize attribute
          associations that appear between unconnected nodes. Thus,
          only node pairs with very significant attribute
          associations will be mapped in close proximity to each
          other in the embedding space.
          <p></p>
        </section>
        <section id="sec-10">
          <p><em>3.3.2 Modeling Structural Proximity.</em> While
          some existing works take into account preserving the
          first-order and second-order proximity
          simultaneously&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0018">18</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0020">20</a>], we focus on the second-order
          proximity, i.e., common neighbor structure. This is
          acceptable because the first-order proximity is already
          considered to some extent when modeling attribute
          associations. Of course, even though two nodes are
          directly connected, they may not have similar
          low-dimensional representations if the attribute
          association is too weak. This sounds reasonable unless a
          pair of connected nodes necessarily share common values
          on most attributes.</p>
          <p>For a given a node <em>v<sub>i</sub></em> , we use its
          one-hot vector <strong>v<sub>i</sub></strong> as an input
          to <em>A3embed</em>. It is fed-forward into a multi-layer
          perceptron and its latent representation
          <strong>z<sub>i</sub></strong> is combined with another
          latent representation <strong>y<sub>i</sub></strong> to
          produce a joint representation
          <strong>h<sub>i</sub></strong> of both network structure
          and node attribute values. Then the joint representation
          <strong>h<sub>i</sub></strong> is further fed-forward
          into following hidden layers and the final joint
          representation is used to predict neighbors of the node
          <em>v<sub>i</sub></em> . The formulations of the latent
          representations at each layer are as follows:</p>
          <div class="table-responsive" id="eq2">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray}
              \begin{aligned} \mathbf {z_i}^{(k)} &amp; = \sigma
              (W_2^{(k)} \mathbf {z_i}^{(k-1)} + \mathbf
              {b_2}^{(k)}) \\ &amp; = \sigma (W_2^{(k)} (\cdots
              \sigma (W_2^{(1)} \mathbf {v_i} + \mathbf
              {b_2}^{(1)}) \cdots) + \mathbf {b_2}^{(k)})
              \end{aligned} \end{eqnarray}</span><br />
              <span class="equation-number">(5)</span>
            </div>
          </div>
          <div class="table-responsive" id="eq3">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray}
              \begin{aligned} \mathbf {h_i}^{(0)} &amp; = [\omega
              \mathbf {y_i}, \mathbf {z_i}] \\\mathbf {h_i}^{(k)}
              &amp; = \sigma (W_3^{(k)} \mathbf {h_i}^{(k-1)} +
              \mathbf {b_3}^{(k)}) \end{aligned}
              \end{eqnarray}</span><br />
              <span class="equation-number">(6)</span>
            </div>
          </div>where <em>ω</em> is a hyperparameter that controls
          weights on the latent representation from modeling node
          attributes when constructing the first joint
          representation by concatenation.
          <p></p>
          <p>Lastly, we predict the neighbors <span class=
          "inline-equation"><span class="tex">$\mathcal
          {N}_i$</span></span> of the input node
          <em>v<sub>i</sub></em> using the final joint
          representation <strong>h<sub>i</sub></strong> . The
          output vector <span class="inline-equation"><span class=
          "tex">$\hat{\mathcal {N}_i}$</span></span> in
          <em>A3embed</em> should be close to a row or column
          vector of an adjacency matrix indicating neighbor nodes,
          and thus it is a multi-label classification task. The
          predictive probabilities for the neighbor nodes in
          <span class="inline-equation"><span class="tex">$\mathcal
          {N}_i$</span></span> are obtained independently by
          placing a vector of sigmoids. Then the output vector
          <span class="inline-equation"><span class=
          "tex">$\hat{\mathcal {N}_i}$</span></span> is computed as
          follows:</p>
          <div class="table-responsive" id="eq4">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray}
              \begin{aligned} \hat{\mathcal {N}_i} &amp; =
              [p(v_1|v_i), p(v_2|v_i), \cdots , p(v_n|v_i)] \\
              &amp; = [\frac{1}{1+e^{-\mathbf {u_1} \cdot \mathbf
              {h_i}}}, \frac{1}{1+e^{-\mathbf {u_2} \cdot \mathbf
              {h_i}}}, \cdots , \frac{1}{1+e^{-\mathbf {u_n} \cdot
              \mathbf {h_i}}}] \end{aligned}
              \end{eqnarray}</span><br />
              <span class="equation-number">(7)</span>
            </div>
          </div>where <strong>u<sub>j</sub></strong> is a column
          vector of the weight matrix between the last two layers,
          which corresponds to a contextual vector of the neighbor
          <em>v<sub>j</sub></em> . We then construct the loss
          function as:
          <div class="table-responsive" id="eq5">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray}
              \begin{aligned} \mathcal {L}_{net} &amp; =
              -\displaystyle \sum _{i=1}^n \log p({v_1, v_2, \cdots
              , v_n} | v_i) \\ &amp; = -\displaystyle \sum _{i=1}^n
              \sum _{v_j \in \mathcal {N}_i} \log p(v_j | v_i)
              \end{aligned} \end{eqnarray}</span><br />
              <span class="equation-number">(8)</span>
            </div>
          </div>
          <p></p>
          <p>Modeling jointly the network structure based on
          neighbors and the attribute information including
          attribute similarity and significant attribute
          associations, our proposed method <em>A3embed</em> is
          trained while aiming to find optimal weight parameters in
          the following final objective function:</p>
          <div class="table-responsive" id="eq6">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray}
              \mathop{arg\,min}_{f_1, f_2, f_3} \displaystyle \sum
              _{i=1}^3 \lambda _i \cdot R(f_i) + \alpha \mathcal
              {L}_{sim} + \gamma \mathcal {L}_{ass} + \mathcal
              {L}_{net} \end{eqnarray}</span><br />
              <span class="equation-number">(9)</span>
            </div>
          </div>where <em>f<sub>i</sub></em> is a set of weight
          matrices and biases for each component in the deep neural
          network framework of <em>A3embed</em>, <em>R</em> is a
          regularization function which is defined as <span class=
          "inline-equation"><span class="tex">$R(f_i) = \frac{1}{2}
          \sum _{k=1}^{m_i} \Vert W_i^{(k)}\Vert
          _F^2$</span></span> , and <em>λ<sub>i</sub></em> is a
          regularization term.
          <p></p>
        </section>
        <section id="sec-11">
          <p><em>3.3.3 Optimization.</em> Our goal is to find the
          optimal <em>f</em> <sub>1</sub>, <em>f</em> <sub>2</sub>,
          and <em>f</em> <sub>3</sub> that minimize the objective
          function formulated in Eq&nbsp;<a class="eqn" href=
          "#eq6">9</a>. We train <em>A3embed</em> using stochastic
          gradient descent. Specifically, we adopt
          RMSProp&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0019">19</a>], an adaptive learning rate
          method, to update gradients during training. We omit the
          mathematical formulation of the partial derivative for
          each of the loss function because it is
          straightforward.</p>
        </section>
      </section>
    </section>
    <section id="sec-12">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span>
          Experiments</h2>
        </div>
      </header>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Datasets</h3>
          </div>
        </header>
        <p>We evaluate our proposed method as well as competitors
        using one synthetic and three real-world attributed
        networks. Each of the networks contains a set of nodes
        forming network edges and associated attribute vectors for
        each node. Table&nbsp;<a class="tbl" href="#tab2">2</a>
        presents the statistics of the network datasets.</p>
        <p><strong>Synthetic Attributed Network</strong> We
        generated synthetic attributed networks to show the
        robustness of our model to diverse attribute associations,
        not only homophily. The networks are generated using the
        stochastic block model&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>, <a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>]. We first generate several
        disjoint connected components, each of which corresponds to
        a community representing a group of nodes with the same
        class label, where nodes in the same community are
        connected to each other with <em>p</em> probability and
        nodes are connected with <em>q</em> probability across
        different communities. Every node belonging to the same
        community shares the same attribute vector. We then adjust
        the connection probabilities and perturb attribute vectors
        to introduce non-homophily attribute associations between
        nodes in the same community as well as noisy links.
        Embeddings for such contrived attributed networks can
        reveal how a model is able to capture diverse attribute
        associations as well as underlying node similarities from
        noisy connections of network nodes. See more details in
        Section&nbsp;<a class="sec" href="#sec-17">4.5</a>.</p>
        <p><strong>BlogCatalog</strong>&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0004">4</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0005">5</a>] BlogCatalog is a
        blogging platform where users can form a network connecting
        each other. Each blog has a short description and the
        keywords in the description are considered as attributes.
        Users can assign to their blogs a category that represents
        a class label.</p>
        <p><strong>Flickr</strong>&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0004">4</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0005">5</a>] Flickr is an online
        photo management and sharing website where users can
        establish connections to others. For attributes, we use as
        attributes a set of tags that describe users’ specific
        interests in their photos. The groups to which users
        subscribe in the platform are considered as class
        labels.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Dataset Statistics.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">Name</th>
                <th style="text-align:center;">Synthetic</th>
                <th style="text-align:center;">BlogCatalog</th>
                <th style="text-align:center;">Flickr</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">No. Nodes</td>
                <td style="text-align:center;">1,024</td>
                <td style="text-align:center;">5,196</td>
                <td style="text-align:center;">7,575</td>
              </tr>
              <tr>
                <td style="text-align:left;">No. Edges</td>
                <td style="text-align:center;">varied</td>
                <td style="text-align:center;">171,743</td>
                <td style="text-align:center;">239,738</td>
              </tr>
              <tr>
                <td style="text-align:left;">No. Attributes</td>
                <td style="text-align:center;">1,000</td>
                <td style="text-align:center;">8,189</td>
                <td style="text-align:center;">12,047</td>
              </tr>
              <tr>
                <td style="text-align:left;">No. Labels</td>
                <td style="text-align:center;">varied</td>
                <td style="text-align:center;">6</td>
                <td style="text-align:center;">9</td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title">Node classification performance of
            different methods over different training-test split
            ratios on BlogCatalog.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metric</th>
                <th style="text-align:center;">Algorithm</th>
                <th style="text-align:center;">10%</th>
                <th style="text-align:center;">20%</th>
                <th style="text-align:center;">30%</th>
                <th style="text-align:center;">40%</th>
                <th style="text-align:center;">50%</th>
                <th style="text-align:center;">60%</th>
                <th style="text-align:center;">70%</th>
                <th style="text-align:center;">80%</th>
                <th style="text-align:center;">90%</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Macro <em>F</em>
                <sub>1</sub></td>
                <td style="text-align:center;">LINE</td>
                <td style="text-align:center;">0.656</td>
                <td style="text-align:center;">0.672</td>
                <td style="text-align:center;">0.686</td>
                <td style="text-align:center;">0.691</td>
                <td style="text-align:center;">0.684</td>
                <td style="text-align:center;">0.691</td>
                <td style="text-align:center;">0.679</td>
                <td style="text-align:center;">0.683</td>
                <td style="text-align:center;">0.689</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">node2vec</td>
                <td style="text-align:center;">0.514</td>
                <td style="text-align:center;">0.541</td>
                <td style="text-align:center;">0.609</td>
                <td style="text-align:center;">0.624</td>
                <td style="text-align:center;">0.635</td>
                <td style="text-align:center;">0.641</td>
                <td style="text-align:center;">0.639</td>
                <td style="text-align:center;">0.649</td>
                <td style="text-align:center;">0.655</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SDNE</td>
                <td style="text-align:center;">0.551</td>
                <td style="text-align:center;">0.617</td>
                <td style="text-align:center;">0.651</td>
                <td style="text-align:center;">0.678</td>
                <td style="text-align:center;">0.679</td>
                <td style="text-align:center;">0.693</td>
                <td style="text-align:center;">0.684</td>
                <td style="text-align:center;">0.690</td>
                <td style="text-align:center;">0.692</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AANE</td>
                <td style="text-align:center;">0.755</td>
                <td style="text-align:center;">0.858</td>
                <td style="text-align:center;">
                <strong>0.884</strong></td>
                <td style="text-align:center;">0.885</td>
                <td style="text-align:center;">0.886</td>
                <td style="text-align:center;">0.883</td>
                <td style="text-align:center;">0.876</td>
                <td style="text-align:center;">0.889</td>
                <td style="text-align:center;">0.889</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">A3embed</td>
                <td style="text-align:center;">
                <strong>0.837</strong></td>
                <td style="text-align:center;">
                <strong>0.866</strong></td>
                <td style="text-align:center;">0.881</td>
                <td style="text-align:center;">
                <strong>0.888</strong></td>
                <td style="text-align:center;">
                <strong>0.888</strong></td>
                <td style="text-align:center;">
                <strong>0.894</strong></td>
                <td style="text-align:center;">
                <strong>0.901</strong></td>
                <td style="text-align:center;">
                <strong>0.912</strong></td>
                <td style="text-align:center;">
                <strong>0.917</strong></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">Micro <em>F</em>
                <sub>1</sub></td>
                <td style="text-align:center;">LINE</td>
                <td style="text-align:center;">0.661</td>
                <td style="text-align:center;">0.676</td>
                <td style="text-align:center;">0.691</td>
                <td style="text-align:center;">0.696</td>
                <td style="text-align:center;">0.691</td>
                <td style="text-align:center;">0.697</td>
                <td style="text-align:center;">0.684</td>
                <td style="text-align:center;">0.689</td>
                <td style="text-align:center;">0.704</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">node2vec</td>
                <td style="text-align:center;">0.521</td>
                <td style="text-align:center;">0.545</td>
                <td style="text-align:center;">0.614</td>
                <td style="text-align:center;">0.631</td>
                <td style="text-align:center;">0.642</td>
                <td style="text-align:center;">0.648</td>
                <td style="text-align:center;">0.646</td>
                <td style="text-align:center;">0.657</td>
                <td style="text-align:center;">0.669</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SDNE</td>
                <td style="text-align:center;">0.556</td>
                <td style="text-align:center;">0.620</td>
                <td style="text-align:center;">0.654</td>
                <td style="text-align:center;">0.682</td>
                <td style="text-align:center;">0.686</td>
                <td style="text-align:center;">0.698</td>
                <td style="text-align:center;">0.688</td>
                <td style="text-align:center;">0.697</td>
                <td style="text-align:center;">0.702</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AANE</td>
                <td style="text-align:center;">0.783</td>
                <td style="text-align:center;">0.865</td>
                <td style="text-align:center;">
                <strong>0.889</strong></td>
                <td style="text-align:center;">0.890</td>
                <td style="text-align:center;">0.890</td>
                <td style="text-align:center;">0.887</td>
                <td style="text-align:center;">0.879</td>
                <td style="text-align:center;">0.893</td>
                <td style="text-align:center;">0.892</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">A3embed</td>
                <td style="text-align:center;">
                <strong>0.841</strong></td>
                <td style="text-align:center;">
                <strong>0.868</strong></td>
                <td style="text-align:center;">0.883</td>
                <td style="text-align:center;">
                <strong>0.891</strong></td>
                <td style="text-align:center;">
                <strong>0.891</strong></td>
                <td style="text-align:center;">
                <strong>0.897</strong></td>
                <td style="text-align:center;">
                <strong>0.902</strong></td>
                <td style="text-align:center;">
                <strong>0.913</strong></td>
                <td style="text-align:center;">
                <strong>0.915</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">Node classification performance of
            different methods over different training-test split
            ratios on Flickr.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Metric</th>
                <th style="text-align:center;">Algorithm</th>
                <th style="text-align:center;">10%</th>
                <th style="text-align:center;">20%</th>
                <th style="text-align:center;">30%</th>
                <th style="text-align:center;">40%</th>
                <th style="text-align:center;">50%</th>
                <th style="text-align:center;">60%</th>
                <th style="text-align:center;">70%</th>
                <th style="text-align:center;">80%</th>
                <th style="text-align:center;">90%</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Macro <em>F</em>
                <sub>1</sub></td>
                <td style="text-align:center;">LINE</td>
                <td style="text-align:center;">0.576</td>
                <td style="text-align:center;">0.601</td>
                <td style="text-align:center;">0.604</td>
                <td style="text-align:center;">0.610</td>
                <td style="text-align:center;">0.617</td>
                <td style="text-align:center;">0.621</td>
                <td style="text-align:center;">0.627</td>
                <td style="text-align:center;">0.626</td>
                <td style="text-align:center;">0.624</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">node2vec</td>
                <td style="text-align:center;">0.358</td>
                <td style="text-align:center;">0.438</td>
                <td style="text-align:center;">0.470</td>
                <td style="text-align:center;">0.479</td>
                <td style="text-align:center;">0.497</td>
                <td style="text-align:center;">0.508</td>
                <td style="text-align:center;">0.514</td>
                <td style="text-align:center;">0.517</td>
                <td style="text-align:center;">0.521</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SDNE</td>
                <td style="text-align:center;">0.506</td>
                <td style="text-align:center;">0.559</td>
                <td style="text-align:center;">0.582</td>
                <td style="text-align:center;">0.592</td>
                <td style="text-align:center;">0.600</td>
                <td style="text-align:center;">0.609</td>
                <td style="text-align:center;">0.609</td>
                <td style="text-align:center;">0.610</td>
                <td style="text-align:center;">0.609</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AANE</td>
                <td style="text-align:center;">0.754</td>
                <td style="text-align:center;">0.781</td>
                <td style="text-align:center;">0.818</td>
                <td style="text-align:center;">0.843</td>
                <td style="text-align:center;">0.847</td>
                <td style="text-align:center;">0.858</td>
                <td style="text-align:center;">0.861</td>
                <td style="text-align:center;">0.865</td>
                <td style="text-align:center;">0.872</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">A3embed</td>
                <td style="text-align:center;">
                <strong>0.816</strong></td>
                <td style="text-align:center;">
                <strong>0.840</strong></td>
                <td style="text-align:center;">
                <strong>0.849</strong></td>
                <td style="text-align:center;">
                <strong>0.855</strong></td>
                <td style="text-align:center;">
                <strong>0.856</strong></td>
                <td style="text-align:center;">
                <strong>0.864</strong></td>
                <td style="text-align:center;">
                <strong>0.865</strong></td>
                <td style="text-align:center;">
                <strong>0.874</strong></td>
                <td style="text-align:center;">
                <strong>0.890</strong></td>
              </tr>
              <tr style="border-top: solid 2px">
                <td style="text-align:center;">Micro <em>F</em>
                <sub>1</sub></td>
                <td style="text-align:center;">LINE</td>
                <td style="text-align:center;">0.585</td>
                <td style="text-align:center;">0.608</td>
                <td style="text-align:center;">0.611</td>
                <td style="text-align:center;">0.619</td>
                <td style="text-align:center;">0.626</td>
                <td style="text-align:center;">0.630</td>
                <td style="text-align:center;">0.638</td>
                <td style="text-align:center;">0.639</td>
                <td style="text-align:center;">0.640</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">node2vec</td>
                <td style="text-align:center;">0.363</td>
                <td style="text-align:center;">0.444</td>
                <td style="text-align:center;">0.475</td>
                <td style="text-align:center;">0.486</td>
                <td style="text-align:center;">0.507</td>
                <td style="text-align:center;">0.518</td>
                <td style="text-align:center;">0.524</td>
                <td style="text-align:center;">0.529</td>
                <td style="text-align:center;">0.533</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SDNE</td>
                <td style="text-align:center;">0.508</td>
                <td style="text-align:center;">0.562</td>
                <td style="text-align:center;">0.586</td>
                <td style="text-align:center;">0.597</td>
                <td style="text-align:center;">0.607</td>
                <td style="text-align:center;">0.617</td>
                <td style="text-align:center;">0.617</td>
                <td style="text-align:center;">0.617</td>
                <td style="text-align:center;">0.620</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AANE</td>
                <td style="text-align:center;">0.781</td>
                <td style="text-align:center;">0.806</td>
                <td style="text-align:center;">0.831</td>
                <td style="text-align:center;">0.850</td>
                <td style="text-align:center;">0.855</td>
                <td style="text-align:center;">0.863</td>
                <td style="text-align:center;">0.865</td>
                <td style="text-align:center;">0.869</td>
                <td style="text-align:center;">0.877</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">A3embed</td>
                <td style="text-align:center;">
                <strong>0.819</strong></td>
                <td style="text-align:center;">
                <strong>0.843</strong></td>
                <td style="text-align:center;">
                <strong>0.852</strong></td>
                <td style="text-align:center;">
                <strong>0.856</strong></td>
                <td style="text-align:center;">
                <strong>0.860</strong></td>
                <td style="text-align:center;">
                <strong>0.867</strong></td>
                <td style="text-align:center;">
                <strong>0.868</strong></td>
                <td style="text-align:center;">
                <strong>0.877</strong></td>
                <td style="text-align:center;">
                <strong>0.894</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-14">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Baselines</h3>
          </div>
        </header>
        <p>We evaluate the following baseline methods as well as
        <em>A3embed</em> for comparison. All the baselines were
        published recently and are known as good performers for
        network embedding. They are categorized into two groups.
        <em>node2vec</em>, <em>SDNE</em>, and <em>LINE</em> use
        only the network structure information while <em>AANE</em>
        uses both structural and attribute information. The brief
        descriptions of the baselines are as follows:</p>
        <p><em>node2vec</em>&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0003">3</a>] <em>node2vec</em>, extending
        <em>DeepWalk</em>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0015">15</a>], is one of the state-of-the art
        methods for network embedding and it uses only structural
        information. It exploits truncated random walk sequences to
        obtain context nodes for a given node while allowing
        flexibility between homophily and structural equivalence,
        and then computes node embeddings by maximizing the
        likelihood of observing context nodes.</p>
        <p><em>SDNE</em>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0020">20</a>] This method uses structural
        information only as well but focuses on the first-order and
        second-order proximity among nodes to preserve the network
        structure.</p>
        <p><em>LINE</em>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0018">18</a>] As in <em>SDNE</em>, <em>LINE</em>
        preserves the first-order and second-order proximity, but
        it does not model them jointly. They are considered
        separately to learn low-dimensional representations for
        each, and then concatenated. In our experiments, only the
        second-order proximity is used because it does not differ
        much from the concatenated representations, in terms of the
        effectiveness on downstream tasks.</p>
        <p><em>AANE</em>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>] <em>AANE</em> models and
        incorporates node attribute proximity into network
        embedding in a distributed way. It learns a low-dimensional
        representation based on the decomposition of attribute
        affinity and the embedding between connected nodes. The key
        difference from <em>A3embed</em> is that the node attribute
        information is learned in <em>A3embed</em> allowing
        implicit similarity and diverse relationships of attribute
        values whereas the node attributes have to be explicitly
        similar in <em>AANE</em>.</p>
      </section>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.3</span>
            Experimental Setup</h3>
          </div>
        </header>
        <p>All experiments were conducted on a machine with Intel
        i5-4690K 3.50GHz CPU, 32 GB memory and GTX Titan X GPU,
        running 64bit Ubuntu 14.04. <em>A3embed</em> is implemented
        using TensorFlow 1.2.1&nbsp;<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a> in Python 2.7, and for the
        implementations of all the baselines, we use the source
        code from the authors.</p>
        <p>All the methods we evaluate include various
        hyperparameters that may affect the performances of the
        methods significantly and thus need to be tuned. We
        basically seek optimal hyperparameter values through
        grid-search and run each of the baseline algorithms with
        multiple epochs until we achieve the best results. Note
        that all the notations we use in the following discussion
        are ones from the original papers for the methods. For
        <em>AANE</em>, we use the parameter values that are already
        specified in the source code written by the author for each
        dataset (<em>λ</em> ∈ {1<em>e</em> − 6, 0.0425} and
        <em>ρ</em> ∈ {4, 5}). For <em>node2vec</em>, the search
        strategy parameters <em>p</em> and <em>q</em> are set to 2
        and 0.5 respectively, and we use typical values for any
        other parameters such as the length of random walk
        (<em>l</em> = 80) and the size of network neighborhoods
        (<em>k</em> = 10). For <em>SDNE</em>, we also use
        <em>α</em> = 1, <em>β</em> = 5, <em>γ</em> = 5, and the
        shape of its autoencoder structure is the same as ones
        described in its paper. All the parameters in <em>LINE</em>
        are set as used in the paper, except that we vary the
        number of samples used for optimization to find the best
        performance. For <em>A3embed</em>, we found that the
        following parameter setting works best for both BlogCatalog
        and Flickr: <em>α</em> = 1, <em>τ</em> = 5, <em>γ</em> = 5,
        <em>δ</em> = −0.5, <em>ω</em> = 0.5, <em>λ</em>
        <sub>1</sub> = <em>λ</em> <sub>2</sub> = <em>λ</em>
        <sub>3</sub> = 1, and the learning rate is set to 0.001,
        For fair comparisons, the dimensionality of the embeddings
        is set to 200 for BlogCatalog and Flickr and 100 for
        synthetic networks for all the methods. In addition to
        <em>d</em> = 200, the impact of different embedding
        dimensions is discussed in Section&nbsp;<a class="sec"
        href="#sec-18">4.6</a>.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.4</span> Multi-Label
            Classification</h3>
          </div>
        </header>
        <p>One of the most common analytics tasks in network data
        is node classification, and so we evaluate the
        effectiveness of different network representations obtained
        from considered network embedding algorithms through a
        multi-label classification task on the real-world datasets.
        Every node in the network data is associated with one or
        more labels. Given a set of low-dimensional representations
        of the nodes generated by a network embedding algorithm, we
        randomly split them into training and test sets with varied
        ratios and train a classification model over the training
        nodes and their labels using the learned representations as
        features. Then, we see how accurately the models predict
        the labels of test nodes using Macro- and Micro-<em>F</em>
        <sub>1</sub> metrics. Here, for the classification model,
        we use a one-vs-rest support vector machine classifier
        provided by scikit-learn library&nbsp;<a class="fn" href=
        "#fn2" id="foot-fn2"><sup>2</sup></a>.</p>
        <p>Table&nbsp;<a class="tbl" href="#tab3">3</a> and
        Table&nbsp;<a class="tbl" href="#tab4">4</a> show the
        classification results of different methods on the two
        real-world attributed network, BlogCatalog and Flickr. By
        looking at the performance difference between
        structure-only based methods and joint models, it is clear
        that using attribute information, if available, is critical
        to learn better representations. Especially,
        <em>A3embed</em> almost consistently outperforms all the
        other competitive methods over different split ratios of
        training and test samples, which demonstrates the network
        representations learned from our proposed model can capture
        more meaningful underlying characteristics of the network
        nodes. Moreover, <em>A3embed</em> is very robust to even
        small training sample sizes. As we decreases the size of
        the training set, the improvement margin of
        <em>A3embed</em> over the baseline methods increases. This
        observation can tell us that our proposed method is better
        suited to many different real-world applications where only
        few nodes actually have labeles.</p>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">Node classification performance on
            synthetic attributed networks</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;"></th>
                <th colspan="2" style="text-align:center;">
                  <em>p</em> = 0.3, <em>q</em> = 0.1, <em>r</em> =
                  5
                  <hr />
                </th>
                <th colspan="2" style="text-align:center;">
                  <em>p</em> = 0.3, <em>q</em> = 0.3, <em>r</em> =
                  5
                  <hr />
                </th>
              </tr>
              <tr>
                <th style="text-align:center;">Algorithm</th>
                <th style="text-align:center;">Macro <em>F</em>
                <sub>1</sub></th>
                <th style="text-align:center;">Micro <em>F</em>
                <sub>1</sub></th>
                <th style="text-align:center;">Macro <em>F</em>
                <sub>1</sub></th>
                <th style="text-align:center;">Micro <em>F</em>
                <sub>1</sub></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">LINE</td>
                <td style="text-align:center;">0.921</td>
                <td style="text-align:center;">0.924</td>
                <td style="text-align:center;">0.088</td>
                <td style="text-align:center;">0.091</td>
              </tr>
              <tr>
                <td style="text-align:center;">node2vec</td>
                <td style="text-align:center;">0.911</td>
                <td style="text-align:center;">0.912</td>
                <td style="text-align:center;">0.061</td>
                <td style="text-align:center;">0.068</td>
              </tr>
              <tr>
                <td style="text-align:center;">SDNE</td>
                <td style="text-align:center;">0.918</td>
                <td style="text-align:center;">0.920</td>
                <td style="text-align:center;">0.083</td>
                <td style="text-align:center;">0.085</td>
              </tr>
              <tr>
                <td style="text-align:center;">AANE</td>
                <td style="text-align:center;">1.0</td>
                <td style="text-align:center;">1.0</td>
                <td style="text-align:center;">0.701</td>
                <td style="text-align:center;">0.712</td>
              </tr>
              <tr>
                <td style="text-align:center;">A3embed</td>
                <td style="text-align:center;">1.0</td>
                <td style="text-align:center;">1.0</td>
                <td style="text-align:center;">1.0</td>
                <td style="text-align:center;">1.0</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.5</span> Capturing
            Attribute Associations</h3>
          </div>
        </header>
        <p><em>A3embed</em> not only takes into account attribute
        proximity, but also diverse attribute associations between
        different attribute vectors, whereas existing methods work
        only on homophily relationship. This property implies that
        the representation learning of <em>A3embed</em> is more
        robust and generalizes to various patterns of relationships
        between nodes. In order to highlight how robust
        <em>A3embed</em> is compared to the baselines, we generate
        synthetic attributed networks in such a way that we can
        control certain properties held by network data by changing
        link probabilities and attribute values, as described in
        <a class="sec" href="#sec-13">4.1</a>. We start with a
        naive network where every node in the same community is
        assigned an identical attribute vector (<em>r</em> = 1) and
        nodes are more likely to be linked with others in the same
        community (<em>p</em> &gt; <em>q</em>). We then change
        <em>q</em> such that nodes are connected across different
        communities. We also randomly divide the nodes in each
        community into <em>r</em> disjoint subsets and perturb the
        attribute values of the nodes such that there are
        <em>r</em> different attribute vectors in the same
        community. In this way, we have diverse attribute
        associations, not only homophily relationships.</p>
        <p>Having different values of <em>p</em>, <em>q</em>, and
        <em>r</em>, we generate various synthetic attributed
        networks with ten communities and predict which communities
        the nodes in the test set belong to by using learned
        low-dimensional representations. Changing the value of
        <em>r</em> does not affect the behaviors of <em>LINE</em>,
        <em>node2vec</em>, and <em>SDNE</em> at all because it
        preserves the structural proximity only without using the
        node attributes. It is also not surprising that both
        <em>A3embed</em> and <em>AANE</em> perform the
        classification task with high accuracy if <em>r</em> is
        low, that is, diverse attribute associations are rare.
        Table&nbsp;<a class="tbl" href="#tab5">5</a> shows the
        methods’ robustness to existence of diverse attribute
        associations. When <em>p</em> = 0.3, <em>q</em> = 0.1, and
        <em>r</em> = 5, while <em>LINE</em>, <em>node2vec</em>, and
        <em>SDNE</em> lose some accuracy due to the noisy links,
        <em>A3embed</em> and <em>AANE</em> classify every node
        perfectly. If nodes in the same community are tightly
        connected with a small fraction of noisy links, then the
        structural proximity can be a strong signal for such nodes
        to stay close in the low-dimensional embedding space even
        if <em>r</em> is high. However, it does not mean
        <em>AANE</em> is able to capture diverse attribute
        associations. We discuss more details in
        Section&nbsp;<a class="sec" href="#sec-19">4.7</a>. If
        <em>p</em> = 0.3, <em>q</em> = 0.3, and <em>r</em> = 5,
        then the network structure is not helpful anymore
        (explaining poor performance of <em>LINE</em>,
        <em>node2vec</em>, and <em>SDNE</em>) and it becomes very
        important to be able to capture and model attribute
        associations. <em>A3embed</em> still achieves 100% accuracy
        but <em>AANE</em>’s performance gets worsen due to lack of
        its ability to model attribute associations.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3200000/3191563/images/www18companion-302-fig2a.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Classification performance
            of learned representation over different embedding
            dimensions.</span>
          </div>
        </figure>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3200000/3191563/images/www18companion-302-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Visualization of synthetic
            attributed networks. Color of a point indicates its
            community. (<em>p</em>: in-community link probability,
            <em>q</em>: cross-community link probability,
            <em>r</em>: number of distinct attribute vectors in a
            community).</span>
          </div>
        </figure>
        <p></p>
      </section>
      <section id="sec-18">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.6</span> Impact of
            Embedding Dimensions</h3>
          </div>
        </header>
        <p>We study how the classification performance of learned
        representations changes with respect to varying embedding
        dimensions <em>d</em> ∈ {32, 64, 128, 256}. Ideally, a
        network embedding method is expected to be able to learn
        good representations regardless of the embedding
        dimensions. Figure&nbsp;<a class="fig" href="#fig2">2</a>
        illustrates the effect of embedding dimensions on node
        classification with the BlogCatalog and Flickr datasets. We
        here report only Macro-<em>F</em> <sub>1</sub> because we
        observed Macro-<em>F</em> <sub>1</sub> and Micro-<em>F</em>
        <sub>1</sub> have almost the same trend in this experiment.
        As demonstrated in Figure&nbsp;<a class="fig" href=
        "#fig2">2</a>, <em>A3embed</em> and <em>AANE</em> work
        better as <em>d</em> increases while the other methods
        based on only network structure saturate or deteriorate
        after certain number of dimensions. Since <em>A3embed</em>
        and <em>AANE</em> use both network structure and node
        attributes for joint modeling, they have greater capacity
        to embed latent features compared to the other three.
        <em>node2vec</em> goes even worse when <em>d</em> =
        128 or 256, which implies overfitting.</p>
      </section>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.7</span>
            Visualization</h3>
          </div>
        </header>
        <p>In addition to measuring effectivenesses over different
        downstream tasks we have discussed so far, it is also very
        important to visualize a network because such
        visualizations can help us more intuitively understand how
        the network nodes are distributed and interact with each
        other. Since different network embedding methods preserve
        different properties of a network, they have different
        ability and interpretation of node visualization. We use
        the synthetic networks with different parameter settings as
        discussed in <a class="sec" href="#sec-17">4.5</a> and
        learn new representations of nodes using <em>A3embed</em>,
        <em>AANE</em>, and <em>node2vec</em>. We omit <em>SDNE</em>
        and <em>LINE</em> for the visualization task because they
        are basically not much different from <em>node2vec</em> in
        that all of them model only network structure. The learned
        representations are used as input to
        <em>t-SNE</em>&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0008">8</a>] with its default parameter
        values.</p>
        <p>Figure&nbsp;<a class="fig" href="#fig3">3</a>
        illustrates visualization of low-dimensional
        representations of various synthetic attributed networks.
        The synthetic attributed networks are built with different
        parameter settings (<em>p</em>, <em>q</em>, and <em>r</em>)
        and all of them include three communities, each of which is
        indicated by a color. When <em>p</em> = 0.3, <em>q</em> =
        0.1, and <em>r</em> = 1, all the methods produce nice
        visualization where every community is well-separated
        (Figure&nbsp;<a class="fig" href="#fig3">3</a>a, <a class=
        "fig" href="#fig3">3</a>b, and <a class="fig" href=
        "#fig3">3</a>c). However, if the number of distinct
        attribute vectors in each community increases (<em>r</em> =
        3) and thus there are diverse attribute associations,
        <em>AANE</em> fails to keep every node in a community close
        to each other (Figure&nbsp;<a class="fig" href=
        "#fig3">3</a>e). It makes <em>r</em> disjoint groups for
        each community in its visualization because <em>AANE</em>
        optimizes its objective based on only homophily
        relationship. In contrast, <em>A3embed</em> captures the
        attribute associations between even different attribute
        vectors and the nodes in the same community are better
        clustered together than <em>AANE</em>
        (Figure&nbsp;<a class="fig" href="#fig3">3</a>f).
        <em>node2vec</em> is not affected by changing <em>r</em>
        due to its inability to model node attributes
        (Figure&nbsp;<a class="fig" href="#fig3">3</a>d). If we
        have large numbers of noisy links (those that cross
        different communities), it must be critical to benefit from
        modeling node attributes. While <em>A3embed</em> and
        <em>AANE</em> can visualize the network in perfect shape
        (Figure&nbsp;<a class="fig" href="#fig3">3</a>h and
        <a class="fig" href="#fig3">3</a>i), <em>node2vec</em>
        fails to visualize the nodes correctly due to the absence
        of any clues to differentiate the in-community and
        cross-community links in the network (Figure&nbsp;<a class=
        "fig" href="#fig3">3</a>g).</p>
      </section>
    </section>
    <section id="sec-20">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Conclusion</h2>
        </div>
      </header>
      <p>In this paper, we propose a novel network embedding
      method, called <em>A3embed</em>, for attributed networks.
      <em>A3embed</em> learns new network representations by
      jointly exploiting network structural information and node
      attribute values. While preserving the network structure, it
      also uses various attribute associations, not limited to
      homophily relationships, among nodes. The existence of
      non-homophily but significant attribute associations in
      networks can play an important role for finding
      well-represented embeddings. The experiments are conducted on
      two real-world attributed networks to demonstrate the
      effectiveness of <em>A3embed</em> in some downstream tasks
      such as multi-label classification and visualization. We also
      use synthetic attributed networks to show how well
      <em>A3embed</em> is able to capture diverse attribute
      associations. The experimental results show that our proposed
      method outperforms other network embedding methods in
      different downstream machine learning tasks, which confirms
      the importance of using attribute associations in
      representation learning.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Yoshua Bengio, Aaron
        Courville, and Pascal Vincent. 2013. Representation
        learning: A review and new perspectives. <em><em>IEEE
        transactions on pattern analysis and machine
        intelligence</em></em> 35, 8(2013), 1798–1828.</li>
        <li id="BibPLXBIB0002" label="[2]">Shaosheng Cao, Wei Lu,
        and Qiongkai Xu. 2016. Deep Neural Networks for Learning
        Graph Representations.. In <em><em>AAAI</em></em> .
        1145–1152.</li>
        <li id="BibPLXBIB0003" label="[3]">Aditya Grover and Jure
        Leskovec. 2016. node2vec: Scalable feature learning for
        networks. In <em><em>Proceedings of the 22nd ACM SIGKDD
        international conference on Knowledge discovery and data
        mining</em></em> . ACM, 855–864.</li>
        <li id="BibPLXBIB0004" label="[4]">Xiao Huang, Jundong Li,
        and Xia Hu. 2017. Accelerated attributed network embedding.
        In <em><em>Proceedings of the 2017 SIAM International
        Conference on Data Mining</em></em> . SIAM, 633–641.</li>
        <li id="BibPLXBIB0005" label="[5]">Xiao Huang, Jundong Li,
        and Xia Hu. 2017. Label informed attributed network
        embedding. In <em><em>Proceedings of the Tenth ACM
        International Conference on Web Search and Data
        Mining</em></em> . ACM, 731–739.</li>
        <li id="BibPLXBIB0006" label="[6]">Myunghwan Kim and Jure
        Leskovec. 2012. Multiplicative attribute graph model of
        real-world networks. <em><em>Internet Mathematics</em></em>
        8, 1-2 (2012), 113–160.</li>
        <li id="BibPLXBIB0007" label="[7]">Jihwan Lee, Keehwan
        Park, and Sunil Prabhakar. 2016. Mining Statistically
        Significant Attribute Associations in Attributed Graphs. In
        <em><em>IEEE 16th International Conference on Data Mining
        (ICDM)</em></em> . IEEE, 991–996.</li>
        <li id="BibPLXBIB0008" label="[8]">Laurens van&nbsp;der
        Maaten and Geoffrey Hinton. 2008. Visualizing data using
        t-SNE. <em><em>Journal of Machine Learning
        Research</em></em> 9, Nov (2008), 2579–2605.</li>
        <li id="BibPLXBIB0009" label="[9]">Miller McPherson, Lynn
        Smith-Lovin, and James&nbsp;M Cook. 2001. Birds of a
        feather: Homophily in social networks. <em><em>Annual
        review of sociology</em></em> 27, 1 (2001), 415–444.</li>
        <li id="BibPLXBIB0010" label="[10]">Tomas Mikolov, Kai
        Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient
        estimation of word representations in vector space.
        <em><em>arXiv preprint arXiv:1301.3781</em></em>
        (2013).</li>
        <li id="BibPLXBIB0011" label="[11]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Greg&nbsp;S Corrado, and Jeff Dean.
        2013. Distributed representations of words and phrases and
        their compositionality. In <em><em>Advances in neural
        information processing systems</em></em> . 3111–3119.</li>
        <li id="BibPLXBIB0012" label="[12]">Krzysztof Nowicki and
        Tom A&nbsp;B Snijders. 2001. Estimation and prediction for
        stochastic blockstructures. <em><em>J. Amer. Statist.
        Assoc.</em></em> 96, 455 (2001), 1077–1087.</li>
        <li id="BibPLXBIB0013" label="[13]">Shirui Pan, Jia Wu,
        Xingquan Zhu, Chengqi Zhang, and Yang Wang. 2016. Tri-party
        Deep Network Representation. In <em><em>Proceedings of the
        Twenty-Fifth International Joint Conference on Artificial
        Intelligence</em></em> (<em>IJCAI’16</em>). AAAI Press,
        1895–1901. <a class="link-inline force-break" href=
        "http://dl.acm.org/citation.cfm?id=3060832.3060886"
          target="_blank">http://dl.acm.org/citation.cfm?id=3060832.3060886</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Shirui Pan, Jia Wu,
        Xingquan Zhu, Chengqi Zhang, and Yang Wang. 2016. Tri-Party
        Deep Network Representation. In <em><em>Proceedings of the
        Twenty-Fifth International Joint Conference on Artificial
        Intelligence, IJCAI 2016, New York, NY, USA, 9-15
        July2016</em></em> . 1895–1901. <a class=
        "link-inline force-break" href=
        "http://www.ijcai.org/Abstract/16/271" target=
        "_blank">http://www.ijcai.org/Abstract/16/271</a>
        </li>
        <li id="BibPLXBIB0015" label="[15]">Bryan Perozzi, Rami
        Al-Rfou, and Steven Skiena. 2014. Deepwalk: Online learning
        of social representations. In <em><em>Proceedings of the
        20th ACM SIGKDD international conference on Knowledge
        discovery and data mining</em></em> . ACM, 701–710.</li>
        <li id="BibPLXBIB0016" label="[16]">Everett&nbsp;M Rogers
        and Dilip&nbsp;K Bhowmik. 1970. Homophily-heterophily:
        Relational concepts for communication research.
        <em><em>Public opinion quarterly</em></em> 34, 4 (1970),
        523–538.</li>
        <li id="BibPLXBIB0017" label="[17]">Ruslan Salakhutdinov
        and Geoffrey Hinton. 2009. Semantic hashing.
        <em><em>International Journal of Approximate
        Reasoning</em></em> 50, 7(2009), 969–978.</li>
        <li id="BibPLXBIB0018" label="[18]">Jian Tang, Meng Qu,
        Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015.
        Line: Large-scale information network embedding. In
        <em><em>Proceedings of the 24th International Conference on
        World Wide Web</em></em> . International World Wide Web
        Conferences Steering Committee, 1067–1077.</li>
        <li id="BibPLXBIB0019" label="[19]">T. Tieleman and G.
        Hinton. 2012. Lecture 6.5—RmsProp: Divide the gradient by a
        running average of its recent magnitude. COURSERA: Neural
        Networks for Machine Learning. (2012).</li>
        <li id="BibPLXBIB0020" label="[20]">Daixin Wang, Peng Cui,
        and Wenwu Zhu. 2016. Structural deep network embedding. In
        <em><em>Proceedings of the 22nd ACM SIGKDD international
        conference on Knowledge discovery and data mining</em></em>
        . ACM, 1225–1234.</li>
        <li id="BibPLXBIB0021" label="[21]">Yuchung&nbsp;J Wang and
        George&nbsp;Y Wong. 1987. Stochastic blockmodels for
        directed graphs. <em><em>J. Amer. Statist. Assoc.</em></em>
        82, 397 (1987), 8–19.</li>
        <li id="BibPLXBIB0022" label="[22]">Cheng Yang, Zhiyuan
        Liu, Deli Zhao, Maosong Sun, and Edward&nbsp;Y Chang. 2015.
        Network representation learning with rich text information.
        In <em><em>Proceedings of the 24th International Joint
        Conference on Artificial Intelligence, Buenos Aires,
        Argentina</em></em> . 2111–2117.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.tensorflow.org">https://www.tensorflow.org</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a><a class=
    "link-inline force-break" href=
    "https://scikit-learn.org/">https://scikit-learn.org/</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3191563">https://doi.org/10.1145/3184558.3191563</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
