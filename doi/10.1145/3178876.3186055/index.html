<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Did You Really Just Have a Heart Attack? Towards Robust
  Detection of Personal Health Mentions in Social Media</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186055'>https://doi.org/10.1145/3178876.3186055</a> 
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186055'>https://w3id.org/oa/10.1145/3178876.3186055</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Did You Really Just Have a Heart
          Attack? Towards Robust Detection of Personal Health
          Mentions in Social Media</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Payam</span> <span class=
          "surName">Karisani</span>, Emory University, <a href=
          "mailto:payam.karisani@emory.edu">payam.karisani@emory.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Eugene</span> <span class=
          "surName">Agichtein</span>, Emory University, <a href=
          "mailto:eugene.agichtein@emory.edu">eugene.agichtein@emory.edu</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186055"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186055</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Millions of users share their experiences on
        social media sites, such as Twitter, which in turn generate
        valuable data for public health monitoring, digital
        epidemiology, and other analyses of population health at
        global scale. The first, critical, task for these
        applications is classifying whether a personal health event
        was mentioned, which we call the (<em>PHM</em>) problem.
        This task is challenging for many reasons, including
        typically short length of social media posts, inventive
        spelling and lexicons, and figurative language, including
        hyperbole using diseases like “heart attack” or “cancer”
        for emphasis, and not as a health self-report. This problem
        is even more challenging for rarely reported, or frequent
        but ambiguously expressed conditions, such as “stroke”. To
        address this problem, we propose a general, robust method
        for detecting PHMs in social media, which we call
        <em>WESPAD</em> , that combines lexical, syntactic, word
        embedding-based, and context-based features.
        <em>WESPAD</em> is able to generalize from few examples by
        automatically distorting the word embedding space to most
        effectively detect the true health mentions. Unlike
        previously proposed state-of-the-art supervised and
        deep-learning techniques, <em>WESPAD</em> requires
        relatively little training data, which makes it possible to
        adapt, with minimal effort, to each new disease and
        condition. We evaluate <em>WESPAD</em> on both an
        established publicly available Flu detection benchmark, and
        on a new dataset that we have constructed with mentions of
        multiple health conditions. Our experiments show that
        <em>WESPAD</em> outperforms the baselines and
        state-of-the-art methods, especially in cases when the
        number and proportion of true health mentions in the
        training data is small.</small></p>
      </div>
      <div class="author">
        <span style=
        "font-weight:bold;"><small>Keywords:</small></span>
        <span class="keyword"><small>Social media classification;
        Health tracking in social media; Representation learning
        for text classification.</small></span>
      </div><br />
      <div class="AcmReferenceFormat">
        <p><small><span style="font-weight:bold;">ACM Reference
        Format:</span><br />
        Payam Karisani and Eugene Agichtein. 2018. Did You Really
        Just Have a Heart Attack? Towards Robust Detection of
        Personal Health Mentions in Social Media. In
        <em>Proceedings of The 2018 Web Conference (WWW 2018).</em>
        ACM, New York, NY, USA, 10 pages. <a href=
        "https://doi.org/10.1145/3178876.3186055" class=
        "link-inline force-break" target=
        "_blank">https://doi.org/10.1145/3178876.3186055</a></small></p>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Individuals and organizations increasingly rely on social
      data, created on platforms such as Twitter or Facebook, for
      sharing information or communicating with others. Large
      volumes of this data have been available for research,
      opening new opportunities to answer questions about society,
      language, human behavior, and health. Among these, monitoring
      and analyzing social data for public health has been an
      active area of research, due to both the importance of the
      topic, and to the unprecedented opportunities afforded by a
      real-time window into the self-reported experience of
      millions of people online. These social data come with many
      challenges and potential biases[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>]. Nevertheless, these data already
      enabled many public health applications, such as tracking the
      spread of influenza[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>], understanding suicide
      ideation[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0010">10</a>],
      monitoring and providing support during humanitarian
      crises[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0017">17</a>],
      drug use [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0013">13</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0035">35</a>], drinking
      problems [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0024">24</a>],
      and public reactions to vaccination&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0037">37</a>].</p>
      <p>The main advantages of social data over traditional
      methods of public health surveillance such as phone surveys,
      in-person interviews, and clinical reports, include
      scalability and potential near-real time responsiveness.
      Therefore, social data has become a valuable source to
      monitor and analyze people's reports and reactions to
      health-related incidents. A crucial first step in disease
      analysis and surveillance using social data, is to identify
      whether a user post is actually mentioning a specific person
      reporting a health event. All subsequent processing and
      analysis, whether it is epidemic detection (e.g., mentioning
      an affected person in the post), or individual analysis
      (e.g., reporting one's own health condition), depends on the
      accuracy of the detection and categorization of the
      individual postings. If the posting were mis-categorized, and
      did not in fact report a health-related event, all subsequent
      analysis and conclusions arising from the data might be
      flawed.</p>
      <p>Our goal is to accurately identify postings in social
      data, which not only contain a specific disease or condition,
      but also mention a person who is affected. For instance, we
      aim to identify posts such as: <em>”My grandpa has
      Alzheimer's &amp; he keeps singing songs about wanting to
      forget”</em> or <em>”Yo Anti-Smoking group that advertises on
      twitch, I don't smoke. My mom died to lung cancer thanks to
      smoking for like 40 years. I get it.”</em>. In contrast, we
      wish to filter out postings like: <em>”I almost had a heart
      attack when I found out they're doing a lettering workshop at
      @heathceramics in SF”</em> or <em>”Dani seems like a cancer,
      spreads herself anywhere for attention!”</em>. In terms of
      previous work, we aim to identify specific health reports,
      rather than non-relevant postings or postings expressing
      general concern or awareness of a disease or condition
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0022">22</a>]. We call
      this task detecting Personal Health Mentions, or <em>PHM</em>
      . Further, we aim to develop a solution that is both robust
      and general, so that it can scale to many diseases or
      conditions of current or future interest. In turn, more
      accurately detecting personal health mentions in social data,
      without requiring extensive disease-specific development and
      tuning, would empower public health researchers, digital
      epidemiologists and computational social scientists to ask
      new questions, and to answer them with higher confidence.</p>
      <p>Detecting health mentions in social data is a challenging
      task. Social data posts, such as those on Twitter, tend to be
      short, and are often written informally, using diverse
      dialects, and inventive and specialized lexicons. Previous
      efforts for similar tasks applied machine learning methods
      that relied on extensive feature engineering, or on external
      feature augmentation to address the sparsity in the feature
      space, e.g., for company name detection [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0040">40</a>], reputation measurement
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0005">5</a>], sarcasm
      detection [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0003">3</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>], and for
      public health [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>]. In the health context, the problem
      is exacerbated by the limited availability of training data,
      and by the low frequency of the health reports in even
      keyword-based samples: on Twitter, our experiments show that
      of the tweets containing a disease name keyword, only 19% are
      actual health reports. The resulting classifiers tend to have
      high precision, but relatively low recall (i.e., high false
      negative rate), which may not be desirable for applications
      such as disease surveillance or detecting epidemics.</p>
      <p>Our goal is to address the problems of sparsity and
      imbalanced training data for <em>PHM</em> detection, by
      explicitly modeling the differences in the distribution of
      the training examples in the word embeddings space. For this,
      we introduce a novel social text data classification method,
      <em>WESPAD</em> (<strong>W</strong>ord
      <strong>E</strong>mbedding <strong>S</strong>pace
      <strong>Pa</strong>rtitioning and
      <strong>D</strong>istortion), which learns to
      <em>partition</em> the word embeddings space to more
      effectively generalize from few training examples, and to
      <em>distort</em> the embeddings space to more effectively
      separate examples of true health mentions from the rest.
      While deep neural networks have been proposed for this
      purpose, our method works well even with small amounts of
      training data, and is more simple and intuitive to tune for
      each new task, as we show empirically in this paper. We
      emphasize that <em>WESPAD</em> requires no topic or
      disease-specific feature engineering, and fewer training
      examples than previously reported methods, while more
      accurately detecting true health mentions. These properties
      make <em>WESPAD</em> particularly valuable for extending
      public health monitoring to a wider range of diseases and
      conditions. Specifically, our contributions are:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">We propose a novel, general
        approach to discover domain-specific signals in word
        embeddings to overcome the challenges inherent in the
        <em>PHM</em> problem.<br /></li>
        <li id="list2" label="•">We implement our approach as an
        effective <em>PHM</em> classification method, which
        outperforms the state-of-the-art classifiers in the
        majority of settings.<br /></li>
        <li id="list3" label="•">To validate our approach, we have
        constructed and released a manually-annotated dataset of
        Twitter posts for six prominent diseases and
        conditions<a class="fn" href="#fn1" id=
        "foot-fn1"><sup>1</sup></a>.<br />
        </li>
      </ul>
      <p>Next, we review related work to place our contributions in
      context.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Related
          work</h2>
        </div>
      </header>
      <p>Social network data and user-contributed posts on
      platforms such as Facebook and Twitter, have been extensively
      studied for diverse applications in business, politics,
      science, and public health. Some prominent examples include
      work on answering social science questions [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>], and analyzing influenza
      epidemics [<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0007">7</a>].
      Our work builds on three general directions in this area:
      general classification techniques that serve as the
      foundation of our work; disease-specific classifiers for
      social text data; and, closest to our work, prior research on
      general health classifiers that could be potentially applied
      to different diseases and conditions.</p>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Text
            Classification: Models and Techniques</h3>
          </div>
        </header>
        <p>Methods for automatic text classification have been
        studied for decades, and have evolved from simple
        bag-of-words models to sophisticated algorithms
        incorporating lexical, syntactic, and semantic information
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0001">1</a>]. Many of
        these algorithms have been adapted for biomedical text
        processing, with varying success [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0011">11</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0033">33</a>]. Recently, deep neural
        networks have emerged in many areas of natural language
        processing as an alternative to feature engineering and
        demonstrating new state-of-the-art performance on a wide
        range of tasks. However, there are two main challenges in
        making these models effective. First, it is commonly known
        that deep neural models usually need a large amount of
        training data to reach their ultimate capacity. This is an
        active area of research, and workshops such as Limited
        Labeled Data (LLD)<a class="fn" href="#fn2" id=
        "foot-fn2"><sup>2</sup></a> are held to investigate this
        area. Second, it is not clear how to incorporate domain
        knowledge into the training–e.g., social network topology,
        or user activities. Nevertheless, we include three
        state-of-the-art deep neural network models as baselines,
        chosen as representative of the most effective neural
        network methods reported for text classification. We show
        empirically that our proposed method performs better than
        these techniques, especially in the settings with small
        amounts of available training data.</p>
        <p>To improve the generalization of classification to
        unseen textual cases, <em>word embeddings</em> [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0028">28</a>] have
        been proposed as a semantic, and broader, representation of
        text. This idea has been used, for example, to compare two
        sentences, in addition to lexical features. For instance,
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0004">4</a>] proposed
        a system for detecting semantic similarity between two
        pieces of texts using word embeddings similarity.
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0019">19</a>] proposed
        an algorithm similar to [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0004">4</a>], for paraphrase detection task.
        Their main contribution is that the algorithm can capture
        the similarity between two sentences with higher details
        through binning the word similarity values. For another
        task, sarcasm detection, reference&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0018">18</a>] evaluated a number of
        word embeddings features to discover word incongruity, by
        measuring the similarities between the word vectors in the
        sentence. While these studies have been done for different
        domains and tasks, they all share the same idea of using
        word embeddings space as a resource to extract features; we
        also build on this general idea for the <em>PHM</em>
        detection problem. Reference&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0044">44</a>] proposed the idea of
        clusters of word vectors to address the problems of word
        ambiguity. The clusters are used to generate compound
        embeddings features. In our experiments we observed that
        word clusters do not accurately characterize social data
        texts when used directly. Instead, we propose
        <em>partitioning</em> the word embeddings space to generate
        features describing the distribution of training examples
        in the different regions of the space. The resulting
        distributions are subsequently used to map the instances of
        each class to different categories, which, as we show
        allows <em>WESPAD</em> to more precisely identify true
        instances of <em>PHM</em> .</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Text
            Classification for Health</h3>
          </div>
        </header>
        <p><strong>Disease-specific text classifiers</strong>:
        Since building a large training set for public health
        monitoring is costly, and in some cases impossible (e.g.,
        for rarely reported diseases); it has been shown that
        domain knowledge in the form of rule-based, or
        domain-specific classifiers is effective in monitoring
        certain diseases, e.g., [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>]. A large body of work has been
        done for detecting and tracking information about specific
        diseases. This includes investigations on tracking the
        spread of flu epidemics [<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>], cancer analysis [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0030">30</a>], asthma prediction
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0012">12</a>],
        depression prediction [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0042">42</a>], and anorexia characterization
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0008">8</a>]. To
        improve accuracy for each of these domains, studies such as
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>] have
        shown that certain aspects of tweets are also good
        indicators of health reports, and have been successfully
        operationalized as lexical and syntactic features, which we
        incorporate into our baseline system. A thorough overview
        of the published papers can be found in references
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0006">6</a>, <a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0033">33</a>]. Our
        work can be potentially used to improve the accuracy of
        health mention detection for <em>all</em> of the mentioned
        disease specific studies. We emphasize that an advantage of
        our model, introduced in the next section, is that without
        imposing any restriction on the original features, our
        method can substantially improve the detection accuracy,
        even when there is only a small set of positive examples
        available.</p>
        <p><strong>General-purpose text classification for
        Health</strong>A more attractive strategy than developing
        disease-specific classifiers, is to develop a single
        classification algorithm, that could be easily adapted to
        detect mentions of different diseases and conditions. This
        is the direction we chose in this work. Reference
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0032">32</a>] reports
        using an LDA topic-based model, which also incorporates
        domain knowledge, to discover the symptoms and their
        associated ailments in Twitter. [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0036">36</a>] proposed a two step
        process, which is representative of a common methodology,
        to detect the health mentions in social text data. The
        first, high-recall step is to collect the tweets using
        keywords and regular expressions, and the second step is to
        use a high-precision classifier – in this case, by using a
        correlation-based feature extraction method.
        Reference&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0043">43</a>] reported using a dataset of tweets
        across 34 health topics, and investigated the accuracy of
        the classifiers trained over multiple diseases and tested
        on new diseases. The authors conclude that training a
        classifier on four diseases: cancer, depression,
        hypertension, and leukemia can lead to a general health
        classifier with 77% accuracy using standard SVM classifiers
        and bag-of-words features, similar to one of our baselines
        used in empirical evaluation, which, as we will show, is
        not able to generalize well to unseen test data. We
        emphasize that our aim is also to develop a general health
        mention detection model that could apply to a variety of
        diseases and conditions.</p>
        <p>In summary, to our knowledge, our <em>WESPAD</em> model
        (presented next) is the first <em>general</em> health
        report detection method that requires only small amounts of
        training data, does not do any domain-specific feature
        engineering, yet performs as well as, and often better
        than, other methods, including a disease-specific
        rule-based classifier.</p>
      </section>
    </section>
    <section id="sec-7">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> <em>WESPAD</em>
          Model Description</h2>
        </div>
      </header>
      <p>This section introduces our method, <em>WESPAD</em> , for
      robust classification of health mentions in social data. We
      first summarize previously proposed lexical and syntactic
      features for social media classification, used both for
      health mentions, and other domains, which we use as starting
      point for our method. We then introduce the novel steps of
      our work, for learning topic-specific representation of the
      data derived from word embeddings (Sections&nbsp;<a class=
      "sec" href="#sec-10">3.3</a> and <a class="sec" href=
      "#sec-11">3.4</a>).</p>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> Lexical and
            Syntactical Features</h3>
          </div>
        </header>
        <p>Previous studies on analyzing social media (primarily
        Twitter and Facebook posts) for depression prediction
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0009">9</a>],
        influenza tracking [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>], and tobacco use [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0035">35</a>], have shown that
        certain words and phrases are key indicators of the health
        reports. Therefore, we use all word unigrams and bigrams as
        features, in order to capture any words or phrases that may
        be salient.</p>
        <p>Additionally, to model syntactic dependencies in the
        text, we use the approach proposed in [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0026">26</a>] to identify common
        syntactical dependencies in the tweets through detecting
        the frequent syntactical subtrees, and use them as
        features. To detect the sentences in the posts, we used the
        tweet dependency trees [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0021">21</a>] to detect sentence boundaries. We
        conjecture that even with small amount of training data,
        the frequent subtrees can automatically detect a subset of
        syntactic patterns which are usually designed manually for
        certain health cases (such as those of flu detection in
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>]). Our
        experiments (in Section <a class="sec" href=
        "#sec-18">6.1</a>) show that lexical and syntactic features
        provide high precision for health mention detection, but
        are not sufficient to generalize from the (relatively
        small) amounts of training data. To improve generalization,
        we now describe our use of word embeddings, which allows
        learning from a few positive examples of health mentions.
        In the next sections, we use word <em>lex_feats</em> to
        refer to the bigrams, and use word <em>syn_feats</em> to
        refer to the features extracted from the frequent
        subtrees.</p>
      </section>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Detecting
            “Noisy” Regions in the Word Embeddings Space</h3>
          </div>
        </header>
        <p>Word embeddings [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0028">28</a>] is an approach to map words to
        linguistic concepts in a lower-dimensional vector space.
        The motivation for using word embeddings to address
        sparsity in our task is that it could help match and
        generalize training examples to unseen examples at test
        time, which may not share the same words but have
        semantically related meaning. A common way to represent a
        short piece of text in the word embeddings space is to
        average of the constituent word vectors, and use the
        centroid of the vectors directly as features for a
        classifier. Although this approach has some drawbacks,
        e.g., losing information about individual words, several
        studies have shown that it can be effective [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0004">4</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0019">19</a>, <a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0039">39</a>]. Here we explore the
        ways that the centroid representation can be extended to
        improve classifier generalization to unseen cases.</p>
        <p>One could incorporate the classifier output (predicted
        class) alongside other features in the final feature
        vector. However, as we will show, incorporating the
        classifier output, as is, would propagate the false
        positive matches in the word embeddings space to generate
        noisy features for the final classifier. Another problem
        with the approach above is that a centroid in the word
        embeddings space does not preserve information about the
        constituent words, and thus likely to map both positive and
        negative examples of a health mention (if they share common
        words) into similar vectors in the word embeddings space.
        However, we will use the centroids as a starting point, in
        combination with a classifier trained over the centroid
        features, to detect, and downweight, the regions in the
        word embeddings space, where the positive and negative
        training examples have such a similar centroid vectors that
        they are no longer distinguishable.</p>
        <p><strong>Definition: “Noisy” regions in the word
        embeddings space</strong>: We define “noisy” regions as
        those, where the precision of a centroid-based classifier
        is lower than a certain threshold <em>α</em>.</p>
        <p>Using this definition, we can now <em>filter</em> out
        the noisy regions (and the corresponding features from
        training data). Detecting the regions which are
        <em>not</em> noisy also can help us to address the
        challenge of an imbalanced training set. Examples with
        centroids mapped to these regions can be directly used in
        the model to predict the label of the instances which are
        not present in the training set. This can lead to a model
        which can generalize better with only a small set of
        positive cases. To detect the noisy regions in the
        embeddings space we define a probabilistic function
        <em>Pr</em> to be the probability of assigning the tweet
        centroid to the positive class. The values associated with
        <em>Pr</em> can be extracted from the training set using
        the logistic regression model as the centroid-based
        classifier. Given the function <em>Pr</em> and the
        associated values, the probability of assigning tweet
        <em>t<sub>i</sub></em> to the positive class would be
        <em>Pr</em>(<em>t<sub>i</sub></em> ). Based on the
        definition, in the noisy regions the value of <em>Pr</em>
        is close to 0.5. More formally, we define binary features
        <em>PFlag</em>(<em>t<sub>i</sub></em> ) and
        <em>NFlag</em>(<em>t<sub>i</sub></em> ) for tweet
        <em>t<sub>i</sub></em> as follows:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} PFlag(t_i) =
            {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}1 &amp;
            0.5 + \alpha \le Pr(t_i)\\ 0 &amp; Otherwise
            \end{array}\right.}\end{align}</span><br />
          </div>
        </div>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} NFlag(t_i) =
            {\left\lbrace \begin{array}{@{}l@{\quad }l@{}}1 &amp;
            0.5 - \alpha \ge Pr(t_i)\\ 0 &amp; Otherwise
            \end{array}\right.}\end{align}</span><br />
          </div>
        </div>in which <em>α</em> is the threshold to detect the
        noisy regions, and can be tuned in the training phase. If
        tweet <em>t<sub>i</sub></em> is predicted to be positive
        and is not located in a noisy region, the value of
        <em>PFlag</em>(<em>t<sub>i</sub></em> ) is set, and
        likewise, if it is predicted to be negative, and is not
        located in the noisy regions the value of
        <em>NFlag</em>(<em>t<sub>i</sub></em> ) is set. The output
        of one example of the noisy region detection is illustrated
        in Figure&nbsp;<a class="fig" href="#fig1">1</a>(a).
        Figure&nbsp;<a class="fig" href="#fig1">1</a>(a)
        illustrates a 2-dimensional projection of positive and
        negative examples (marked with ’x’ and ’o’, respectively)
        using t-SNE [<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0025">25</a>], and the corresponding noisy
        region, the circle area where the centroids of positive and
        negative examples are not distinguishable with high
        confidence. All the data points contain word
        <em>heart&nbsp;attack</em>.
        <p></p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span>
            Partitioning the Word Embeddings Space</h3>
          </div>
        </header>
        <p>The “noisy region” flags <em>PFlag</em> and
        <em>NFlag</em> can help us capture the semantic similarity
        between the tweets which potentially belong to the same
        class. However, even though we can control the degree of
        uncertainty in <em>PFlag</em> and <em>NFlag</em> through
        the parameter <em>α</em>, they may still propagate the
        noise in the embeddings space to the final feature vectors.
        Since the original lexical feature vectors are sparse,
        <em>PFlag</em> and <em>NFlag</em> may be awarded a high
        weight by the final classifier, and potentially cause more
        errors. To reduce the effect of these features, and also to
        utilize the association between the lexical features and
        their representation in the embeddings space, we constrain
        <em>PFlag</em>(<em>t<sub>i</sub></em> ) and
        <em>NFlag</em>(<em>t<sub>i</sub></em> ) to the region in
        the embeddings space in which <em>t<sub>i</sub></em> is
        located. Thus, we expect the two features also reflect the
        lexical similarity to some extent (in addition to storing
        information about the class label). The idea is illustrated
        in Figure&nbsp;<a class="fig" href="#fig1">1</a>(b).</p>
        <p>Figure <a class="fig" href="#fig1">1</a>(b) shows the
        same space that we discussed earlier with 3 hypothetical
        partitions, and two features for the examples mapped to
        each partition. Given a tweet <em>t<sub>i</sub></em>
        appearing in partition <em>P<sub>k</sub></em> feature
        <em>PFlag<sub>k</sub></em> (<em>t<sub>i</sub></em> ) or
        <em>NFlag<sub>k</sub></em> (<em>t<sub>i</sub></em> ) can be
        set. For instance, for the positive set of tweets which
        appear in partition <em>P</em> <sub>2</sub>, only the value
        of <em>PFlag</em> <sub>2</sub> is set, and for the negative
        set of tweets which appear in partition <em>P</em>
        <sub>3</sub>, only the value of <em>NFlag</em> <sub>3</sub>
        is set. We emphasize that this is different from the idea
        of clustering the embeddings space. The partitions in our
        case are used to represent the original posting text along
        with the class labels, since the tweets which are close in
        the embeddings space are likely to also share lexical
        content. On the other hand, we don't expect to have pure
        partitions due to the expected overlap in the vocabulary
        between the negative and positive classes.</p>
        <figure id="fig1">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186055/images/www2018-64-fig1.jpg"
          class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span>
            <span class="figure-title">(a) Tweet centroids in the
            word embeddings space that contain the phrase <em>heart
            attack</em>, projected to two dimensional space using
            t-SNE. (b) The same word embeddings space with 3
            hypothetical partitions, and a pair of features
            associated with each partition.</span>
          </div>
        </figure>
        <p></p>
        <p>The number of partitions (<em>K</em>) can be tuned
        experimentally in the training phase. In general, we expect
        large partitions to improve recall, but to decrease
        precision. This is because larger partitions could result
        in a higher number of tweets to be mapped to the same pair
        of <em>PFlag</em> and <em>NFlag</em>, which can potentially
        increase the number of detected positive cases, and also
        increase the chance of mislabeling the tweets. In the rest
        of the paper, we use the word <em>we_partitioning</em> to
        refer to the features proposed in this section.</p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> Distorting
            the Word Embeddings Space</h3>
          </div>
        </header>
        <p>In Section <a class="sec" href="#sec-9">3.2</a> we tried
        to partially address one of the drawbacks of directly using
        the word embeddings centroids, which is the loss of
        information about the constituent words. However, this fix
        does not resolve the inherent problem of using the
        centroids in the original word embeddings space. One
        approach to incorporate the information about individual
        terms is to integrate word importance into the computation
        of the tweet centroid vector. For instance, to reduce the
        effect of the less informative words on the centroid
        values, [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>] suggests using IDF-weighting to
        compute the weighted average of the word vectors in the
        sentence representation context. In classification context,
        we propose to use information gain [<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0029">29</a>] weighting to compute
        the centroid vector to boost the impact of the words which
        are effective in the classification, effectively
        “distorting” the word embeddings space. More formally, we
        compute the new, “distorted” centroid of tweet <em>t</em>
        as:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ \vec{M_t} = \frac{\sum
            _{i=0}^{n}IG_i \times \vec{W_i}}{\sum
            _{i=0}^{n}IG_i}.\]</span><br />
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\vec{M_t}$</span></span> is the weighted mean vector
        for tweet <em>t</em>, <span class=
        "inline-equation"><span class=
        "tex">$\vec{W_i}$</span></span> is the vector
        representation of word <em>W<sub>i</sub></em> in tweet
        <em>t</em>, and <em>n</em> is the length of the tweet.
        <em>IG<sub>i</sub></em> is the information gain of word
        <em>W<sub>i</sub></em> in the training set, and is computed
        as:
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\[ IG_i = Entr(D) -
            (\frac{|D_{w_i}|}{|D|} \times Entr(D_{w_i}) +
            \frac{|\overline{D_{w_i}}|}{|D|} \times
            Entr(\overline{D_{w_i}}))\]</span><br />
          </div>
        </div>where <em>D</em> is the training set,
        <em>Entr</em>(<em>D</em>) is the entropy of <em>D</em>
        relative to our classification problem, |<em>D</em>| is the
        size of the training set, <span class=
        "inline-equation"><span class="tex">$D_{w_i}$</span></span>
        is the subset of the training set for which
        <em>W<sub>i</sub></em> occurs, and <span class=
        "inline-equation"><span class=
        "tex">$\overline{D_{w_i}}$</span></span> is the subset of
        the training set for which <em>W<sub>i</sub></em> does not
        occur. For the words which do not appear in the training
        set, we estimate their information gain using the
        information gain of their closest word in the embeddings
        space, that do appear in the training set.
        <p></p>
        <p>Figure <a class="fig" href="#fig2">2</a> shows the same
        set of tweets from Figure&nbsp;<a class="fig" href=
        "#fig1">1</a>, after applying “IG-weighting”. The
        projection illustrates that in some cases, the
        transformation can successfully separate the tweets in
        different classes by mapping them to different regions of
        the word embeddings space.</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186055/images/www2018-64-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">The same set of tweet
            centroids reported in Figure <a class="fig" href=
            "#fig1">1</a>, after applying IG-weighting
            transformation.</span>
          </div>
        </figure>
        <p></p>
        <p>To convert the new, weighed, centroids into features, we
        transform all the centroids using the information gain
        values extracted in the training set, and follow the model
        described in the previous sections to extract the
        centroid-based features for each tweet. The values of the
        parameter <em>α</em> and the number of partitions
        <em>K</em>, can be potentially different in the distorted
        word embeddings space, therefore, we call them <em>α</em>
        <sub>2</sub> and <em>K</em> <sub>2</sub>. In the rest of
        the paper, we use the term <em>we_distortion</em> to refer
        to the features introduced in this section.</p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span>
            Representing the Posting Context</h3>
          </div>
        </header>
        <p>Previous studies in monitoring public health have shown
        that user posting history is a good indicator of his or her
        current state, e.g., for depression detection [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0009">9</a>]. We
        hypothesize that the users who post a message which
        includes a true personal health mention, might have already
        posted or will post a similar message. Although those
        messages may not necessarily contain the disease keywords,
        they may be semantically or lexically related to the
        current one. Therefore, we assume that true health-related
        postings will be somewhat consistent with the other,
        contemporaneous, posts by the user. Of course, the actual
        effect of the health event on the user depends on a variety
        of factors, e.g., the severity of the condition. To enable
        our model to capture these effects, in a way appropriate
        for each disease or condition of interest, we include a
        representation of the prior and subsequent posts by the
        user<a class="fn" href="#fn3" id=
        "foot-fn3"><sup>3</sup></a>. Therefore, we use the
        representation described in Sections&nbsp;<a class="sec"
        href="#sec-9">3.2</a> and <a class="sec" href=
        "#sec-10">3.3</a> to also represent the prior- and next-
        tweets of the user, and incorporate the resulting features
        into the final combined feature vector. In the subsequent
        sections, we use the terms <em>context_prev</em> and
        <em>context_next</em> to refer to the features extracted
        from the previous and next user messages respectively.</p>
      </section>
    </section>
    <section id="sec-13">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> <em>WESPAD</em>
          Classifier Implementation</h2>
        </div>
      </header>
      <p>So far, the proposed representation model provides a
      general approach for feature learning, and can be implemented
      with a number of different algorithms. We now describe the
      specific implementation to operationalize <em>WESPAD</em>
      into a classifier used for experiments in the rest of the
      paper. We emphasize that the implementation described below
      is just one (effective) way to operationalize the proposed
      model.</p>
      <p><strong>Lexical and syntactic features
      (Section&nbsp;<a class="sec" href="#sec-8">3.1</a>)</strong>:
      to parse and build the dependency tree for the tweet contents
      we used the parser introduced in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0021">21</a>]. No stemming or stopword
      removal was performed<a class="fn" href="#fn4" id=
      "foot-fn4"><sup>4</sup></a>. To extract the frequent
      subtrees, we used the approach proposed in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>], with minimum support 10
      and minimum tree size 2, as suggested in [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0026">26</a>].</p>
      <p><strong>Word embeddings implementation</strong>: We
      experimented with multiple pre-trained word embeddings
      implementations. Specifically, we compared the
      <em>word2vec</em> word embeddings [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0028">28</a>] (with 300 dimensions),
      and the pretrained <em>GloVe</em> word embeddings [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0034">34</a>] (with 200
      dimensions), specifically trained on Twitter data. We
      observed that the performance was impacted relatively equally
      by both word embeddings; for generality, we use the
      “standard” available <em>word2vec</em> word
      embeddings<a class="fn" href="#fn5" id=
      "foot-fn5"><sup>5</sup></a> for all of the reported
      experiments. Additional incremental improvements to our
      method may be achieved with further training of the word
      embeddings on domain-specific data, as done by some of the
      methods that we compare to in Section&nbsp;<a class="sec"
      href="#sec-18">6.1</a>.</p>
      <p><strong>Detecting noisy regions in the word embeddings
      space (Section &nbsp;<a class="sec" href=
      "#sec-9">3.2</a>)</strong>: to detect the “noisy” regions in
      the word embeddings space, we implement the probabilistic
      mapping function <em>Pr</em> by using the Mallet
      implementation [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>] of the multivariate logistic
      regression classifier with default settings.</p>
      <p><strong>Partitioning the word embeddings space
      (Section&nbsp;<a class="sec" href=
      "#sec-10">3.3</a>)</strong>: to partition the word embeddings
      space into homogeneous regions we used the ELKI [<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0038">38</a>]
      implementation of the <em>K</em>-means clustering algorithm.
      The value of K was chosen automatically for each task as
      described in Section <a class="sec" href=
      "#sec-17">5.3</a>.</p>
      <p><strong>Combining</strong> <em>WESPAD</em>
      <strong>features for health mention prediction</strong>:
      Finally, we combine the lexical, syntactic, word
      embedding-based, and context features described above into a
      joint model. For simplicity and interpretability, we used a
      logistic regression classifier, trained over the final
      feature vectors to label the tweets<a class="fn" href="#fn6"
      id="foot-fn6"><sup>6</sup></a>. Other classification
      algorithms, such as GBDT [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>], may potentially provide additional
      improvements by capturing non-linear relationships between
      the features, and may be explored in the future work.</p>
      <p>For simplicity, the specific <em>WESPAD</em>
      implementation described above will be simply referenced as
      <em>WESPAD</em> for all of the reported experiments in the
      rest of the paper.</p>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experimental
          Setup</h2>
        </div>
      </header>
      <p>We now describe the datasets that we used in the
      experiments, which include both an established benchmark
      dataset, and a new dataset created for the evaluation. Then
      we describe the baseline methods, and the experimental setup
      used for reporting and analyzing the results.</p>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span>
            Datasets</h3>
          </div>
        </header>
        <p>We used two datasets for training and evaluation. First,
        the prominent benchmark dataset introduced in
        reference&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>], focusing on identifying reports
        of influenza infection. This dataset, which we call
        <em>FLU2013</em> serves for calibration and benchmarking of
        our method and others, against a state-of-the-art method
        specifically designed for detecting Flu infection reports
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>]. To
        explore the scalability of the <em>PHM</em> detection of
        multiple diseases and conditions, we also created a new
        dataset, PHM2017, described below.</p>
        <p><strong>FLU2013</strong>: this dataset was introduced in
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>], and
        focused on separating <em>awareness</em> of the disease
        from actual <em>infection</em> reports. Each tweet in the
        dataset was manually labeled into classes of flu awareness
        (negative) or flu report (positive). Since only Twitter IDs
        were distributed, the content of the tweets had to be
        retrieved for this study, which was done in winter 2017. At
        that time, there were 2,837 tweets still available to
        download, which is 63% of the original dataset. There were
        1,393 awareness (negative class) tweets, which account for
        49% of the dataset, and 1,444 report (positive class)
        tweets which account for 51% of the dataset.</p>
        <p><strong>PHM2017</strong>: We also constructed a new
        dataset consisting of 7,192 English tweets across six
        diseases and conditions: Alzheimer's Disease, heart attack
        (any severity), Parkinson's disease, cancer (any type),
        Depression (any severity), and Stroke. We used the Twitter
        search API to retrieve the data using the colloquial
        disease names as search keywords, with the expectation of
        retrieving a high-recall, low precision dataset. After
        removing the re-tweets and replies, the tweets were
        manually annotated. The labels are:</p>
        <ul class="list-no-style">
          <li id="list4" label="•"><em>self-mention</em>. The tweet
          contains a health mention with a health self-report of
          the Twitter account owner, e.g., <em>”However, I worked
          hard and ran for Tokyo Mayer Election Campaign in January
          through February, 2014, without publicizing the
          cancer.”</em><br /></li>
          <li id="list5" label="•"><em>other-mention</em>. The
          tweet contains a health mention of a health report about
          someone other than the account owner, e.g., <em>”Designer
          with Parkinson's couldn't work then engineer invents
          bracelet + changes her world”</em><br /></li>
          <li id="list6" label="•"><em>awareness</em>. The tweet
          contains the disease name, but does not mention a
          specific person, e.g., <em>”A Month Before a Heart
          Attack, Your Body Will Warn You With These 8
          Signals”</em><br /></li>
          <li id="list7" label="•"><em>non-health</em>. The tweet
          contains the disease name, but the tweet topic is not
          about health. <em>”Now I can have cancer on my wall for
          all to see &lt;3”</em><br /></li>
        </ul>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class=
            "table-title">The distribution of tweets over topics
            and labels in <strong>PHM2017</strong> dataset.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">
                <strong>Topic</strong></th>
                <th style="text-align:center;"><strong>Tweet
                count</strong></th>
                <th style="text-align:center;">
                <em>self-mention</em></th>
                <th style="text-align:center;">
                <em>other-mention</em></th>
                <th style="text-align:center;">
                <em>awareness</em></th>
                <th style="text-align:center;">
                <em>non-health</em></th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">Alzheimer</td>
                <td style="text-align:center;">1256</td>
                <td style="text-align:center;">1%</td>
                <td style="text-align:center;">17%</td>
                <td style="text-align:center;">80%</td>
                <td style="text-align:center;">2%</td>
              </tr>
              <tr>
                <td style="text-align:center;">heart attack</td>
                <td style="text-align:center;">1219</td>
                <td style="text-align:center;">4%</td>
                <td style="text-align:center;">9%</td>
                <td style="text-align:center;">17%</td>
                <td style="text-align:center;">70%</td>
              </tr>
              <tr>
                <td style="text-align:center;">Parkinson</td>
                <td style="text-align:center;">1040</td>
                <td style="text-align:center;">2%</td>
                <td style="text-align:center;">9%</td>
                <td style="text-align:center;">65%</td>
                <td style="text-align:center;">24%</td>
              </tr>
              <tr>
                <td style="text-align:center;">cancer</td>
                <td style="text-align:center;">1242</td>
                <td style="text-align:center;">3%</td>
                <td style="text-align:center;">18%</td>
                <td style="text-align:center;">62%</td>
                <td style="text-align:center;">17%</td>
              </tr>
              <tr>
                <td style="text-align:center;">depression</td>
                <td style="text-align:center;">1213</td>
                <td style="text-align:center;">37%</td>
                <td style="text-align:center;">3%</td>
                <td style="text-align:center;">49%</td>
                <td style="text-align:center;">11%</td>
              </tr>
              <tr>
                <td style="text-align:center;">stroke</td>
                <td style="text-align:center;">1222</td>
                <td style="text-align:center;">3%</td>
                <td style="text-align:center;">11%</td>
                <td style="text-align:center;">29%</td>
                <td style="text-align:center;">57%</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>In our experiments, <em>self-mention</em> and
        <em>other-mention</em> labels are taken as positive class;
        and <em>awareness</em> and <em>non-health</em> labels are
        taken as negative class. To validate the labels, we engaged
        another annotator and randomly re-annotated 10% of the
        tweets for each topic. Since we observed that the
        probability of having a disputed positive label is higher
        than having a disputed negative label, the 10% re-labeling
        subset was drawn from the positive set. The re-annotation
        showed 85% agreement between the annotators, which is
        acceptable for a challenging topic like health report.
        Table <a class="tbl" href="#tab1">1</a> summarizes
        <em>PHM2017</em> dataset. We observe that for each topic, a
        large portion of the tweets are in <em>non-health</em>
        category, which shows that people tend to use these words
        in other contexts too–which confirms the previous findings
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0043">43</a>]. The
        statistics also show that, on average 19.5% of the tweets
        in each topic are positive, which makes the classification
        task more challenging. Having both a balanced dataset
        (FLU2013) and an imbalanced dataset (PHM2017) helps us to
        evaluate our method in different settings.</p>
        <p>To build the context features for <em>PHM2017</em>
        dataset, we used Twitter API to download the user
        timelines. We were unable to build the context features for
        many of the tweets in the flu dataset, since in many cases
        either the timeline was unaccessible, or access to the user
        profile was restricted. Therefore, we report the results
        for FLU2013 without incorporating the context features,
        which, as we show, are helpful in PHM2017 dataset, and are
        expected to be available for many applications.</p>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Methods
            Compared</h3>
          </div>
        </header>
        <p>We implemented or adapted the following methods to
        compare <em>WESPAD</em> to both previously used methods for
        health classification, and to the latest classification
        methods based on deep neural networks that have shown
        promising performance for other tasks. In Section <a class=
        "sec" href="#sec-4">2</a>, we discussed that deep neural
        network classifiers need a large training set to reach
        their best performance; however, we included these
        state-of-the-art baselines to compare to the models which
        only rely on word embeddings.</p>
        <ul class="list-no-style">
          <li id="list8" label="•"><em>ME+lex</em>. We used a
          logistic regression classifier (a.k.a Maximum Entropy
          classifier) trained over unigrams and bigrams.<br /></li>
          <li id="list9" label="•"><em>ME+cen</em>. We used a
          logistic regression classifier trained over the text
          centroid representation of the tweets in the embeddings
          space.<br /></li>
          <li id="list10" label="•"><em>ME+lex+emb</em>. We
          computed the text centroid representation of each tweet
          in the embeddings space, and combined the resulting
          vector with the unigrams and bigrams of the tweet. Then a
          logistic regression classifier was trained over the final
          vectors.<br /></li>
          <li id="list11" label="•">
            <em>ME+lex+cen</em>. We added two features
            <em>PFlag</em> and <em>NFlag</em> to the corresponding
            vector of unigrams and bigrams of each tweet. Then we
            used the prediction of <em>ME+cen</em> to set the
            values of <em>PFlag</em> to true if predicted positive,
            and <em>NFlag</em> to true if predicted negative.
            Finally, a logistic regression classifier was trained
            over the resulting vectors, to evaluate the
            contribution of our noise filtering method (Section
            <a class="sec" href="#sec-9">3.2</a>).<br />
          </li>
          <li id="list12" label="•">
            <em>Rules</em>. Experiments in [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0022">22</a>]
            suggest that manually extracted templates and features
            are effective in detecting flu reports. We implemented
            the top six set of features reported in [<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href="#BibPLXBIB0022">22</a>], and
            trained a logistic regression classifier over the
            resulting vectors. This model was used only in FLU2013
            dataset.<br />
          </li>
          <li id="list13" label="•">
            <em>CNN</em>. We used the convolutional neural network
            classifier introduced in [<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0020">20</a>]. We used the non-static
            variant, which can update the word vectors in the
            training. Using grid search, we tuned the number of
            convolution feature maps from values: {50, 100, 150},
            and observed that the number of features highly depends
            on the training data, and thus was optimized
            automatically using grid search for each classification
            task. The rest of the hyperparameters were set to the
            suggested values<a class="fn" href="#fn7" id=
            "foot-fn7"><sup>7</sup></a>.<br />
          </li>
          <li id="list14" label="•">
            <em>FastText</em>. We used the shallow neural network
            introduced in [<a class="bib" data-trigger="hover"
            data-toggle="popover" data-placement="top" href=
            "#BibPLXBIB0015">15</a>], known as FastText. This model
            represents the documents by taking the average over the
            individual word vectors, and can also update the
            vectors during the training. To tune the model we tried
            values: {0.05, 0.1, 0.25, 0.5} for learning rate, and
            values: {2, 4} for window size. We observed that the
            optimal value of the learning rate was not fixed,
            neither in FLU2013 nor in PHM2017. The value of window
            size was optimal at 4 in FLU2013, but was not fixed in
            PHM2017. The rest of the hyperparameters were set to
            the suggested values [<a class="bib" data-trigger=
            "hover" data-toggle="popover" data-placement="top"
            href="#BibPLXBIB0015">15</a>].<br />
          </li>
          <li id="list15" label="•">
            <em>LSTM-GRNN</em>. We used the model proposed in
            [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0041">41</a>], which is a two-step
            classifier. In the first step the model uses a long
            short-term memory neural network (LSTM) to produce the
            sentence representations, and in the second step, uses
            a gated recurrent neural network (GRNN) to encode the
            sentence relations in the document. Tweet dependency
            trees [<a class="bib" data-trigger="hover" data-toggle=
            "popover" data-placement="top" href=
            "#BibPLXBIB0021">21</a>] were used to detect sentence
            boundaries in order to produce the sentence
            representations. To tune the model, we used the values:
            {0.03, 0.3, 0.5} for learning rate, and observed that
            it is optimal at 0.3 in FLU2013, but is not fixed in
            PHM2017. The rest of the hyperparameters were set to
            the suggested values in the original
            implementation&nbsp;reference&nbsp;[<a class="bib"
            data-trigger="hover" data-toggle="popover"
            data-placement="top" href=
            "#BibPLXBIB0041">41</a>].<br />
          </li>
          <li id="list16" label="•">
            <em>WESPAD</em> : our method, described in
            Section&nbsp;<a class="sec" href="#sec-7">3</a> and
            implemented as described in Section&nbsp;<a class="sec"
            href="#sec-13">4</a>.<br />
          </li>
        </ul>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Training
            setup</h3>
          </div>
        </header>
        <p>To train and evaluate all of the methods in a fair and
        consistent way, we used the standard 10 fold
        Cross-Validation in FLU2013 dataset, and within each topic
        of PHM2017 dataset. The results reported in the next
        section are the averages over the test folds. To build the
        folds, we preserved the original distribution of the
        labels, and randomly assigned the tweets to each fold.
        Since the set of the positive tweets is small, we kept the
        folds fixed across all of the cross validation experiments,
        to ensure that all of the methods were trained and tested
        in identical train/validate/test folds and thus the results
        can be compared directly.</p>
        <p>We used grid search to tune the model hyper-parameters
        by maximizing the F1-measure in the target (positive) set.
        To tune the number of partitions <em>K</em> and <em>K</em>
        <sub>2</sub> in the word embedding-based features of
        <em>WESPAD</em> (introduced in Sections <a class="sec"
        href="#sec-10">3.3</a> and <a class="sec" href=
        "#sec-11">3.4</a>), we experimented with the values: {3, 4,
        5}, and observed that their optimal values depended on the
        training data, and thus were chosen automatically for each
        task. To tune <em>α</em> and <em>α</em> <sub>2</sub>
        (introduced in Sections <a class="sec" href=
        "#sec-9">3.2</a> and <a class="sec" href="#sec-11">3.4</a>)
        we tried values: {0.05, 0.15, 0.3}, and observed the best
        performance for the value 0.05 in the FLU2013 dataset, and
        for the value 0.3, for all the topics, in PHM2017
        dataset<a class="fn" href="#fn8" id=
        "foot-fn8"><sup>8</sup></a>.</p>
        <p><strong>Evaluation Metrics</strong>: Since the
        proportion of the positive class in PHM2017 dataset was
        relatively low, the accuracy of all the models was high (on
        average 90%), due primarily to accurately predicting the
        negative (majority) class–which is not as practically
        important as the target (positive) class (the true health
        mentions and the target of our study). Therefore, in the
        next section we report the <strong>F1-measure</strong>,
        <strong>Precision</strong>, and <strong>Recall</strong> for
        the positive class.</p>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Results and
          Discussion</h2>
        </div>
      </header>
      <p>We now report the experimental results. First, we report
      the main results in Section <a class="sec" href=
      "#sec-19">6.1</a>, followed by the discussion and feature
      analysis in Section&nbsp;<a class="sec" href=
      "#sec-20">6.2</a>.</p>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.1</span> Main
            Results</h3>
          </div>
        </header>
        <p>Table <a class="tbl" href="#tab2">2</a> reports
        F1-measure of all the models (described in Section
        <a class="sec" href="#sec-16">5.2</a>) across the topics in
        PHM2017 dataset. The experiments show that our model
        <em>WESPAD</em> outperforms all the baselines in the
        majority of the topics. The substantial difference in terms
        of F1-measure between <em>ME+lex</em> and <em>WESPAD</em>
        models, shows that our model has successfully managed to
        learn the characteristics of the small set of the positive
        tweets, and to generalize better. Another observation is
        that model <em>ME+lex+cen</em>, which uses lexical features
        alongside an output of a centroid-based classifier as an
        additional feature (see Section <a class="sec" href=
        "#sec-16">5.2</a> for details), is performing relatively
        poorly. This validates our strategy described in <a class=
        "sec" href="#sec-9">3.2</a> and <a class="sec" href=
        "#sec-10">3.3</a>, and the need to detect and filter out
        the noisy regions in the word embeddings space. We can also
        see that <em>CNN</em>, although with small amount of
        training data, is working surprisingly well. On the other
        hand, the complex <em>LSTM+GRNN</em> model is outperformed
        on all the topics by our <em>WESPAD</em> classifier.</p>
        <p>Table <a class="tbl" href="#tab3">3</a> reports the
        average F1-measure, precision, and recall for all the
        models across the six topics in PHM2017 dataset. The
        results show that the main improvement of <em>WESPAD</em>
        comes from the higher recall, i.e., detecting additional
        true health mentions. Table <a class="tbl" href=
        "#tab3">3</a> also shows that the highest precision is
        achieved by the simple <em>ME+lex</em> model, since this
        model only relies on the lexical features. On the other
        hand, <em>LSTM+GRNN</em> has the lowest precision, and this
        can be attributed to the complex structure of the network
        which expects to be fine-tuned during the training.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">F1-measure for the models across all the
            topics in PHM2017 dataset.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">
                <strong>Model</strong></td>
                <td style="text-align:center;">
                <strong>Alzheimer's</strong></td>
                <td style="text-align:center;"><strong>Heart
                attack</strong></td>
                <td style="text-align:center;">
                <strong>Parkinson's</strong></td>
                <td style="text-align:center;">
                <strong>Cancer</strong></td>
                <td style="text-align:center;">
                <strong>Depression</strong></td>
                <td style="text-align:center;">
                <strong>Stroke</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ME+lex</em></td>
                <td style="text-align:center;">0.701</td>
                <td style="text-align:center;">0.399</td>
                <td style="text-align:center;">0.468</td>
                <td style="text-align:center;">0.533</td>
                <td style="text-align:center;">0.722</td>
                <td style="text-align:center;">0.610</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ME+cen</em></td>
                <td style="text-align:center;">0.704</td>
                <td style="text-align:center;">0.327</td>
                <td style="text-align:center;">0.383</td>
                <td style="text-align:center;">0.587</td>
                <td style="text-align:center;">0.727</td>
                <td style="text-align:center;">0.453</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>ME+lex+emb</em></td>
                <td style="text-align:center;">0.723</td>
                <td style="text-align:center;">0.460</td>
                <td style="text-align:center;">0.486</td>
                <td style="text-align:center;">0.559</td>
                <td style="text-align:center;">0.718</td>
                <td style="text-align:center;">0.612</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>ME+lex+cen</em></td>
                <td style="text-align:center;">0.720</td>
                <td style="text-align:center;">0.415</td>
                <td style="text-align:center;">0.464</td>
                <td style="text-align:center;">0.628</td>
                <td style="text-align:center;">0.737</td>
                <td style="text-align:center;">0.601</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>LSTM-GRNN</em></td>
                <td style="text-align:center;">0.725</td>
                <td style="text-align:center;">0.482</td>
                <td style="text-align:center;">0.617</td>
                <td style="text-align:center;">0.624</td>
                <td style="text-align:center;">0.676</td>
                <td style="text-align:center;">0.564</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>FastText</em></td>
                <td style="text-align:center;">0.769</td>
                <td style="text-align:center;">0.491</td>
                <td style="text-align:center;">0.540</td>
                <td style="text-align:center;">0.605</td>
                <td style="text-align:center;">0.741</td>
                <td style="text-align:center;">0.633</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>CNN</em></td>
                <td style="text-align:center;">0.767</td>
                <td style="text-align:center;">0.554</td>
                <td style="text-align:center;">0.653</td>
                <td style="text-align:center;">0.622</td>
                <td style="text-align:center;">
                <strong>0.768</strong></td>
                <td style="text-align:center;">0.676</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>WESPAD</em></td>
                <td style="text-align:center;">
                <strong>0.800</strong></td>
                <td style="text-align:center;">
                <strong>0.571</strong></td>
                <td style="text-align:center;">
                <strong>0.672</strong></td>
                <td style="text-align:center;">
                <strong>0.670</strong></td>
                <td style="text-align:center;">0.758</td>
                <td style="text-align:center;">
                <strong>0.698</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class=
            "table-title"><span style=
            "text-decoration: underline;">Average</span>
            F1-measure, precision, and recall in PHM2017
            dataset.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">
                <strong>Model</strong></td>
                <td style="text-align:center;">
                <strong>F1</strong></td>
                <td style="text-align:center;">
                <strong>Precision</strong></td>
                <td style="text-align:center;">
                <strong>Recall</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ME+lex</em></td>
                <td style="text-align:center;">0.572</td>
                <td style="text-align:center;">
                <strong>0.834</strong></td>
                <td style="text-align:center;">0.462</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ME+cen</em></td>
                <td style="text-align:center;">0.530</td>
                <td style="text-align:center;">0.819</td>
                <td style="text-align:center;">0.429</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>ME+lex+emb</em></td>
                <td style="text-align:center;">0.593</td>
                <td style="text-align:center;">0.833</td>
                <td style="text-align:center;">0.483</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>ME+lex+cen</em></td>
                <td style="text-align:center;">0.594</td>
                <td style="text-align:center;">0.827</td>
                <td style="text-align:center;">0.493</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>LSTM-GRNN</em></td>
                <td style="text-align:center;">0.615</td>
                <td style="text-align:center;">0.638</td>
                <td style="text-align:center;">0.605</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>FastText</em></td>
                <td style="text-align:center;">0.630</td>
                <td style="text-align:center;">0.802</td>
                <td style="text-align:center;">0.538</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>CNN</em></td>
                <td style="text-align:center;">0.673</td>
                <td style="text-align:center;">0.794</td>
                <td style="text-align:center;">0.610</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>WESPAD</em></td>
                <td style="text-align:center;">
                <strong>0.695</strong></td>
                <td style="text-align:center;">0.803</td>
                <td style="text-align:center;">
                <strong>0.628</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab4">
          <div class="table-caption">
            <span class="table-number">Table 4:</span> <span class=
            "table-title">F1-measure, precision, and recall in
            FLU2013 dataset.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;">
                <strong>Model</strong></td>
                <td style="text-align:center;">
                <strong>F1</strong></td>
                <td style="text-align:center;">
                <strong>Precision</strong></td>
                <td style="text-align:center;">
                <strong>Recall</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ME+lex</em></td>
                <td style="text-align:center;">0.838</td>
                <td style="text-align:center;">0.832</td>
                <td style="text-align:center;">0.846</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>ME+cen</em></td>
                <td style="text-align:center;">0.827</td>
                <td style="text-align:center;">0.815</td>
                <td style="text-align:center;">0.840</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>ME+lex+emb</em></td>
                <td style="text-align:center;">0.843</td>
                <td style="text-align:center;">0.837</td>
                <td style="text-align:center;">0.850</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>ME+lex+cen</em></td>
                <td style="text-align:center;">0.844</td>
                <td style="text-align:center;">0.843</td>
                <td style="text-align:center;">0.845</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>Rules</em></td>
                <td style="text-align:center;">0.845</td>
                <td style="text-align:center;">0.837</td>
                <td style="text-align:center;">0.855</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>LSTM-GRNN</em></td>
                <td style="text-align:center;">0.818</td>
                <td style="text-align:center;">0.805</td>
                <td style="text-align:center;">0.833</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>FastText</em></td>
                <td style="text-align:center;">0.841</td>
                <td style="text-align:center;">0.831</td>
                <td style="text-align:center;">0.852</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>CNN</em></td>
                <td style="text-align:center;">0.833</td>
                <td style="text-align:center;">
                <strong>0.864</strong></td>
                <td style="text-align:center;">0.806</td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>WESPAD</em></td>
                <td style="text-align:center;">
                <strong>0.851</strong></td>
                <td style="text-align:center;">0.845</td>
                <td style="text-align:center;">
                <strong>0.858</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>Table <a class="tbl" href="#tab4">4</a> reports
        F1-measure, precision, and recall of all the baselines in
        comparison to <em>WESPAD</em> in FLU2013 dataset. The
        results show that <em>WESPAD</em> outperforms all the
        baselines, even though there are considerable differences
        between PHM2017 and FLU2013 datasets (in terms of the
        proportion of the positive tweets). The results also show
        that <em>WESPAD</em> performs slightly better than the
        disease-specific <em>Rules</em> classifier, implemented
        according to the descriptions in reference&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>]. More
        detailed analysis revealed that the syntactic subtrees that
        we use in our model, to some extent, can also automatically
        capture the manually designed patterns reported in
        [<a class="bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0022">22</a>]. It is
        also worth mentioning that, all the improvements of
        <em>WESPAD</em> model over the lexical baseline
        <em>ME+lex</em> in both datasets are statistically
        significant using paired t-test at <em>p &lt;
        0.05</em>.</p>
        <p>The comparison between the relative improvement of
        <em>WESPAD</em> in PHM2017 and FLU2013 datasets shows that
        our model performs significantly better in PHM2017 dataset.
        The improvement can be attributed to the inherent
        differences between these two datasets, and the fact that
        PHM2017 is highly imbalanced and FLU2013 is nearly
        balanced. We discuss this issue further in the next
        section.</p>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">6.2</span>
            Discussion</h3>
          </div>
        </header>
        <p>We now analyze the performance of <em>WESPAD</em> in
        more detail, focusing on the effects of the word embeddings
        partitioning, contribution of different features, and the
        ability of <em>WESPAD</em> to generalize from few positive
        examples in training.</p>
        <p><strong>Word embeddings partitions</strong>: in
        Section&nbsp;<a class="sec" href="#sec-10">3.3</a> we
        argued that large partitions can increase recall, and
        degrade precision. To support the argument, we fixed the
        values of <em>α</em> and <em>α</em> <sub>2</sub>; and
        experimented with different values for <em>K</em> (the
        number of the partitions in the regular embeddings space)
        and <em>K</em> <sub>2</sub> (the number of the partitions
        in the distorted embeddings space). Figure <a class="fig"
        href="#fig3">3</a> illustrates the result of this
        experiment. To be able to easier interpret the results, we
        also set <em>K</em> to be equal to <em>K</em> <sub>2</sub>.
        The experiment confirms that by decreasing the number of
        partitions (and thereby increasing the partition sizes),
        the <em>Recall</em> of <em>WESPAD</em> improves. However,
        this comes at the cost of degrading the <em>Precision</em>
        (specifically at <em>K</em> = 1).</p>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186055/images/www2018-64-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Impact of the number of
            partitions <em>K</em> on <em>WESPAD</em> on F1-measure,
            precision, and recall (PHM2017 dataset).</span>
          </div>
        </figure><strong>Feature ablation</strong>: Table <a class=
        "tbl" href="#tab5">5</a> reports the result of the ablation
        study on the features in <em>WESPAD</em> model in PHM2017
        dataset. The experiment shows that <em>we_distortion</em>
        and <em>we_partitioning</em> feature sets have the highest
        impact, in terms of F1-measure. We also observe that, in
        terms of precision, <em>we_partitioning</em> performs
        better than <em>we_distortion</em>. One possible
        explanation is that due to the small size of the positive
        sets, IG-weighting may fail to accurately assign the
        weights to the word vectors, and thus, the tweet centroid
        is drifted.
        <p></p>
        <div class="table-responsive" id="tab5">
          <div class="table-caption">
            <span class="table-number">Table 5:</span> <span class=
            "table-title">Feature ablation of <em>WESPAD</em> on
            PHM2017 dataset.</span>
          </div>
          <table class="table">
            <tbody>
              <tr>
                <td style="text-align:center;"><strong>Feature
                set</strong></td>
                <td style="text-align:center;">
                <strong>F1</strong></td>
                <td style="text-align:center;">
                <strong>Precision</strong></td>
                <td style="text-align:center;">
                <strong>Recall</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;"><em>WESPAD</em> (all
                features)</td>
                <td style="text-align:center;">0.695</td>
                <td style="text-align:center;">0.803</td>
                <td style="text-align:center;">0.628</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>we_distortion</em></td>
                <td style="text-align:center;">0.643 (-7.4%)</td>
                <td style="text-align:center;">0.804</td>
                <td style="text-align:center;">0.554</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>we_partitioning</em></td>
                <td style="text-align:center;">0.652 (-6.1%)</td>
                <td style="text-align:center;">0.788</td>
                <td style="text-align:center;">0.578</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>context_next</em></td>
                <td style="text-align:center;">0.680 (-2.1%)</td>
                <td style="text-align:center;">0.800</td>
                <td style="text-align:center;">0.609</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>syn_feats</em></td>
                <td style="text-align:center;">0.682 (-1.8)</td>
                <td style="text-align:center;">0.800</td>
                <td style="text-align:center;">0.613</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>context_prev</em></td>
                <td style="text-align:center;">0.686 (-1.2%)</td>
                <td style="text-align:center;">0.801</td>
                <td style="text-align:center;">0.616</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>context</em></td>
                <td style="text-align:center;">0.687 (-1.1)</td>
                <td style="text-align:center;">0.795</td>
                <td style="text-align:center;">0.620</td>
              </tr>
              <tr>
                <td style="text-align:center;">
                <em>lex_feats</em></td>
                <td style="text-align:center;">0.696 (+0.1)</td>
                <td style="text-align:center;">0.782</td>
                <td style="text-align:center;">0.640</td>
              </tr>
            </tbody>
          </table>
        </div>
        <p><strong>Effect of the number of positive
        examples</strong>: in Section&nbsp;<a class="sec" href=
        "#sec-19">6.1</a> we observed that the relative improvement
        of <em>WESPAD</em> in PHM2017 dataset is considerably
        higher than its relative improvement in FLU2013 dataset. We
        argue that since FLU2013 dataset is nearly balanced, and
        also has a substantially larger set of positive tweets,
        simple models such as <em>ME+lex</em> can perform
        relatively well. To analyze the effect of the size of the
        training data, and specifically the availability of true
        positive examples, we varied the number of the positive
        examples in the training folds, by randomly sampling from
        10% to 90% of the positive examples (and keeping all of the
        negative examples), and re-trained <em>WESPAD</em> ,
        <em>Rules</em>, and <em>ME+lex</em> in the reduced training
        sets in FLU2013 dataset. Figure <a class="fig" href=
        "#fig4">4</a> reports the values of the F1-measure for
        <em>ME+lex</em>, <em>Rules</em>, and <em>WESPAD</em> at
        varying fractions of the positive tweets used in the
        training data. The experiment shows that at smaller
        fractions of available positive tweets (10%-30%),
        <em>WESPAD</em> dramatically outperforms the
        <em>ME+lex</em> baseline, demonstrating that
        <em>WESPAD</em> is able to generalize from fewer positive
        training examples. <em>WESPAD</em> also significantly
        outperforms <em>Rules</em> at small fractions of positive
        tweets (10%-20%), signifying that the rule based models
        highly depend on their lexical based counterparts. We also
        observe that learning from just 20% of the available
        positive examples, the F1-measure for <em>ME+lex</em> model
        is 0.564, and for <em>WESPAD</em> model is 0.658. These F1
        values are comparable to the F1 values that these models
        achieved in PHM2017 dataset, which also contains only 19%
        of the positive class in the training and test data (on
        average, across the different disease topics).</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186055/images/www2018-64-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">F1 for <em>WESPAD</em> ,
            <em>Rules</em>, and <em>ME+lex</em> trained on varying
            subsets of the positive examples (in FLU2013
            dataset).</span>
          </div>
        </figure>
        <p></p>
        <p>In summary, our results show that WESPAD is able to
        outperform the state-of-the-art baselines for both datasets
        and under variety of settings, and even outperforms a
        disease-specific classifier in the prominent FLU2013
        benchmark dataset. This is striking, as WESPAD does not
        require manual feature engineering, and can be trained with
        a relatively small number of (positive) training examples
        which makes WESPAD a valuable tool for extending health
        monitoring over social data to new diseases and
        conditions.</p>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span>
          Conclusions</h2>
        </div>
      </header>
      <p>We presented a new method, <em>WESPAD</em> , designed to
      detect personal health mentions in social data, such as
      Twitter posts. Unlike previously proposed methods for health
      classification, our method requires no manual feature
      engineering, and can be trained on relatively few positive
      examples of true health mentions. The improvements are due to
      a new approach to analyzing the representation of the
      examples in word embedding spaces, allowing <em>WESPAD</em>
      to discover a small number of effective features for
      classification. Furthermore, <em>WESPAD</em> can easily
      incorporate additional domain knowledge and can be extended
      to detect new diseases and conditions with relatively little
      effort.</p>
      <p>Our experimental evaluation compares <em>WESPAD</em> to a
      variety of previously proposed methods, including three
      state-of-the-art deep neural network approaches (LSTM,
      FasText, and CNN), on both an established benchmark dataset
      for detecting Flu infection reports, and a new PHM2017
      dataset we created, with manual annotations of mentions for
      six different diseases and conditions. In the majority of the
      conditions, <em>WESPAD</em> exhibits superior overall
      performance.</p>
      <p>By requiring a smaller number of training examples to
      achieve state-of-the-art performance, <em>WESPAD</em> can
      enable rapid development of domain-specific and robust text
      classifiers, which could in turn be valuable for tracking
      emerging diseases and conditions via social media.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Charu&nbsp;C. Aggarwal
        and ChengXiang Zhai. 2012. <em><em>A Survey of Text
        Classification Algorithms</em></em> . Springer US, Boston,
        MA, 163–222.</li>
        <li id="BibPLXBIB0002" label="[2]">Eiji Aramaki, Sachiko
        Maskawa, and Mizuki Morita. 2011. Twitter Catches the Flu:
        Detecting Influenza Epidemics Using Twitter. In
        <em><em>Proceedings of the Conference on Empirical Methods
        in Natural Language Processing</em></em> (EMNLP ’11).
        Association for Computational Linguistics, Stroudsburg, PA,
        USA, 1568–1576.</li>
        <li id="BibPLXBIB0003" label="[3]">David Bamman and
        Noah&nbsp;A. Smith. 2015. Contextualized Sarcasm Detection
        on Twitter. In <em><em>Proceedings of the Ninth
        International Conference on Web and Social Media, (ICWSM
        2015), 2015</em></em> . 574–577.</li>
        <li id="BibPLXBIB0004" label="[4]">Carmen Banea, Di Chen,
        Rada Mihalcea, Claire Cardie, and Janyce Wiebe. 2014.
        SimCompass: Using Deep Learning Word Embeddings to Assess
        Cross-level Similarity. In <em><em>Proceedings of the 8th
        International Workshop on Semantic Evaluation,
        SemEval@COLING 2014, Dublin, Ireland, August 23-24,
        2014.</em></em> 560–565.</li>
        <li id="BibPLXBIB0005" label="[5]">Meeyoung Cha, Hamed
        Haddadi, Fabrício Benevenuto, and P.&nbsp;Krishna Gummadi.
        2010. Measuring User Influence in Twitter: The Million
        Follower Fallacy. In <em><em>Proceedings of the Fourth
        International Conference on Weblogs and Social Media,
        (ICWSM 2010), 2010</em></em> . 10–17.</li>
        <li id="BibPLXBIB0006" label="[6]">Lauren&nbsp;E.
        Charles-Smith, Tera&nbsp;L. Reynolds, Mark&nbsp;A. Cameron,
        Mike Conway, Eric H.&nbsp;Y. Lau, Jennifer&nbsp;M. Olsen,
        Julie&nbsp;A. Pavlin, Mika Shigematsu, Laura&nbsp;C.
        Streichert, Katie&nbsp;J. Suda, and Courtney&nbsp;D.
        Corley. 2015. Using Social Media for Actionable Disease
        Surveillance and Outbreak Management: A Systematic
        Literature Review. <em><em>PLOS ONE</em></em> 10, 10 (10
        2015), 1–20.</li>
        <li id="BibPLXBIB0007" label="[7]">Cynthia Chew and Gunther
        Eysenbach. 2010. Pandemics in the Age of Twitter: Content
        Analysis of Tweets during the 2009 H1N1 Outbreak.
        <em><em>PLOS ONE</em></em> 5, 11 (11 2010), 1–13.</li>
        <li id="BibPLXBIB0008" label="[8]">Munmun&nbsp;De
        Choudhury. 2015. Anorexia on Tumblr: A Characterization
        Study. In <em><em>Proceedings of the 5th International
        Conference on Digital Health 2015, Florence, Italy, May
        18-20, 2015</em></em> . 43–50.</li>
        <li id="BibPLXBIB0009" label="[9]">Munmun&nbsp;De
        Choudhury, Michael Gamon, Scott Counts, and Eric Horvitz.
        2013. Predicting Depression via Social Media. In
        <em><em>Proceedings of the Seventh International Conference
        on Weblogs and Social Media, (ICWSM 2013), 2013.</em></em>
        1–10.</li>
        <li id="BibPLXBIB0010" label="[10]">Munmun&nbsp;De
        Choudhury, Emre Kiciman, Mark Dredze, Glen Coppersmith, and
        Mrinal Kumar. 2016. Discovering Shifts to Suicidal Ideation
        from Mental Health Content in Social Media. In
        <em><em>Proceedings of the 2016 CHI Conference on Human
        Factors in Computing Systems, San Jose, CA, USA, May 7-12,
        2016</em></em> . 2098–2110.</li>
        <li id="BibPLXBIB0011" label="[11]">Aaron&nbsp;M. Cohen and
        William&nbsp;R. Hersh. 2005. A survey of current work in
        biomedical text mining. <em><em>Briefings in
        Bioinformatics</em></em> 6, 1 (2005), 57–71.</li>
        <li id="BibPLXBIB0012" label="[12]">Hongying Dai,
        Brian&nbsp;R. Lee, and Jianqiang Hao. 2017. Predicting
        Asthma Prevalence by Linking Social Media Data and
        Traditional Surveys. <em><em>The ANNALS of the American
        Academy of Political and Social Science</em></em> 669,
        1(2017), 75–92.</li>
        <li id="BibPLXBIB0013" label="[13]">Raminta Daniulaityte,
        Lu Chen, R.&nbsp;Francois Lamy, G.&nbsp;Robert Carlson,
        Krishnaprasad Thirunarayan, and Amit Sheth. 2016. “When
        ‘Bad’ is ‘Good”’: Identifying Personal Communication and
        Sentiment in Drug-Related Tweets. <em><em>JMIR Public
        Health Surveill</em></em> 2, 2 (24 Oct 2016), e162.</li>
        <li id="BibPLXBIB0014" label="[14]">Jerome&nbsp;H.
        Friedman. 2001. Greedy Function Approximation: A Gradient
        Boosting Machine. <em><em>The Annals of
        Statistics</em></em> 29, 5 (2001), 1189–1232.</li>
        <li id="BibPLXBIB0015" label="[15]">Edouard Grave, Tomas
        Mikolov, Armand Joulin, and Piotr Bojanowski. 2017. Bag of
        Tricks for Efficient Text Classification. In
        <em><em>Proceedings of the 15th Conference of the European
        Chapter of the Association for Computational Linguistics,
        (EACL 2017), Valencia, Spain, April 3-7, 2017, Volume 2:
        Short Papers</em></em> . 427–431.</li>
        <li id="BibPLXBIB0016" label="[16]">Eric&nbsp;H. Huang,
        Richard Socher, Christopher&nbsp;D. Manning, and
        Andrew&nbsp;Y. Ng. 2012. Improving Word Representations via
        Global Context and Multiple Word Prototypes. In
        <em><em>Proceedings of the 50th Annual Meeting of the
        Association for Computational Linguistics</em></em> (ACL
        2012). 873–882.</li>
        <li id="BibPLXBIB0017" label="[17]">Muhammad Imran,
        Prasenjit Mitra, and Carlos Castillo. 2016. Twitter as a
        Lifeline: Human-annotated Twitter Corpora for NLP of
        Crisis-related Messages. In <em><em>Proceedings of the
        Tenth International Conference on Language Resources and
        Evaluation (LREC 2016), 2016.</em></em> 1638–1643.</li>
        <li id="BibPLXBIB0018" label="[18]">Aditya Joshi, Vaibhav
        Tripathi, Kevin Patel, Pushpak Bhattacharyya, and
        Mark&nbsp;James Carman. 2016. Are Word Embedding-based
        Features Useful for Sarcasm Detection?. In
        <em><em>Proceedings of the 2016 Conference on Empirical
        Methods in Natural Language Processing, (EMNLP 2016),
        2016</em></em> . 1006–1011.</li>
        <li id="BibPLXBIB0019" label="[19]">Tom Kenter and Maarten
        de Rijke. 2015. Short Text Similarity with Word Embeddings.
        In <em><em>Proceedings of the 24th ACM International
        Conference on Information and Knowledge
        Management</em></em> (CIKM 2015). 1411–1420.</li>
        <li id="BibPLXBIB0020" label="[20]">Yoon Kim. 2014.
        Convolutional Neural Networks for Sentence Classification.
        In <em><em>Proceedings of the 2014 Conference on Empirical
        Methods in Natural Language Processing, (EMNLP
        2014)</em></em> . 1746–1751.</li>
        <li id="BibPLXBIB0021" label="[21]">Lingpeng Kong, Nathan
        Schneider, Swabha Swayamdipta, Archna Bhatia, Chris Dyer,
        and Noah&nbsp;A. Smith. 2014. A Dependency Parser for
        Tweets. In <em><em>Proceedings of the 2014 Conference on
        Empirical Methods in Natural Language Processing, (EMNLP
        2014)</em></em> . 1001–1012.</li>
        <li id="BibPLXBIB0022" label="[22]">Alex Lamb,
        Michael&nbsp;J. Paul, and Mark Dredze. 2013. Separating
        Fact from Fear: Tracking Flu Infections on Twitter. In
        <em><em>Human Language Technologies: Conference of the
        North American Chapter of the Association of Computational
        Linguistics, (NAACL 2013)</em></em> . 789–795.</li>
        <li id="BibPLXBIB0023" label="[23]">David Lazer, Alex
        Pentland, Lada Adamic, Sinan Aral, Albert-László Barabási,
        Devon Brewer, Nicholas Christakis, Noshir Contractor, James
        Fowler, Myron Gutmann, Tony Jebara, Gary King, Michael
        Macy, Deb Roy, and Marshall Van&nbsp;Alstyne. 2009.
        Computational Social Science. <em><em>Science</em></em>
        323, 5915 (2009), 721–723.</li>
        <li id="BibPLXBIB0024" label="[24]">Moreno MA, Christakis
        DA, Egan KG, Brockman LN, and Becker T. 2012. Associations
        between displayed alcohol references on facebook and
        problem drinking among college students. <em><em>Archives
        of Pediatrics and Adolescent Medicine</em></em> 166,
        2(2012), 157–163.</li>
        <li id="BibPLXBIB0025" label="[25]">Laurens van&nbsp;der
        Maaten and Geoffrey Hinton. 2008. Visualizing data using
        t-SNE. <em><em>Journal of Machine Learning
        Research</em></em> 9, Nov (2008), 2579–2605.</li>
        <li id="BibPLXBIB0026" label="[26]">Shotaro Matsumoto,
        Hiroya Takamura, and Manabu Okumura. 2005. Sentiment
        Classification Using Word Sub-sequences and Dependency
        Sub-trees. In <em><em>Proceedings of the 9th Pacific-Asia
        Conference on Advances in Knowledge Discovery and Data
        Mining</em></em> (PAKDD’05). Springer-Verlag, Berlin,
        Heidelberg, 301–311.</li>
        <li id="BibPLXBIB0027" label="[27]">Andrew&nbsp;Kachites
        McCallum. 2002. Mallet: A machine learning for language
        toolkit. (2002).</li>
        <li id="BibPLXBIB0028" label="[28]">Tomas Mikolov, Ilya
        Sutskever, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013.
        Distributed Representations of Words and Phrases and Their
        Compositionality. In <em><em>Proceedings of the 26th
        International Conference on Neural Information Processing
        Systems - Volume 2</em></em> (NIPS’13). Curran Associates
        Inc., USA, 3111–3119.</li>
        <li id="BibPLXBIB0029" label="[29]">Tom&nbsp;M. Mitchell.
        1997. <em><em>Machine learning.</em></em> McGraw-Hill
        Boston, MA:.</li>
        <li id="BibPLXBIB0030" label="[30]">Yishai Ofran, Ora
        Paltiel, Dan Pelleg, Jacob&nbsp;M. Rowe, and Elad Yom-Tov.
        2012. Patterns of Information-Seeking for Cancer on the
        Internet: An Analysis of Real World Data. <em><em>PLOS
        ONE</em></em> 7, 9 (09 2012), 1–7.</li>
        <li id="BibPLXBIB0031" label="[31]">Alexandra Olteanu, Emre
        Kiciman, and Carlos Castillo. 2018. A Critical Review of
        Online Social Data: Biases, Methodological Pitfalls, and
        Ethical Boundaries. In <em><em>Proceedings of the Eleventh
        ACM International Conference on Web Search and Data
        Mining</em></em> (WSDM ’18). ACM, New York, NY, USA,
        785–786.</li>
        <li id="BibPLXBIB0032" label="[32]">Michael&nbsp;J. Paul
        and Mark Dredze. 2011. You Are What You Tweet: Analyzing
        Twitter for Public Health. In <em><em>Proceedings of the
        Fifth International Conference on Weblogs and Social Media,
        ICWSM 2011, Barcelona, Catalonia, Spain, July 17-21,
        2011</em></em> . 265–272.</li>
        <li id="BibPLXBIB0033" label="[33]">Michael&nbsp;J. Paul
        and Mark Dredze. 2017. <em><em>Social Monitoring for Public
        Health</em></em> . Morgan &amp; Claypool Publishers.</li>
        <li id="BibPLXBIB0034" label="[34]">Jeffrey Pennington,
        Richard Socher, and Christopher&nbsp;D. Manning. 2014.
        Glove: Global Vectors for Word Representation. In
        <em><em>Proceedings of the 2014 Conference on Empirical
        Methods in Natural Language Processing, EMNLP 2014, October
        25-29, 2014, Doha, Qatar</em></em> . 1532–1543.</li>
        <li id="BibPLXBIB0035" label="[35]">Kyle&nbsp;W. Prier,
        Matthew&nbsp;S. Smith, Christophe Giraud-Carrier, and
        Carl&nbsp;L. Hanson. 2011. Identifying Health-Related
        Topics on Twitter. In <em><em>International Conference on
        Social Computing, Behavioral-Cultural Modeling, and
        Prediction</em></em> , John Salerno, Shanchieh&nbsp;Jay
        Yang, Dana Nau, and Sun-Ki Chai (Eds.). Springer Berlin
        Heidelberg, Berlin, Heidelberg, 18–25.</li>
        <li id="BibPLXBIB0036" label="[36]">Victor&nbsp;M. Prieto,
        Sergio Matos, Manuel Alvarez, Fidel Cacheda, and
        Jose&nbsp;Luis Oliveira. 2014. Twitter: A Good Place to
        Detect Health Conditions. <em><em>PLOS ONE</em></em> 9, 1
        (01 2014), 1–11.</li>
        <li id="BibPLXBIB0037" label="[37]">Marcel Salathe and
        Shashank Khandelwal. 2011. Assessing Vaccination Sentiments
        with Online Social Media: Implications for Infectious
        Disease Dynamics and Control. <em><em>PLOS Computational
        Biology</em></em> 7, 10 (10 2011), 1–7.</li>
        <li id="BibPLXBIB0038" label="[38]">Erich Schubert,
        Alexander Koos, Tobias Emrich, Andreas Züfle,
        Klaus&nbsp;Arthur Schmid, and Arthur Zimek. 2015. A
        Framework for Clustering Uncertain Data. <em><em>Proc. VLDB
        Endowment</em></em> 8, 12 (Aug. 2015), 1976–1979.</li>
        <li id="BibPLXBIB0039" label="[39]">Richard Socher, Danqi
        Chen, Christopher&nbsp;D. Manning, and Andrew&nbsp;Y. Ng.
        2013. Reasoning With Neural Tensor Networks for Knowledge
        Base Completion. In <em><em>Advances in Neural Information
        Processing Systems 26: 27th Annual Conference on Neural
        Information Processing Systems 2013. Proceedings of a
        meeting held December 5-8, (NIPS 2013), 2013.</em></em>
        926–934.</li>
        <li id="BibPLXBIB0040" label="[40]">Damiano Spina, Julio
        Gonzalo, and Enrique Amigó. 2013. Discovering Filter
        Keywords for Company Name Disambiguation in Twitter.
        <em><em>Expert Syst. Appl.</em></em> 40, 12 (Sept. 2013),
        4986–5003.</li>
        <li id="BibPLXBIB0041" label="[41]">Duyu Tang, Bing Qin,
        and Ting Liu. 2015. Document Modeling with Gated Recurrent
        Neural Network for Sentiment Classification. In
        <em><em>Proceedings of the 2015 Conference on Empirical
        Methods in Natural Language Processing, (EMNLP 2015),
        2015</em></em> . 1422–1432.</li>
        <li id="BibPLXBIB0042" label="[42]">Amir&nbsp;Hossein
        Yazdavar, Hussein&nbsp;S. Al-Olimat, Monireh Ebrahimi,
        Goonmeet Bajaj, Tanvi Banerjee, Krishnaprasad Thirunarayan,
        Jyotishman Pathak, and Amit Sheth. 2017. Semi-Supervised
        Approach to Monitoring Clinical Depressive Symptoms in
        Social Media. In <em><em>Proceedings of the 2017 IEEE/ACM
        International Conference on Advances in Social Networks
        Analysis and Mining 2017</em></em> (ASONAM ’17). ACM, New
        York, NY, USA, 1191–1198.</li>
        <li id="BibPLXBIB0043" label="[43]">Zhijun Yin, Daniel
        Fabbri, Trent&nbsp;S. Rosenbloom, and Bradley Malin. 2015.
        A Scalable Framework to Detect Personal Health Mentions on
        Twitter. <em><em>Journal of Medical Internet
        Research</em></em> 17, 6 (05 Jun 2015), e138.</li>
        <li id="BibPLXBIB0044" label="[44]">Mo Yu, Tiejun Zhao,
        Daxiang Dong, Hao Tian, and Dianhai Yu. 2013. Compound
        Embedding Features for Semi-supervised Learning. In
        <em><em>Human Language Technologies: Conference of the
        North American Chapter of the Association of Computational
        Linguistics, (NAACL 2013), 2013</em></em> . 563–568.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>The dataset and
    code are available at&nbsp; <a class="link-inline force-break"
    href=
    "https://github.com/emory-irlab/PHM2017">https://github.com/emory-irlab/PHM2017</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Available
    at&nbsp;<a class="link-inline force-break" href=
    "https://lld-workshop.github.io/">https://lld-workshop.github.io/</a></p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>Given the
    regular limitations in using social network API to retrieve the
    user postings, accessing the previous and next messages of the
    user might be problematic, specifically in the real-time
    large-scale applications.</p>
    <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>In our
    development experiments stemming and stopword removal was not
    helpful.</p>
    <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>Available
    at&nbsp;<a class="link-inline force-break" href=
    "https://code.google.com/archive/p/word2vec/">https://code.google.com/archive/p/word2vec/</a>.</p>
    <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>We also
    experimented with an SVM classifier with linear kernel, and
    initially achieved slightly better results on development data.
    However, the improvement came at the cost of higher training
    time, therefore, we opted to stay with the simpler logistic
    regression model.</p>
    <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a>Available
    at&nbsp;<a class="link-inline force-break" href=
    "https://github.com/harvardnlp/sent-conv-torch">https://github.com/harvardnlp/sent-conv-torch</a></p>
    <p id="fn8"><a href="#foot-fn8"><sup>8</sup></a>For simplicity
    in the grid search we set <em>α</em> = <em>α</em>
    <sub>2</sub>.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186055">https://doi.org/10.1145/3178876.3186055</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
