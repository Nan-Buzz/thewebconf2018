<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content="HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>AdaError: An Adaptive Learning Rate Method for Matrix Approximation-based Collaborative Filtering</title><!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content="text/html; charset=utf-8" />
  <meta name="viewport" content="width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href="../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>
  <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
  <link rel="cite-as" href="https://doi.org/10.1145/3178876.3186155"/>
</head>
<body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a web copy of <a href='https://doi.org/10.1145/3178876.3186155'>https://doi.org/10.1145/3178876.3186155</a>.
 Published in WWW2018 Proceedings © 2018 International World Wide Web Conference Committee, published under 
 <a rel='license' property='license' href='https://creativecommons.org/licenses/by/4.0/'>
 Creative Commons CC By 4.0 License</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML aiming to make it Findable, Accessible, Interoperable and Reusable. 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3178876.3186155'>https://w3id.org/oa/10.1145/3178876.3186155</a>
</p></div>
<hr>


  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">AdaError: An Adaptive Learning Rate Method for Matrix Approximation-based Collaborative Filtering</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Dongsheng</span> <span class="surName">Li</span>, IBM Research - China, Shanghai, China, <a href="mailto:ldsli@cn.ibm.com">ldsli@cn.ibm.com</a>
        </div>
        <div class="author">
          <span class="givenName">Chao</span> <span class="surName">Chen</span>, IBM Research - China, Shanghai, China, <a href="mailto:cchao@cn.ibm.com">cchao@cn.ibm.com</a>
        </div>
        <div class="author">
          <span class="givenName">Qin</span> <span class="surName">Lv</span>, University of Colorado Boulder, Boulder, CO, USA, <a href="mailto:qin.lv@colorado.edu">qin.lv@colorado.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Hansu</span> <span class="surName">Gu</span>, Seagate Technology, Longmont, CO, USA, <a href="mailto:guhansu@gmail.com">guhansu@gmail.com</a>
        </div>
        <div class="author">
          <span class="givenName">Tun</span> <span class="surName">Lu</span>, Fudan University, Shanghai, China, <a href="mailto:lutun@fudan.edu.cn">lutun@fudan.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Li</span> <span class="surName">Shang</span>, University of Colorado Boulder, Boulder, CO, USA, <a href="mailto:li.shang@colorado.edu">li.shang@colorado.edu</a>
        </div>
        <div class="author">
          <span class="givenName">Ning</span> <span class="surName">Gu</span>, Fudan University, Shanghai, China, <a href="mailto:ninggu@fudan.edu.cn">ninggu@fudan.edu.cn</a>
        </div>
        <div class="author">
          <span class="givenName">Stephen M.</span> <span class="surName">Chu</span>, IBM Research - China, Shanghai, China, <a href="mailto:schu@cn.ibm.com">schu@cn.ibm.com</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186155" target="_blank">https://doi.org/10.1145/3178876.3186155</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Gradient-based learning methods such as stochastic gradient descent are widely used in matrix approximation-based collaborative filtering algorithms to train recommendation models based on observed user-item ratings. One major difficulty in existing gradient-based learning methods is determining proper learning rates, since model convergence would be inaccurate or very slow if the learning rate is too large or too small, respectively. This paper proposes AdaError, an adaptive learning rate method for matrix approximation-based collaborative filtering. AdaError eliminates the need of manually tuning the learning rates by adaptively adjusting the learning rates based on the noisiness level of user-item ratings, using smaller learning rates for noisy ratings so as to reduce their impact on the learned models. Our theoretical and empirical analysis shows that AdaError can improve the generalization performance of the learned models. Experimental studies on the MovieLens and Netflix datasets also demonstrate that AdaError outperforms state-of-the-art adaptive learning rate methods in matrix approximation-based collaborative filtering. Furthermore, by applying AdaError to the standard matrix approximation method, we can achieve statistically significant improvements over state-of-the-art collaborative filtering methods in both rating prediction accuracy and top-N recommendation accuracy.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS Concepts:</span> • <strong>Information systems</strong> → <strong>Collaborative filtering;</strong> <strong>Recommender systems;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style="font-weight:bold;"><small>Keywords:</small></span> <span class="keyword"><small>collaborative filtering</small>,</span> <span class="keyword"><small>matrix approximation</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference Format:</span><br />
          Dongsheng Li, Chao Chen, Qin Lv, Hansu Gu, Tun Lu, Li Shang, Ning Gu, and Stephen M. Chu. 2018. AdaError: An Adaptive Learning Rate Method for Matrix Approximation-based Collaborative Filtering. In <em>WWW 2018: The 2018 Web Conference,</em> <em>April 23–27, 2018,</em> <em>Lyon, France</em>. ACM, New York, NY, USA, 11 pages. <a href="https://doi.org/10.1145/3178876.3186155" class="link-inline force-break" target="_blank">https://doi.org/10.1145/3178876.3186155</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span> Introduction</h2>
        </div>
      </header>
      <p>Matrix approximation (MA) methods have become increasingly popular among existing collaborative filtering (CF)-based solutions due to their superior accuracy&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0046">46</a>]. In MA-based CF algorithms, gradient-based learning methods such as stochastic gradient descent (SGD) are widely adopted to learn MA models based on observed user-item ratings&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>]. The learned MA models are then used to predict user ratings on unseen items. One major difficulty in existing gradient-based learning methods is determining proper learning rates for gradient descent&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0017">17</a>], since the model would diverge if the learning rate is too large and the model convergence would be very slow if the learning rate is too small.</p>
      <p>Recently, several adaptive learning rate methods, such as Adagrad&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>], AdaDelta&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>], and Adam&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>], have been proposed to address the learning rate issue, and have achieved good performance in several algorithms, especially neural networks&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>]. In general, existing adaptive learning rate methods aim to improve model convergence on sparse data by increasing gradient updates for infrequent parameters and decreasing gradient updates for frequent parameters. However, in real-world recommender systems, the observed user-item ratings are not only very sparse but also very noisy&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0043">43</a>]. A recent study&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] showed that only 60% of user ratings are unchanged when users are asked to re-rate the same items, and such rating noises can lead to 40% variation in recommendation RMSE&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>]. Therefore, it is important to consider rating noises when choosing the learning rates in MA-based CF solutions, i.e., performing small gradient updates for noisy ratings to prevent the learned models from overreacting to rating noises.</p>
      <p>To this end, this paper proposes AdaError — an adaptive learning rate method for matrix approximation-based collaborative filtering. AdaError reduces the learning rates for noisy training examples so that the learned models are less prone to the noisy ratings in the training data. AdaError also adaptively shrinks the learning rates as the number of epochs increases, thus eliminating the need of manually tuning the learning rates. Our theoretical and empirical analysis shows that AdaError can improve the generalization performance of learned MA models and are less sensitive to <em>L</em> <sub>2</sub> regularization coefficients. Experimental studies using the MovieLens and Netflix datasets demonstrate that AdaError outperforms state-of-the-art adaptive learning rate methods in matrix approximation-based collaborative filtering. Furthermore, by applying AdaError to the standard matrix approximation method, we can statistically significantly improve recommendation accuracy for both the rating prediction task and the top-N recommendation task, compared with state-of-the-art collaborative filtering methods.</p>
    </section>
    <section id="sec-10">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Problem Formulation</h2>
        </div>
      </header>
      <p>In this section, we first introduce the basic concepts of matrix approximation-based collaborative filtering. Then, we analyze the noisy rating problem in real-world recommender systems. At last, we motivate the targeted problem through a case study.</p>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Matrix Approximation-based Collaborative Filtering</h3>
          </div>
        </header>
        <p>Given a user-item rating matrix <span class="inline-equation"><span class="tex">$R\in \mathbb {R}^{m\times n}$</span></span> , where <em>m</em> is the number of users and <em>n</em> is the number of items, matrix approximation methods aim to determine a user feature matrix <span class="inline-equation"><span class="tex">$U\in \mathbb {R}^{m\times k}$</span></span> and an item feature matrix <span class="inline-equation"><span class="tex">$V\in \mathbb {R}^{n\times k}$</span></span> , such that</p>
        <div class="table-responsive" id="eq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} R\approx \hat{R} = UV^T. \end{equation}</span><br />
            <span class="equation-number">(1)</span>
          </div>
        </div><em>k</em>, the rank of <em>R</em>, is typically much smaller than <em>m</em> and <em>n</em> in real-world recommender systems. After obtaining <em>U</em> and <em>V</em>, the predicted rating of the <em>i</em>-th user on the <em>j</em>-th item can be computed by the dot product of their corresponding feature vectors, i.e., <span class="inline-equation"><span class="tex">$\hat{r}_{i,j} = U_iV_j^T$</span></span> .
        <p></p>
        <p>To obtain optimal <em>U</em> and <em>V</em> in Equation&nbsp;<a class="eqn" href="#eq1">1</a>, gradient-based learning methods such as stochastic gradient descent (SGD) can be adopted to minimize the following regularized least square error problem&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>]:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} L=\sum _{i,j\in \Omega }(R_{i,j}-U_iV_j^T)^2 + \mu ||U||_F^2 + \mu ||V||_F^2, \end{equation}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>where <em>Ω</em> is the set of observed entries in the rating matrix <em>R</em> and || · || <sub><em>F</em></sub> is the Frobenius norm. When using SGD to solve the minimization problem above at entry <em>R</em> <sub><em>i</em>, <em>j</em></sub> , the gradient update rules can be described as follows:
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} U_i \leftarrow U_i - \lambda \frac{\partial L}{\partial U_i},~~ V_j \leftarrow V_j - \lambda \frac{\partial L}{\partial V_j}. \end{eqnarray}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div><em>λ</em> is the learning rate, which controls the convergence of the model learning process.
        <p></p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Noisy Ratings in Recommender Systems</h3>
          </div>
        </header>
        <p>User-item ratings in real-world recommender systems are typically noisy&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>]. A recent work&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0033">33</a>] pointed out that <em>natural noise</em>, which arises when recommender systems collect or infer user preferences, commonly exist in today's recommender system databases. The natural noises are caused by various reasons, including difficulty for users to quantify their preferences&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>], inappropriate granularity of rating scales&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>], memory loss due to the long time between seeing and rating items&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>], bad mood&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>], etc. These natural noises in user-item ratings are inevitable due to so many complex reasons. As such, matrix approximation-based CF methods should consider rating noises in their algorithm design.</p>
        <p>The fraction of noisy ratings in recommender systems is large according to recent studies&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>]. Cosley&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] found that about 40% of user ratings are different from their previous ratings when users are asked to re-rate the same movies they rated before. Similarly, Jones&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] found that user stability on rating items is around 63% in their study. These noisy ratings can significantly influence the accuracy of matrix approximation-based CF methods&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>]. Cosley&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0010">10</a>] observed significant MAE differences when using collaborative filtering algorithms on users’ original ratings and new ratings. Amatriain&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0001">1</a>] found that the recommendation RMSE variations can be as high as 40% when noises exist in the user-item rating matrix. Therefore, if we can properly address the noisy ratings issue, it is very promising for us to improve the recommendation accuracy of collaborative filtering methods.</p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Case Study: Noise vs. Learning Rate</h3>
          </div>
        </header>
        <p>Here, we conduct a case study to empirically investigate the potential improvement in recommendation accuracy when considering rating noises in the model learning process. The study is carried out on the MovieLens 1M dataset, which contains ∼ 10<sup>6</sup> ratings from 6k users on 4k items. The basic idea is to use larger learning rates for robust ratings and smaller learning rates for noisy ratings, thus reducing the impact of noisy ratings on the learned model. However, rating noises are difficult to quantify. So we rely on the learned MA models to identify noisy ratings. Specifically, we assume that a rating is noisy if the learned MA model cannot fit the rating accurately, i.e., a rating is noisy if the learned MA model has a large training error on this rating.</p>
        <p>Based on the idea above, given a predefined learning rate <em>λ</em> and a training example <em>R</em> <sub><em>i</em>, <em>j</em></sub> , we adopt the following adaptive learning rate: 1) if the training example's prediction error <span class="inline-equation"><span class="tex">$(R_{i,j}-\hat{R}_{i,j})^2$</span></span> is larger than the average square error of all training examples, then its learning rate is decreased by 10%, i.e., 0.9<em>λ</em>; 2) otherwise, its learning rate is increased by 10%, i.e., 1.1<em>λ</em>. As shown in Figure&nbsp;<a class="fig" href="#fig1">1</a>, RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] with adaptive learning rate outperforms RSVD with fixed learning rate in recommendation accuracy, i.e., achieving lower Root Mean Square Error (RMSE). And this is true for rank variations from 10 to 50. The only difference between the two methods is that RSVD with adaptive learning rate can use smaller updates for entries with large training error (noisy ratings) and larger updates for entries with small training error (robust ratings). As a result, the learned MA models are less prone to noises in the ratings.</p>
        <figure id="fig1">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig1.jpg" class="img-responsive" alt="Figure 1" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 1:</span> <span class="figure-title">Case study: Recommendation accuracy comparison between RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] with adaptive / fixed learning rate when varying rank values on the MovieLens 1M dataset. We use 0.01 as the fixed learning rate. For adaptive learning rate, we use 0.011 for entries with small training error and 0.009 for entries with large training error.</span>
          </div>
        </figure>
        <p>This case study confirms that we can improve the recommendation accuracy of MA-based CF methods by considering rating noises in the model learning process. The key question is – how to design an intelligent adaptive learning rate method which can address the different levels of noises across different ratings for MA-based CF solutions.</p>
      </section>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Algorithm Design</h2>
        </div>
      </header>
      <p>In this section, we first propose the AdaError method, which can adaptively adjust the learning rates for entries with different noise levels. Then, we present how to apply the proposed AdaError method in matrix approximation for two collaborative filtering tasks: rating prediction and top-N recommendation.</p>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span> The Proposed AdaError Method</h3>
          </div>
        </header>
        <p>AdaError is designed based on the following idea: Entries with larger training errors should be given smaller learning rates and entries with smaller training errors should be given larger learning rates. This idea may be implemented in different ways. Here, we propose an adaptive method which is similar to AdaGrad&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>]. Given a predefined learning rate <em>λ</em> and an observed entry <em>R</em> <sub><em>i</em>, <em>j</em></sub> ∈ <em>Ω</em>, its learning rate at the <em>t</em>-th iteration is defined as follows:</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \lambda _{i,j}^{(t)} = \frac{\lambda }{\sqrt {E_{i,j}^{(t-1)} + \epsilon }} + \beta . \end{equation}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div><span class="inline-equation"><span class="tex">$E_{i,j}^{(t-1)} = \sum _{x=0}^{t-1}(R_{i,j}-\hat{R}_{i,j}^{(x)})^2$</span></span> is the sum of squared training error w.r.t. <em>R</em> <sub><em>i</em>, <em>j</em></sub> up to the (<em>t</em> − 1)-th iteration. ϵ is a small constant to prevent 0 in the denominator, which is set to 1<em>e</em>-8 in this paper. <em>β</em> is a constant to prevent <span class="inline-equation"><span class="tex">$\lambda _{i,j}^{(t)}$</span></span> from becoming infinitely small after a large number of iterations, which is set to 1<em>e</em>-4 in this paper.
        <p></p>
        <p>The advantages of the proposed AdaError method are summarized as follows:</p>
        <ul class="list-no-style">
          <li id="uid10" label="•"><em>Addressing different levels of noises</em>: The learning rates of different entries will vary according to their training errors, so that entries with different cumulative training errors will have varying learning rates;<br /></li>
          <li id="uid11" label="•"><em>Adaptive tuning of the learning rates</em>: The learning rates will shrink as the number of iterations increases, so that it is unnecessary to manually tune the learning rates. Meanwhile, the adoption of <em>β</em> can prevent the learning rates from becoming infinitely small, so that the learning process will stop within an acceptable number of iterations; and<br /></li>
          <li id="uid12" label="•">
            <em>Efficiency</em>: The proposed AdaError method is entry-wise, i.e., different entries will have different learning rates. This is different from many existing parameter-wise adaptive learning rate methods, e.g., AdaGrad&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>], AdaDelta&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>] and Adam&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>], in which the learning rates are different across different parameters. For matrix approximation, the computation complexity of AdaError, which is <em>O</em>(|<em>Ω</em>|) per iteration, is smaller than that of those parameter-wise adaptive learning rate methods, which is <em>O</em>(<em>k</em>|<em>Ω</em>|) per iteration.<br />
          </li>
        </ul>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> AdaError for Rating Prediction</h3>
          </div>
        </header>
        <p>Here, we present the algorithm design of applying the proposed AdaError method for matrix approximation in the rating prediction task, in which we solve the minimization problem defined by Equation&nbsp;<a class="eqn" href="#eq2">2</a>. We first initialize the parameters and <span class="inline-equation"><span class="tex">$E\in \mathbb {R}^{m\times n}$</span></span> . Then, we iteratively update the parameters and <em>E</em> <sup>(<em>t</em>)</sup> (<em>t</em> &gt; 0) until convergence using stochastic gradient descent. The details are presented in Algorithm&nbsp;1 .</p>
        <p><img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-img1.svg" class="img-responsive" alt="" longdesc="" /></p>
      </section>
      <section id="sec-17">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> AdaError for Top-N Recommendation</h3>
          </div>
        </header>
        <p>Here, we present the algorithm design of applying the proposed AdaError method for matrix approximation in the top-N recommendation task. In many top-N recommendation tasks, the user-item ratings are binary, i.e., <em>R</em> <sub><em>i</em>, <em>j</em></sub> ∈ { − 1, 1}. The mean square loss as defined in Equation&nbsp;<a class="eqn" href="#eq2">2</a> will not be appropriate in such setting&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>]. Therefore, we adopt the “0-1” loss with exponential surrogate function. Note that other surrogate loss functions&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0042">42</a>], such as square loss, log loss, and hinge loss, can also be adopted in Equation&nbsp;<a class="eqn" href="#eq5">5</a>. However, many real-world datasets only provide positive feedbacks, e.g., click through data, which is also known as implicit feedback data. To address the implicit feedback issue, we give a larger weight to positive ratings and a smaller weight to negative ratings following the WRMF method&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>]. We set <em>w</em> <sub><em>i</em>, <em>j</em></sub> = 1 if <em>R</em> <sub><em>i</em>, <em>j</em></sub> = 1 and <em>w</em> <sub><em>i</em>, <em>j</em></sub> = 0.04 if <em>R</em> <sub><em>i</em>, <em>j</em></sub> = −1 based on our empirical study. Finally, the loss function for top-N recommendation can be defined as follows:</p>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} L^{\prime } = \sum _{(i,j)\in \Omega } w_{i,j}\exp \lbrace -R_{i,j}\hat{R}_{i,j}\rbrace + \mu ||U||_F^2 + \mu ||V||_F^2. \end{equation}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <p></p>
        <p>Then, we can similarly solve the above minimization problem as in the rating prediction problem. The details are presented in Algorithm&nbsp;2 .</p>
        <p><img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-img2.svg" class="img-responsive" alt="" longdesc="" /></p>
      </section>
    </section>
    <section id="sec-18">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Theoretical Analysis</h2>
        </div>
      </header>
      <p>In this section, we first analyze the convergence rate of applying AdaError in SGD for solving strongly convex problems. Then, we analyze the generalization error bound of AdaError-based SGD, and compare it with standard SGD.</p>
      <section id="sec-19">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span> Convergence Rate</h3>
          </div>
        </header>
        <p>The convergence rate of SGD has been extensively studied in the literature and a recent result showed that the SGD algorithm can return a solution which is <em>O</em>(1/<em>T</em>)-close to the optimum after <em>T</em> iterations&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]. More formally, the convergence rate of SGD can be analyzed as follows&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0014">14</a>]:</p>
        <div class="theorem" id="enc1">
          <label>Theorem 4.1.</label>
          <p>Assuming that the loss function <em>L</em> is <em>l</em>-strongly convex and its gradients satisfy that <span class="inline-equation"><span class="tex">$\mathbb {E}(||g||^2)\le G^2$</span></span> for all model <em>w</em> ∈ <em>Φ</em>, where <span class="inline-equation"><span class="tex">$\mathbb {E}(g) = \nabla L(w)$</span></span> . Then, there exists a deterministic algorithm that can return a <em>w</em> after at most <em>T</em> iterations such that, for optimal <em>w</em> <sup>*</sup> ∈ <em>Φ</em>, we have <span class="inline-equation"><span class="tex">$\mathbb {E}[L(w)] - L(w^*)\le O(\frac{G^2}{lT})$</span></span> .</p>
        </div>
        <p>Following the above results, we can similarly derive the convergence rate of AdaError-based SGD in the following Theorem&nbsp;<a class="enc" href="#enc2">4.2</a>.</p>
        <div class="theorem" id="enc2">
          <label>Theorem 4.2.</label>
          <p>Assuming that the loss function <em>L</em> is <em>l</em>-strongly convex and its gradients satisfy that <span class="inline-equation"><span class="tex">$\mathbb {E}(||g||^2)\le G^2$</span></span> for all model <em>w</em> ∈ <em>Φ</em>, where <span class="inline-equation"><span class="tex">$\mathbb {E}(g) = \nabla L(w)$</span></span> . Then, by properly choosing <em>λ</em> and <em>β</em> in Equation&nbsp;<a class="eqn" href="#eq4">4</a> for each iteration, AdaError-based SGD can return a <em>w</em> after at most <em>T</em> iterations such that, for optimal <em>w</em> <sup>*</sup> ∈ <em>Φ</em>, we have <span class="inline-equation"><span class="tex">$\mathbb {E}[L(w)] - L(w^*)\le O(\frac{G^2}{lT})$</span></span> .</p>
        </div>
        <p>Theorem&nbsp;<a class="enc" href="#enc2">4.2</a> proves that solving a strongly convex loss using AdaError-based SGD can achieve a convergence rate of <em>O</em>(1/<em>T</em>). It is easy to verify that mean square loss (Equation&nbsp;<a class="eqn" href="#eq2">2</a>) is strongly convex, because its second order derivative is a constant 2. The exponential loss (Equation&nbsp;<a class="eqn" href="#eq5">5</a>) is not always strongly convex, because its second order derivative <em>e<sup>x</sup></em> → 0 when <em>x</em> → −∞. However, <span class="inline-equation"><span class="tex">$-R_{i,j}\hat{R}_{i,j}$</span></span> will not diverge to − ∞ in matrix approximation if the learning rate is properly set in SGD, so we can assume that <span class="inline-equation"><span class="tex">$-R_{i,j}\hat{R}_{i,j}\ge C$</span></span> for all (<em>i</em>, <em>j</em>) ∈ <em>Ω</em> with some properly chosen learning rate. Then, Equation&nbsp;<a class="eqn" href="#eq5">5</a> will satisfy the strongly convex assumption.</p>
      </section>
      <section id="sec-20">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span> Generalization Error Bound</h3>
          </div>
        </header>
        <p>The generalization performance of matrix approximation would be poor if the learned MA models are prone to the noisy training data. Since AdaError can prevent the learned MA models from overreacting to the noises in the training data, AdaError can naturally improve the generalization performance. Here, we theoretically analyze the generalization error bound of AdaError-based SGD.</p>
        <p>Uniform stability&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0006">6</a>] is adopted to analyze the generalization error of SGD. The expected generalization error of SGD with fixed learning rate can be bounded as follows&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0013">13</a>]:</p>
        <div class="theorem" id="enc3">
          <label>Theorem 4.3.</label>
          <p>Given a loss function <span class="inline-equation"><span class="tex">$L: \Phi \rightarrow \mathbb {R}$</span></span> , assuming <em>L</em>(· ; <em>x</em>) is convex, ||∇<em>L</em>(· ; <em>x</em>)|| ≤ <em>P</em> and ||∇<em>L</em>(<em>w</em>; <em>x</em>) − ∇<em>L</em>(<em>w</em>′; <em>x</em>)|| ≤ <em>b</em>||<em>w</em> − <em>w</em>′|| for all training example <em>x</em> ∈ <em>X</em> and any two models <em>w</em>, <em>w</em>′ ∈ <em>Φ</em>. Suppose that we run SGD with the <em>t</em>-th step size <em>λ</em> ≤ 2/<em>b</em> for totally <em>T</em> steps. Then, SGD satisfies uniform stability on samples with <em>n</em> examples by <span class="inline-equation"><span class="tex">$\epsilon _{stab}\le \frac{2P^2}{n}\sum _{t=1}^T{\lambda }$</span></span> .</p>
        </div>
        <p>Following the above results, we can similarly derive the generalization error bound of AdaError-based SGD in the following Theorem&nbsp;<a class="enc" href="#enc4">4.4</a>.</p>
        <div class="theorem" id="enc4">
          <label>Theorem 4.4.</label>
          <p>Given a loss function <span class="inline-equation"><span class="tex">$L: \Phi \rightarrow \mathbb {R}$</span></span> , assuming <em>L</em>(· ; <em>x</em>) is convex, ||∇<em>L</em>(· ; <em>x</em>)|| ≤ <em>P</em> and ||∇<em>L</em>(<em>w</em>; <em>x</em>) − ∇<em>L</em>(<em>w</em>′; <em>x</em>)|| ≤ <em>b</em>||<em>w</em> − <em>w</em>′|| for all training example <em>x</em> ∈ <em>X</em> and any two models <em>w</em>, <em>w</em>′ ∈ <em>Φ</em>. Suppose that we run AdaError-based SGD with the <em>t</em>-th step size as defined in Equation&nbsp;<a class="eqn" href="#eq4">4</a> satisfying <em>λ</em> <sup>(<em>t</em>)</sup> ≤ 2/<em>b</em> for totally <em>T</em> steps. Then, AdaError-based SGD satisfies uniform stability on samples with <em>n</em> examples by <span class="inline-equation"><span class="tex">$\epsilon _{stab}\le \frac{2P^2}{n}\sum _{t=1}^T{\lambda ^{(t)}}$</span></span> .</p>
        </div>
        <div class="proof" id="proof1">
          <label>Proof.</label>
          <p>The proof can be derived from Theorem&nbsp;<a class="enc" href="#enc3">4.3</a>.</p>
        </div>
        <p>Next, we can compare the generalization error bound of SGD with fixed learning rate and AdaError-based SGD in the following Theorem&nbsp;<a class="enc" href="#enc5">4.5</a>.</p>
        <div class="theorem" id="enc5">
          <label>Theorem 4.5.</label>
          <p>The uniform stability bound of Theorem&nbsp;<a class="enc" href="#enc4">4.4</a> will be sharper than that of Theorem&nbsp;<a class="enc" href="#enc3">4.3</a> if <span class="inline-equation"><span class="tex">$\sum _t\frac{1}{T\sqrt {E^{(t)}+\epsilon }} \le 1 - {\beta }/{\lambda }$</span></span> .</p>
        </div>
        <div class="proof" id="proof2">
          <label>Proof.</label>
          <p>The sharper uniform stability bound of Theorem&nbsp;<a class="enc" href="#enc4">4.4</a> indicates that <span class="inline-equation"><span class="tex">$\frac{2P^2}{n}\sum _{t=1}^T{\lambda ^{(t)}} \le \frac{2P^2}{n}\sum _{t=1}^T{\lambda }$</span></span> , i.e., <span class="inline-equation"><span class="tex">$\sum _{t=1}^T{\lambda ^{(t)}}\le \sum _{t=1}^T{\lambda }$</span></span> . Based on Equation&nbsp;<a class="eqn" href="#eq4">4</a>, we know that <span class="inline-equation"><span class="tex">$\sum _{t=1}^T{(\lambda /\sqrt {E^{(t)}+\epsilon } + \beta)}\le \sum _{t=1}^T{\lambda }$</span></span> . Then, by simple algebra, we can complete the proof.</p>
        </div>
        <p>In AdaError, <em>E</em> <sup>(<em>t</em>)</sup> will accumulate as the number of iterations increases, so the above condition can be easily met when <em>T</em> is large enough, e.g., <em>T</em> &gt; 100 will be fair enough in our empirical studies.</p>
      </section>
    </section>
    <section id="sec-21">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Experiments</h2>
        </div>
      </header>
      <p>This section first presents the experimental setup. Then, we analyze the sensitivity of AdaError and compare it with other adaptive learning rate methods. At last, we compare the recommendation accuracy of AdaError-based matrix approximation method with state-of-the-art CF methods in both rating prediction and top-N recommendation tasks.</p>
      <section id="sec-22">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.1</span> Experimental Setup</h3>
          </div>
        </header>
        <p><em>Dataset Description.</em>. The following real-world datasets are used in the experiments: 1) MovieLens 100K dataset (∼ 10<sup>5</sup> ratings from 1,000 users on 1,700 movies); 2) MovieLens 1M dataset (∼ 10<sup>6</sup> ratings from 6,000 users on 4,000 movies); 3) MovieLens 10M dataset (∼ 10<sup>7</sup> ratings from 72,000 users on 10,000 movies); and 4) Netflix Prize dataset (∼ 10<sup>8</sup> ratings from 480,000 users on 17,770 movies). In each experiment, we randomly split the dataset into training and test sets and keep the ratio as 90% : 10%. All reported results are averaged over five different rounds of random splits. Note that, for top-N recommendation, we predict whether a user will rate an item&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0019">19</a>], i.e., the user rating on an item will be 1 if the user rated the item and -1 otherwise.</p>
        <p><em>Parameter Setting.</em>. We set <em>λ</em> = 0.01 and <em>β</em> = 1<em>e</em>-4 in Equation&nbsp;<a class="eqn" href="#eq4">4</a> if not explicitly specified. The regularization coefficient <em>μ</em> is set to 0.02 for rating prediction and 0.001 for top-N recommendation. The convergence threshold is set to 1<em>e</em>-5 and the maximum number of epochs is set to 1000. The optimal parameters of the compared methods are chosen from their original papers.</p>
        <p><em>Compared Methods.</em>. For the rating prediction task, we compare the proposed method with the following state-of-the-art MA-based CF methods: 1) BPMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>] is a Bayesian extension of the PMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>] method, which can automatically control model capacity by integrating over all model parameters and hyperparameters; 2) DFC&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>] can improve the scalability and accuracy of matrix factorization by a divide and conquer-based ensemble strategy; 3) LLORMA&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>] is an ensemble MA method, which integrates a set of localized MA models through kernel smoothing; 4) GSMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>] can model multiple user behaviors through group sparsity regularization in matrix factorization; 5) WEMAREC&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>] is also an ensemble method, which integrates biased co-clustering-based MA models by weighted average; 6) SMA&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] can improve the stability of matrix approximation by introducing hard-predictable terms in the loss function; and 7) ERMMA&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] can minimize the expected risk in learning MA models.</p>
        <p>For the top-N recommendation task, we compare the proposed method with the following methods: 1) WRMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>] assigns point-wise confidences to individual ratings in the user-item rating matrix to address the implicit feedback issue; 2) BPR&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>] learns a pair-wise loss to optimize ranking measures for top-N setting. They proposed different versions of BPR methods, and this paper compares with the BPR-MF; 3) AOBPR&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>] improves the original BPR method by oversampling informative pairs to speed up convergence and accuracy; and 4) SLIM&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>] generates top-N recommendations by aggregating weighted user ratings learned by solving an <em>L</em> <sub>1</sub> and <em>L</em> <sub>2</sub> regularized optimization problem.</p>
        <p>In addition, we compare AdaError with the following popular adaptive learning rate methods: 1) AdaGrad&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] can adjust the learning rates so that frequently updated parameters will be given smaller learning rates and infrequently updated parameters will be given larger learning rates; 2) RMSprop<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> divides the learning rate with a running average of the magnitudes of recent gradients to prevent the learning rates from becoming infinitely small; and 3) Adam&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>] adjusts the learning rates of individual parameters by considering the first moment and the second moment of the gradients.</p>
        <p><em>Evaluation Metrics.</em>. For the rating prediction task, root mean square error (RMSE) is adopted to measure recommendation accuracy: RMSE(<span class="inline-equation"><span class="tex">$\hat{R}$</span></span> ) = <span class="inline-equation"><span class="tex">$\sqrt {1/|\Omega ^{\prime }|\sum _{(i,j)\in \Omega ^{\prime }}(R_{i,j}-\hat{R}_{i,j})^2}$</span></span> , where <em>Ω</em>′ is the set of entries in the test set. Lower RMSE indicates higher rating prediction accuracy. For the top-N recommendation task, two popular measures are adopted: 1) Precision@N = |<em>I<sub>r</sub></em> ∩<em>I<sub>u</sub></em> |/|<em>I<sub>r</sub></em> |, where <em>I<sub>r</sub></em> is the list of top-N recommendations and <em>I<sub>u</sub></em> is the list of items that <em>u</em> has rated; 2) NDCG@N = DCG@N/IDCG@N, where DCG@N =<span class="inline-equation"><span class="tex">$\sum _{i=1}^n{(2^{rel_i}-1)/log_2(i+1)}$</span></span> and IDCG@N is the value of DCG@N with perfect ranking (<em>rel<sub>i</sub></em> = 1 if <em>u</em> rated the <em>i</em>-th recommended item and <em>rel<sub>i</sub></em> = 0 otherwise). Higher Precision@N and NDCG@N indicate higher recommendation accuracy.</p>
      </section>
      <section id="sec-23">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.2</span> Generalization Error Analysis</h3>
          </div>
        </header>
        <p>Figure&nbsp;<a class="fig" href="#fig2">2</a> compares the gap between training and test errors of RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] using standard SGD and RSVD with AdaError on the MovieLens 10M dataset. As we can see from the results, RSVD with standard SGD has a much larger gap between training and test errors than that of RSVD with AdaError, i.e., RSVD with AdaError can achieve better generalization performance, which confirms the theoretical analysis in Theorem&nbsp;<a class="enc" href="#enc4">4.4</a> and Theorem&nbsp;<a class="enc" href="#enc5">4.5</a> that AdaError can achieve sharper uniform stability bound when the number of epochs is large enough.</p>
        <figure id="fig2">
          <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig2.jpg" class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span> <span class="figure-title">Training and test errors vs. epochs of RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] using standard SGD and AdaError on the MovieLens 10M dataset. We set rank <em>k</em> = 100 and <em>L</em> <sub>2</sub> regularization coefficient <em>μ</em> = 0.02 for both methods.</span>
          </div>
        </figure>
      </section>
      <section id="sec-24">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.3</span> Sensitivity Analysis</h3>
          </div>
        </header>
        <p>Here, we analyze the sensitivity of AdaError with different hyperparameters, and compare AdaError with three popular adaptive learning rate methods: AdaGrad&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>], RMSProp and Adam&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>]. To ensure fair comparison, all methods are applied on RSVD with the same hyperparameters if not explicitly specified.</p>
        <section id="sec-25">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.3.1</span> Sensitivity vs. <em>λ</em></h4>
            </div>
          </header>
          <p>Figure&nbsp;<a class="fig" href="#fig3">3</a> compares the recommendation accuracy of AdaError and the three other methods with different initial learning rates. We can see from the results that AdaError achieves the lowest RMSE values among all the compared methods with <em>λ</em> values varying from 0.05 to 0.001. Meanwhile, the test RMSE of AdaError only changes slightly when <em>λ</em> changes, while the test RMSEs of the other methods change significantly. This is because AdaError can always converge to local minimum due to its insensitivity to rating noises.</p>
          <figure id="fig3">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig3.jpg" class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span> <span class="figure-title">Sensitivity analysis with initial learning step <em>λ</em> on the MovieLens 10M dataset. We set rank <em>k</em> = 100 and <em>L</em> <sub>2</sub> regularization coefficient <em>μ</em> = 0.02 for all methods, and set <em>β</em> = 1<em>e</em> − 4 for AdaError.</span>
            </div>
          </figure>
          <p>Note that the test RMSE of AdaGrad dramatically increases when <em>λ</em> is too small, which is because the learning rates in AdaGrad will quickly become very small when <em>λ</em> is not large enough and the training process will terminate due to too small gains in optimization accuracy. Meanwhile, the test RMSEs of RMSProp and Adam are very high when the learning rates are too large, which is because too large learning rates may affect the convergence of these two methods. In comparison, AdaError can overcome this issue, because the smallest learning steps of AdaError is bounded by <em>β</em>.</p>
        </section>
        <section id="sec-26">
          <header>
            <div class="title-info">
              <h4><span class="section-number">5.3.2</span> Sensitivity vs. <em>β</em></h4>
            </div>
          </header>
          <p>As defined in Equation&nbsp;<a class="eqn" href="#eq4">4</a>, <em>β</em> can prevent the learning steps of AdaError from becoming infinitely small. As shown in Figure&nbsp;<a class="fig" href="#fig4">4</a>, smaller <em>β</em> value can achieve slightly lower test RMSE, because smaller <em>β</em> can reduce the overall learning steps of AdaError and smaller learning steps can ensure better convergence around local minimum. However, the test RMSE only increases by approximately 0.001 when <em>β</em> increases from 1<em>e</em> − 4 to 1<em>e</em> − 3, which indicates that AdaError is very stable with different <em>β</em> values.</p>
          <figure id="fig4">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig4.jpg" class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span> <span class="figure-title">Sensitivity analysis of AdaError with hyperparameter <em>β</em> on the MovieLens 10M dataset. We set rank <em>k</em> − 100 and <em>L</em> <sub>2</sub> regularization coefficient <em>μ</em> = 0.02 for all methods.</span>
            </div>
          </figure>
        </section>
        <section id="sec-27">
          <p><em>5.3.3 Sensitivity vs. Regularization Coefficient.</em> Figure&nbsp;<a class="fig" href="#fig5">5</a> compares the recommendation accuracy of AdaError and three other methods with different <em>L</em> <sub>2</sub> regularization coefficients. As we can see from the results, AdaError achieves smaller test RMSE than all the compared methods when <em>μ</em> increases from 0.01 to 0.05. It is known that proper <em>L</em> <sub>2</sub> regularization coefficient can prevent the learned models from overfitting&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0029">29</a>]. However, AdaError can naturally prevent the learned models from overfitting the training data, so that AdaError is less sensitive to <em>μ</em> than other methods.</p>
          <figure id="fig5">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig5.jpg" class="img-responsive" alt="Figure 5" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 5:</span> <span class="figure-title">Sensitivity analysis with <em>L</em> <sub>2</sub> regularization coefficient <em>μ</em> on the MovieLens 10M dataset. We set <em>k</em> = 100 and <em>λ</em> = 0.01 for all methods, and set <em>β</em> = 1<em>e</em>. − 4 for AdaError.</span>
            </div>
          </figure>
        </section>
        <section id="sec-28">
          <p><em>5.3.4 Sensitivity vs. Rank.</em> Figure&nbsp;<a class="fig" href="#fig6">6</a> compares the recommendation accuracy of AdaError and the three other methods with different rank values. Note that, for all the methods, we use the optimal hyperparameters based on the previous sensitivity analysis. As we can see from the results, the test RMSE of AdaError consistently decreases when rank increases from 50 to 250, which indicates that AdaError will not overfit even with very large ranks. This further confirms that AdaError can achieve better generalization performance. Moreover, AdaError outperforms all the other three methods with all ranks, which further confirms that AdaError is more desirable in collaborative filtering.</p>
          <figure id="fig6">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig6.jpg" class="img-responsive" alt="Figure 6" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 6:</span> <span class="figure-title">Sensitivity analysis with rank on the MovieLens 10M dataset. The hyperparameters of all the methods are chosen as the optimal ones based on the previous sensitivity analysis.</span>
            </div>
          </figure>
        </section>
        <section id="sec-29">
          <p><em>5.3.5 Sensitivity vs. Data Sparsity.</em> Figure&nbsp;<a class="fig" href="#fig7">7</a> compares the recommendation accuracy of AdaError and three other methods with different training / test split ratios, i.e., different sparsity of training data. As shown in the results, the test accuracy of AdaError consistently outperforms all the three compared methods when the training set ratio increases from 20% to 80%. This indicates that AdaError can achieve superior performance even when the training data is very sparse.</p>
          <figure id="fig7">
            <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186155/images/www2018-164-fig7.jpg" class="img-responsive" alt="Figure 7" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 7:</span> <span class="figure-title">Sensitivity analysis with data sparsity on the MovieLens 10M dataset. We set rank <em>k</em> = 100 and <em>L</em> <sub>2</sub> regularization coefficient <em>μ</em> = 0.02 for all methods, and set <em>β</em> = 1<em>e</em> − 4 for AdaError.</span>
            </div>
          </figure>
          <p>In summary, the sensitivity analysis experiments demonstrate that the proposed AdaError method is less sensitive to hyperparameters compared with the three popular adaptive learning rate methods in MA-based collaborative filtering. Therefore, we can conclude that AdaError is more desirable than the other three adaptive learning rate methods in matrix approximation-based collaborative filtering.</p>
        </section>
      </section>
      <section id="sec-30">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.4</span> Rating Prediction Accuracy</h3>
          </div>
        </header>
        <p>Table&nbsp;<a class="tbl" href="#tab1">1</a> compares the recommendation accuracy of the proposed method (AdaError for rating prediction) with seven state-of-the-art matrix approximation-based collaborative filtering methods on the MovieLens 10M and Netflix datasets. As we can see from the results, the proposed method outperforms all the seven compared methods with at least 95% confidence level on both MovieLens 10M and Netflix datasets. The main reasons are: 1) AdaError can prevent the learned MA models from overreacting to noises, so that the learned MA models can achieve better generalization performance when the hyperparameters of AdaError, i.e., <em>λ</em> and <em>β</em>, are properly chosen and 2) AdaError can shrink the learning rates as the number of iterations increases which can ensure better convergence, because smaller learning steps can reduce oscillation near local minimum.</p>
        <div class="table-responsive" id="tab1">
          <div class="table-caption">
            <span class="table-number">Table 1:</span> <span class="table-title">RMSE comparison between the proposed method (<em>k</em> = 500) and seven state-of-the-art matrix approximation-based CF methods — BPMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>], DFC&nbsp;&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0027">27</a>], LLORMA&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>], GSMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>], WEMAREC&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>], SMA&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>], ERMMA&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>]. Note that the proposed method statistically significantly outperforms the other methods with at least 95% confidence level.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:center;">Method</th>
                <th style="text-align:center;">MovieLens (10M)</th>
                <th style="text-align:center;">Netflix</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">BPMF</td>
                <td style="text-align:center;">0.8197 ± 0.0006</td>
                <td style="text-align:center;">0.8421 ± 0.0003</td>
              </tr>
              <tr>
                <td style="text-align:center;">DFC</td>
                <td style="text-align:center;">0.8067 ± 0.0002</td>
                <td style="text-align:center;">0.8453 ± 0.0003</td>
              </tr>
              <tr>
                <td style="text-align:center;">LLORMA</td>
                <td style="text-align:center;">0.7855 ± 0.0002</td>
                <td style="text-align:center;">0.8275 ± 0.0004</td>
              </tr>
              <tr>
                <td style="text-align:center;">GSMF</td>
                <td style="text-align:center;">0.8012 ± 0.0011</td>
                <td style="text-align:center;">0.8420 ± 0.0006</td>
              </tr>
              <tr>
                <td style="text-align:center;">WEMAREC</td>
                <td style="text-align:center;">0.7775 ± 0.0007</td>
                <td style="text-align:center;">0.8143 ± 0.0001</td>
              </tr>
              <tr>
                <td style="text-align:center;">SMA</td>
                <td style="text-align:center;">0.7682 ± 0.0003</td>
                <td style="text-align:center;">0.8036 ± 0.0004</td>
              </tr>
              <tr>
                <td style="text-align:center;">ERMMA</td>
                <td style="text-align:center;">0.7670 ± 0.0007</td>
                <td style="text-align:center;">0.8018 ± 0.0001</td>
              </tr>
              <tr>
                <td style="text-align:center;"><strong>Proposed</strong></td>
                <td style="text-align:center;"><strong>0.7644 ± 0.0003</strong></td>
                <td style="text-align:center;"><strong>0.7980 ± 0.0002</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-31">
        <header>
          <div class="title-info">
            <h3><span class="section-number">5.5</span> Top-N Recommendation Accuracy</h3>
          </div>
        </header>
        <p>Table&nbsp;<a class="tbl" href="#tab2">2</a> and Table&nbsp;<a class="tbl" href="#tab3">3</a> compare the recommendation accuracy (Precision@N and NDCG@N) between the proposed method and five other methods on the MovieLens 100K and MovieLens 1M datasets, respectively. Among the five compared methods, RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>] is a rating-based MA method and WRMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], BPR&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>], SLIM&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>] and AOBRP&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>] are top-N recommendation algorithms. As shown in the results, the proposed method (AdaError for top-N recommendation) outperforms all the compared methods on both Precision@N and NDCG@N when <em>N</em> increases from 1 to 20 with at least 95% confidence level. The main reasons of the superior performance of the proposed method are 1) better generalization performance; 2) stronger capability of reducing oscillation near local minimum; 3) a weighting strategy which gives lower weights to unobserved ratings to address the positive-unlabeled data issue in the top-N recommendation task.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class="table-title">Precision comparison between the proposed method and one rating-based MA method (RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]) and four top-N recommendation methods (WRMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], BPR&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>], SLIM&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>], AOBRP&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]) on the MovieLens 100K and MovieLens 1M datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th colspan="2" style="text-align:center;">
                  Metric
                  <hr />
                </th>
                <th colspan="4" style="text-align:center;">
                  Precision@N
                  <hr />
                </th>
              </tr>
              <tr>
                <th colspan="2" style="text-align:center;">
                  Data | Method
                  <hr />
                </th>
                <th style="text-align:center;">N=1</th>
                <th style="text-align:center;">N=5</th>
                <th style="text-align:center;">N=10</th>
                <th style="text-align:center;">N=20</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">ML-100K</td>
                <td style="text-align:center;">RSVD</td>
                <td style="text-align:center;">0.3155 ± 0.0038</td>
                <td style="text-align:center;">0.2179 ± 0.0007</td>
                <td style="text-align:center;">0.1403 ± 0.0035</td>
                <td style="text-align:center;">0.1300 ± 0.0057</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">WRMF</td>
                <td style="text-align:center;">0.3851 ± 0.0116</td>
                <td style="text-align:center;">0.2752 ± 0.0053</td>
                <td style="text-align:center;">0.2202 ± 0.0056</td>
                <td style="text-align:center;">0.1679 ± 0.0035</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">BPR</td>
                <td style="text-align:center;">0.3439 ± 0.0168</td>
                <td style="text-align:center;">0.2533 ± 0.0082</td>
                <td style="text-align:center;">0.2061 ± 0.0040</td>
                <td style="text-align:center;">0.1581 ± 0.0028</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SLIM</td>
                <td style="text-align:center;">0.3951 ± 0.0056</td>
                <td style="text-align:center;">0.2625 ± 0.0090</td>
                <td style="text-align:center;">0.2055 ± 0.0031</td>
                <td style="text-align:center;">0.1539 ± 0.0015</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AOBPR</td>
                <td style="text-align:center;">0.3395 ± 0.0099</td>
                <td style="text-align:center;">0.2591 ± 0.0057</td>
                <td style="text-align:center;">0.2119 ± 0.0031</td>
                <td style="text-align:center;">0.1632 ± 0.0025</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;"><strong>Proposed</strong></td>
                <td style="text-align:center;"><strong>0.4078 ± 0.0021</strong></td>
                <td style="text-align:center;"><strong>0.2934 ± 0.0049</strong></td>
                <td style="text-align:center;"><strong>0.2331 ± 0.0029</strong></td>
                <td style="text-align:center;"><strong>0.1779 ± 0.0018</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">ML-1M</td>
                <td style="text-align:center;">RSVD</td>
                <td style="text-align:center;">0.1659 ± 0.0017</td>
                <td style="text-align:center;">0.1263 ± 0.0005</td>
                <td style="text-align:center;">0.1037 ± 0.0009</td>
                <td style="text-align:center;">0.0766 ± 0.0020</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">WRMF</td>
                <td style="text-align:center;">0.2761 ± 0.0074</td>
                <td style="text-align:center;">0.2155 ± 0.0009</td>
                <td style="text-align:center;">0.1816 ± 0.0007</td>
                <td style="text-align:center;">0.1459 ± 0.0004</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">BPR</td>
                <td style="text-align:center;">0.3062 ± 0.0030</td>
                <td style="text-align:center;">0.2277 ± 0.0074</td>
                <td style="text-align:center;">0.1896 ± 0.0048</td>
                <td style="text-align:center;">0.1516 ± 0.0007</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SLIM</td>
                <td style="text-align:center;">0.3053 ± 0.0097</td>
                <td style="text-align:center;">0.2208 ± 0.0039</td>
                <td style="text-align:center;">0.1836 ± 0.0006</td>
                <td style="text-align:center;">0.1419 ± 0.0029</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AOBPR</td>
                <td style="text-align:center;">0.3098 ± 0.0076</td>
                <td style="text-align:center;">0.2315 ± 0.0002</td>
                <td style="text-align:center;">0.1926 ± 0.0022</td>
                <td style="text-align:center;">0.1540 ± 0.0016</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;"><strong>Proposed</strong></td>
                <td style="text-align:center;"><strong>0.3692 ± 0.0018</strong></td>
                <td style="text-align:center;"><strong>0.2878 ± 0.0011</strong></td>
                <td style="text-align:center;"><strong>0.2385 ± 0.0014</strong></td>
                <td style="text-align:center;"><strong>0.1891 ± 0.0007</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <div class="table-responsive" id="tab3">
          <div class="table-caption">
            <span class="table-number">Table 3:</span> <span class="table-title">NDCG comparison between the proposed method and one rating-based MA method (RSVD&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>]) and four top-N recommendation methods (WRMF&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>], BPR&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>], SLIM&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0031">31</a>], AOBRP&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0035">35</a>]) on the MovieLens 100K and MovieLens 1M datasets.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th colspan="2" style="text-align:center;">
                  Metric
                  <hr />
                </th>
                <th colspan="4" style="text-align:center;">
                  NDCG@N
                  <hr />
                </th>
              </tr>
              <tr>
                <th colspan="2" style="text-align:center;">
                  Data | Method
                  <hr />
                </th>
                <th style="text-align:center;">N=1</th>
                <th style="text-align:center;">N=5</th>
                <th style="text-align:center;">N=10</th>
                <th style="text-align:center;">N=20</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:center;">ML-100K</td>
                <td style="text-align:center;">RSVD</td>
                <td style="text-align:center;">0.0389 ± 0.0028</td>
                <td style="text-align:center;">0.1047 ± 0.0032</td>
                <td style="text-align:center;">0.0996 ± 0.0059</td>
                <td style="text-align:center;">0.1393 ± 0.0071</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">WRMF</td>
                <td style="text-align:center;">0.0913 ± 0.0034</td>
                <td style="text-align:center;">0.1989 ± 0.0030</td>
                <td style="text-align:center;">0.2535 ± 0.0045</td>
                <td style="text-align:center;">0.3131 ± 0.0043</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">BPR</td>
                <td style="text-align:center;">0.0783 ± 0.0036</td>
                <td style="text-align:center;">0.1803 ± 0.0056</td>
                <td style="text-align:center;">0.2351 ± 0.0056</td>
                <td style="text-align:center;">0.2929 ± 0.0065</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SLIM</td>
                <td style="text-align:center;">0.0922 ± 0.0021</td>
                <td style="text-align:center;">0.1967 ± 0.0036</td>
                <td style="text-align:center;">0.2476 ± 0.0050</td>
                <td style="text-align:center;">0.3017 ± 0.0091</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AOBPR</td>
                <td style="text-align:center;">0.0770 ± 0.0043</td>
                <td style="text-align:center;">0.1801 ± 0.0044</td>
                <td style="text-align:center;">0.2343 ± 0.0051</td>
                <td style="text-align:center;">0.2930 ± 0.0058</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;"><strong>Proposed</strong></td>
                <td style="text-align:center;"><strong>0.0998 ± 0.0014</strong></td>
                <td style="text-align:center;"><strong>0.2143 ± 0.0036</strong></td>
                <td style="text-align:center;"><strong>0.2719 ± 0.0048</strong></td>
                <td style="text-align:center;"><strong>0.3333 ± 0.0053</strong></td>
              </tr>
              <tr>
                <td style="text-align:center;">ML-1M</td>
                <td style="text-align:center;">RSVD</td>
                <td style="text-align:center;">0.0324 ± 0.0020</td>
                <td style="text-align:center;">0.0700 ± 0.0006</td>
                <td style="text-align:center;">0.0864 ± 0.0002</td>
                <td style="text-align:center;">0.1006 ± 0.0001</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">WRMF</td>
                <td style="text-align:center;">0.0510 ± 0.0013</td>
                <td style="text-align:center;">0.1202 ± 0.0002</td>
                <td style="text-align:center;">0.1563 ± 0.0013</td>
                <td style="text-align:center;">0.2012 ± 0.0010</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">BPR</td>
                <td style="text-align:center;">0.0568 ± 0.0006</td>
                <td style="text-align:center;">0.1235 ± 0.0003</td>
                <td style="text-align:center;">0.1601 ± 0.0035</td>
                <td style="text-align:center;">0.2070 ± 0.0011</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">SLIM</td>
                <td style="text-align:center;">0.0551 ± 0.0015</td>
                <td style="text-align:center;">0.1201 ± 0.0023</td>
                <td style="text-align:center;">0.1586 ± 0.0028</td>
                <td style="text-align:center;">0.1948 ± 0.0043</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;">AOBPR</td>
                <td style="text-align:center;">0.0582 ± 0.0018</td>
                <td style="text-align:center;">0.1200 ± 0.0006</td>
                <td style="text-align:center;">0.1567 ± 0.0009</td>
                <td style="text-align:center;">0.2021 ± 0.0009</td>
              </tr>
              <tr>
                <td style="text-align:center;"></td>
                <td style="text-align:center;"><strong>Proposed</strong></td>
                <td style="text-align:center;"><strong>0.0722 ± 0.0010</strong></td>
                <td style="text-align:center;"><strong>0.1653 ± 0.0006</strong></td>
                <td style="text-align:center;"><strong>0.2155 ± 0.0002</strong></td>
                <td style="text-align:center;"><strong>0.2703 ± 0.0003</strong></td>
              </tr>
            </tbody>
          </table>
        </div>
        <p>The main difference between RSVD and WRMF is that WRMF can give unobserved ratings lower weights in the training process whereas RSVD treats all ratings equally. This indicates that setting smaller weights for unobserved ratings can significantly improve recommendation accuracy in top-N recommendation on implicit feedback data. The proposed method adopts the same weighting strategy as WRMF, and the superior performance of the proposed method indicates that the proposed AdaError method can improve the performance of weighted matrix approximation in the top-N recommendation task.</p>
      </section>
    </section>
    <section id="sec-32">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Related Work</h2>
        </div>
      </header>
      <p>Collaborative filtering is an important class of methods in today's recommender systems and matrix approximation methods are popular among existing CF methods for both rating prediction&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0044">44</a>] and top-N recommendation&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0036">36</a>]. The earliest matrix approximation-based CF method tried to discover the latent structures within the user-item rating matrix&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0005">5</a>], in which they claimed that SVD can eliminate the need for users to rate all similar items. In other words, discovering the latent structures can help alleviate the data sparsity issue in real-world recommender systems&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>]. Meanwhile, the scalability of recommender systems can be improved because recommendation scores can be computed by simple dot products of feature vectors&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0041">41</a>]. Later, several works have demonstrated that MA methods can achieve superior accuracy than memory-based methods in the Netflix Prize Competition&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0034">34</a>], after which MA methods have been the focus of collaborative filtering methods&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0024">24</a>]. Salakhutdinov and Mnih first proposed the probabilistic matrix factorization method&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0039">39</a>], and they proposed the BPMF method by extending the PMF method using a Bayesian treatment&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0038">38</a>]. Koren&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0021">21</a>] combined the SVD and neighbor-based CF method and proposed the SVD++ method. Recently, Li&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0026">26</a>] proposed a stable matrix approximation method, which can improve the generalization performance of matrix approximation by improving the algorithm stability. Later, they proposed the ERMMA method&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0025">25</a>] to reduce the expected risk of MA models. However, the above methods did not consider the noisy rating issue in real-world recommender systems, and the learned MA models in their methods may overfit the noises in the ratings and thus achieve non-optimal accuracy.</p>
      <p>The noisy rating issue has been investigated in the literature, and two main categories of works have been proposed to address this issue. The first category of methods tries to alleviate the rating noises by designing new user interfaces or interaction methods&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>]. Amatriain&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0002">2</a>] proposed to denoise the recommender system databases by asking users to re-rate some of their previously rated items. Nguyen&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0030">30</a>] proposed to use exemplars to relate user rating decisions to their prior rating decisions, and they found that presenting exemplars can help users generate more consistent ratings. Jones&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0018">18</a>] found that users comparing items are more reliable and stable than rating items as much as 20%, so that they claimed that comparisons could yield better user modeling. However, the above methods require additional efforts for users and thus may not be practical in real-world recommender systems. The other category of methods tries to address the noisy rating issue by designing robust collaborative filtering algorithms&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>]. Mehta&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0028">28</a>] proposed a robust matrix factorization method based on M-estimator, which can achieve higher accuracy with noisy user profiles. However, their method aimed to make CF more robust when attack profiles are inserted in the databases, which shares the same assumptions with several other approaches&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0037">37</a>]. In contrast, our work assumes that rating noises generally exist in the data and users may or may not intentionally insert these noises when rating items, which is more general than the above works. Lakshminarayanan&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0023">23</a>] proposed a robust Bayesian matrix factorization method, in which the rating noises are claimed to be non-Gaussian and heteroscedastic. However, their method is not easy to train when the number of parameters is large.</p>
      <p>Adaptive learning rate methods have also been proposed to achieve more informative gradient updates than fixed learning rates&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>]. Duchi&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0012">12</a>] first proposed the AdaGrad method, which can perform larger updates for infrequent parameters and smaller updates for frequent parameters to achieve better convergence. However, their method suffers from the issue that the learning rate will shrink and become infinitely small when the number of iterations is large. To address the above issue, Zeiler&nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0045">45</a>] proposed the AdaDelta method, in which a running average of the squared gradients is adopted in the learning rate rather than the accumulated squared gradients in AdaGrad. Similarly, RMSProp proposed by Hinton's group adopts the same idea to prevent the learning rates from becoming infinitely small. Recently, Kingma&nbsp;et al. &nbsp;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top" href="#BibPLXBIB0020">20</a>] proposed the Adam method, in which the learning rates are adjusted by keeping an exponentially decaying average of both the first moment and the second moment of the gradients. However, the above methods are not intentionally designed for noisy training data, so they are not suitable for learning MA models with noisy ratings. In comparison, our experimental studies show that AdaError is less sensitive to hyperparameters than the above methods, which means that AdaError is more desirable in practice.</p>
    </section>
    <section id="sec-33">
      <header>
        <div class="title-info">
          <h2><span class="section-number">7</span> Conclusion</h2>
        </div>
      </header>
      <p>Noisy ratings in real-world recommender systems pose challenges to matrix approximation-based collaborative filtering algorithms, in which the learned MA models will easily be prone to noises in the ratings. This paper proposes AdaError — an adaptive learning rate method for matrix approximation-based collaborative filtering methods. AdaError gives smaller learning rates to ratings with larger noises so as to prevent the learned MA models from overreacting to the noises. Our theoretical analysis shows that AdaError can achieve better generalization performance than fixed learning rate method when the hyperparameters in AdaError are properly chosen. Experimental studies on real-world datasets demonstrate that (1) AdaError can achieve better performance than existing adaptive learning rate methods and (2) our proposed AdaError-based recommendation methods can achieve statistically significant higher recommendation accuracy than state-of-the-art collaborative filtering algorithms in both the rating prediction task and the top-N recommendation task.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="sec-34">
      <header>
        <div class="title-info">
          <h2>ACKNOWLEDGMENTS</h2>
        </div>
      </header>
      <p>This work was supported in part by the National Natural Science Foundation of China under Grant Nos. 61332008 and U1630115, and the National Science Foundation of USA under Grant Nos. 1334351, 1442971, and 1528138.</p>
    </section>
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Xavier Amatriain, Josep&nbsp;M. Pujol, and Nuria Oliver. 2009. I Like It... I Like It Not: Evaluating User Ratings Noise in Recommender Systems. In <em><em>Proceedings of the 17th International Conference on User Modeling, Adaptation, and Personalization</em> (UMAP ’09)</em>. Springer, 247–258.</li>
        <li id="BibPLXBIB0002" label="[2]">Xavier Amatriain, Josep&nbsp;M. Pujol, Nava Tintarev, and Nuria Oliver. 2009. Rate It Again: Increasing Recommendation Accuracy by User Re-rating. In <em><em>Proceedings of the Third ACM Conference on Recommender Systems</em> (RecSys ’09)</em>. ACM, 173–180.</li>
        <li id="BibPLXBIB0003" label="[3]">Alex Beutel, Amr Ahmed, and Alexander&nbsp;J. Smola. 2015. ACCAMS: Additive Co-Clustering to Approximate Matrices Succinctly. In <em><em>Proceedings of the 24th International Conference on World Wide Web</em> (WWW ’15)</em>. 119–129.</li>
        <li id="BibPLXBIB0004" label="[4]">Alex Beutel, Ed&nbsp;H. Chi, Zhiyuan Cheng, Hubert Pham, and John Anderson. 2017. Beyond Globally Optimal: Focused Learning for Improved Recommendations. In <em><em>Proceedings of the 26th International Conference on World Wide Web</em> (WWW ’17)</em>. 203–212.</li>
        <li id="BibPLXBIB0005" label="[5]">Daniel Billsus and Michael&nbsp;J Pazzani. 1998. Learning Collaborative Information Filters.. In <em><em>Proceedings of the Fifteenth International Conference on Machine Learning</em> (ICML ’98)</em>, Vol.&nbsp;98. 46–54.</li>
        <li id="BibPLXBIB0006" label="[6]">Olivier Bousquet and André Elisseeff. 2001. Algorithmic Stability and Generalization Performance. In <em><em>Advances in Neural Information Processing Systems</em></em> . 196–202.</li>
        <li id="BibPLXBIB0007" label="[7]">Emmanuel&nbsp;J. Candès and Yaniv Plan. 2010. Matrix Completion With Noise. <em><em>Proc. IEEE</em></em> 98, 6 (2010), 925–936.</li>
        <li id="BibPLXBIB0008" label="[8]">Chao Chen, Dongsheng Li, Yingying Zhao, Qin Lv, and Li Shang. 2015. WEMAREC: Accurate and Scalable Recommendation through Weighted and Ensemble Matrix Approximation. In <em><em>Proceedings of the 38th International ACM SIGIR Conference on Research and Development in Information Retrieval</em></em> . 303–312.</li>
        <li id="BibPLXBIB0009" label="[9]">Peizhe Cheng, Shuaiqiang Wang, Jun Ma, Jiankai Sun, and Hui Xiong. 2017. Learning to Recommend Accurate and Diverse Items. In <em><em>Proceedings of the 26th International Conference on World Wide Web</em> (WWW ’17)</em>. 183–192.</li>
        <li id="BibPLXBIB0010" label="[10]">Dan Cosley, Shyong&nbsp;K. Lam, Istvan Albert, Joseph&nbsp;A. Konstan, and John Riedl. 2003. Is Seeing Believing?: How Recommender System Interfaces Affect Users’ Opinions. In <em><em>Proceedings of the SIGCHI Conference on Human Factors in Computing Systems</em> (CHI ’03)</em>. ACM, 585–592.</li>
        <li id="BibPLXBIB0011" label="[11]">Jeffrey Dean, Greg Corrado, Rajat Monga, Kai Chen, Matthieu Devin, Mark Mao, Andrew Senior, Paul Tucker, Ke Yang, Quoc&nbsp;V Le, <em>et al.</em> 2012. Large scale distributed deep networks. In <em><em>Advances in neural information processing systems</em></em> . 1223–1231.</li>
        <li id="BibPLXBIB0012" label="[12]">John Duchi, Elad Hazan, and Yoram Singer. 2011. Adaptive subgradient methods for online learning and stochastic optimization. <em><em>Journal of Machine Learning Research</em></em> 12, Jul (2011), 2121–2159.</li>
        <li id="BibPLXBIB0013" label="[13]">Moritz Hardt, Benjamin Recht, and Yoram Singer. 2016. Train Faster, Generalize Better: Stability of Stochastic Gradient Descent. In <em><em>Proceedings of the 33rd International Conference on International Conference on Machine Learning</em> (ICML’16)</em>. JMLR.org, 1225–1234.</li>
        <li id="BibPLXBIB0014" label="[14]">Elad Hazan and Satyen Kale. 2014. Beyond the regret minimization barrier: optimal algorithms for stochastic strongly-convex optimization. <em><em>Journal of Machine Learning Research</em></em> 15, 1 (2014), 2489–2512.</li>
        <li id="BibPLXBIB0015" label="[15]">Liang Hu, Jian Cao, Guandong Xu, Longbing Cao, Zhiping Gu, and Can Zhu. 2013. Personalized Recommendation via Cross-domain Triadic Factorization. In <em><em>Proceedings of the 22Nd International Conference on World Wide Web</em> (WWW ’13)</em>. ACM, 595–606.</li>
        <li id="BibPLXBIB0016" label="[16]">Yifan Hu, Yehuda Koren, and Chris Volinsky. 2008. Collaborative Filtering for Implicit Feedback Datasets. In <em><em>Proceedings of the Eighth IEEE International Conference on Data Mining</em> (ICDM ’08)</em>. 263–272.</li>
        <li id="BibPLXBIB0017" label="[17]">Robert&nbsp;A Jacobs. 1988. Increased rates of convergence through learning rate adaptation. <em><em>Neural networks</em></em> 1, 4 (1988), 295–307.</li>
        <li id="BibPLXBIB0018" label="[18]">Nicolas Jones, Armelle Brun, and Anne Boyer. 2011. Comparisons Instead of Ratings: Towards More Stable Preferences. In <em><em>Proceedings of the 2011 IEEE/WIC/ACM International Conferences on Web Intelligence and Intelligent Agent Technology</em> (WI-IAT ’11)</em>. IEEE, 451–456.</li>
        <li id="BibPLXBIB0019" label="[19]">Santosh Kabbur, Xia Ning, and George Karypis. 2013. FISM: Factored Item Similarity Models for top-N Recommender Systems. In <em><em>Proceedings of the 19th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (KDD ’13)</em>. ACM, 659–667.</li>
        <li id="BibPLXBIB0020" label="[20]">Diederik Kingma and Jimmy Ba. 2014. Adam: A method for stochastic optimization. <em><em>arXiv preprint arXiv:1412.6980</em></em> (2014).</li>
        <li id="BibPLXBIB0021" label="[21]">Yehuda Koren. 2008. Factorization Meets the Neighborhood: A Multifaceted Collaborative Filtering Model. In <em><em>Proceedings of the 14th ACM SIGKDD International Conference on Knowledge Discovery and Data Mining</em> (KDD ’08)</em>. ACM, 426–434.</li>
        <li id="BibPLXBIB0022" label="[22]">Yehuda Koren, Robert Bell, and Chris Volinsky. 2009. Matrix factorization techniques for recommender systems. <em><em>Computer</em></em> 42, 8 (2009), 30–37.</li>
        <li id="BibPLXBIB0023" label="[23]">Balaji Lakshminarayanan, Guillaume Bouchard, and Cedric Archambeau. 2011. Robust Bayesian matrix factorisation. In <em><em>Proceedings of the International Conference on Artificial Intelligence and Statistics (AISTATS)</em></em> . 425–433.</li>
        <li id="BibPLXBIB0024" label="[24]">Joonseok Lee, Seungyeon Kim, Guy Lebanon, and Yoram Singer. 2013. Local low-rank matrix approximation. In <em><em>Proceedings of The 30th International Conference on Machine Learning (ICML ’13)</em></em> . 82–90.</li>
        <li id="BibPLXBIB0025" label="[25]">Dongsheng Li, Chao Chen, Qin Lv, Li Shang, Stephen&nbsp;M. Chu, and Hongyuan Zha. 2017. ERMMA: Expected Risk Minimization for Matrix Approximation-based Recommender Systems. In <em><em>Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI ’17)</em></em> . 1403–1409.</li>
        <li id="BibPLXBIB0026" label="[26]">Dongsheng Li, Chao Chen, Qin Lv, Junchi Yan, Li Shang, and Stephen&nbsp;M. Chu. 2016. Low-rank matrix approximation with stability. In <em><em>Proceedings of The 33rd International Conference on Machine Learning (ICML ’16)</em></em> . 295–303.</li>
        <li id="BibPLXBIB0027" label="[27]">Lester&nbsp;W Mackey, Michael&nbsp;I Jordan, and Ameet Talwalkar. 2011. Divide-and-conquer matrix factorization. In <em><em>Advances in Neural Information Processing Systems</em></em> . 1134–1142.</li>
        <li id="BibPLXBIB0028" label="[28]">Bhaskar Mehta, Thomas Hofmann, and Wolfgang Nejdl. 2007. Robust Collaborative Filtering. In <em><em>Proceedings of the 2007 ACM Conference on Recommender Systems</em> (RecSys ’07)</em>. ACM, 49–56.</li>
        <li id="BibPLXBIB0029" label="[29]">Andrew&nbsp;Y. Ng. 2004. Feature Selection, L1 vs. L2 Regularization, and Rotational Invariance. In <em><em>Proceedings of the Twenty-first International Conference on Machine Learning</em> (ICML ’04)</em>. 78–85.</li>
        <li id="BibPLXBIB0030" label="[30]">Tien&nbsp;T. Nguyen, Daniel Kluver, Ting-Yu Wang, Pik-Mai Hui, Michael&nbsp;D. Ekstrand, Martijn&nbsp;C. Willemsen, and John Riedl. 2013. Rating Support Interfaces to Improve User Experience and Recommender Accuracy. In <em><em>Proceedings of the 7th ACM Conference on Recommender Systems</em> (RecSys ’13)</em>. ACM, 149–156.</li>
        <li id="BibPLXBIB0031" label="[31]">Xia Ning and George Karypis. 2011. SLIM: Sparse Linear Methods for Top-N Recommender Systems. In <em><em>Proceedings of the 2011 IEEE 11th International Conference on Data Mining</em> (ICDM ’11)</em>. 497–506.</li>
        <li id="BibPLXBIB0032" label="[32]">Michael O'Mahony, Neil Hurley, Nicholas Kushmerick, and Guénolé Silvestre. 2004. Collaborative Recommendation: A Robustness Analysis. <em><em>ACM Trans. Internet Technol.</em></em> 4, 4 (2004), 344–377.</li>
        <li id="BibPLXBIB0033" label="[33]">Michael&nbsp;P. O'Mahony, Neil&nbsp;J. Hurley, and Guénolé&nbsp;C.M. Silvestre. 2006. Detecting Noise in Recommender System Databases. In <em><em>Proceedings of the 11th International Conference on Intelligent User Interfaces</em> (IUI ’06)</em>. ACM, 109–115.</li>
        <li id="BibPLXBIB0034" label="[34]">Arkadiusz Paterek. 2007. Improving regularized singular value decomposition for collaborative filtering. In <em><em>Proceedings of KDD cup and workshop</em></em> , Vol.&nbsp;2007. 5–8.</li>
        <li id="BibPLXBIB0035" label="[35]">Steffen Rendle and Christoph Freudenthaler. 2014. Improving Pairwise Learning for Item Recommendation from Implicit Feedback. In <em><em>Proceedings of the 7th ACM International Conference on Web Search and Data Mining</em> (WSDM ’14)</em>. 273–282.</li>
        <li id="BibPLXBIB0036" label="[36]">Steffen Rendle, Christoph Freudenthaler, Zeno Gantner, and Lars Schmidt-Thieme. 2009. BPR: Bayesian personalized ranking from implicit feedback. In <em><em>Proceedings of the twenty-fifth conference on uncertainty in artificial intelligence</em></em> . 452–461.</li>
        <li id="BibPLXBIB0037" label="[37]">Paul Resnick and Rahul Sami. 2007. The Influence Limiter: Provably Manipulation-resistant Recommender Systems. In <em><em>Proceedings of the 2007 ACM Conference on Recommender Systems</em> (RecSys ’07)</em>. ACM, 25–32.</li>
        <li id="BibPLXBIB0038" label="[38]">Ruslan Salakhutdinov and Andriy Mnih. 2008. Bayesian Probabilistic Matrix Factorization Using Markov Chain Monte Carlo. In <em><em>Proceedings of the 25th International Conference on Machine Learning</em> (ICML ’08)</em>. ACM, 880–887.</li>
        <li id="BibPLXBIB0039" label="[39]">Ruslan Salakhutdinov and Andriy Mnih. 2008. Probabilistic matrix factorization. In <em><em>Advances in neural information processing systems</em></em> . 1257–1264.</li>
        <li id="BibPLXBIB0040" label="[40]">Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2000. Application of Dimensionality Reduction in Recommender System - A Case Study. In <em><em>ACM WebKDD 2000 Workshop</em></em> . ACM SIGKDD.</li>
        <li id="BibPLXBIB0041" label="[41]">Badrul Sarwar, George Karypis, Joseph Konstan, and John Riedl. 2002. Incremental Singular Value Decomposition Algorithms for Highly Scalable Recommender Systems. In <em><em>Proceedings of the 5th International Conference in Computers and Information Technology</em></em> .</li>
        <li id="BibPLXBIB0042" label="[42]">Madeleine Udell, Corinne Horn, Reza Zadeh, and Stephen Boyd. 2016. Generalized Low Rank Models. <em><em>Foundations and Trends in Machine Learning</em></em> 9, 1 (2016), 1–118.</li>
        <li id="BibPLXBIB0043" label="[43]">Linli Xu, Zaiyi Chen, Qi Zhou, Enhong Chen, Nicholas&nbsp;Jing Yuan, and Xing Xie. 2016. Aligned Matrix Completion: Integrating Consistency and Independency in Multiple Domains. In <em><em>2016 IEEE 16th International Conference on Data Mining (ICDM)</em></em> . 529–538.</li>
        <li id="BibPLXBIB0044" label="[44]">Ting Yuan, Jian Cheng, Xi Zhang, Shuang Qiu, and Hanqing Lu. 2014. Recommendation by Mining Multiple User Behaviors with Group Sparsity. In <em><em>Proceedings of the 28th AAAI Conference on Artificial Intelligence (AAAI ’14)</em></em> . 222–228.</li>
        <li id="BibPLXBIB0045" label="[45]">Matthew&nbsp;D Zeiler. 2012. ADADELTA: an adaptive learning rate method. <em><em>arXiv preprint arXiv:1212.5701</em></em> (2012).</li>
        <li id="BibPLXBIB0046" label="[46]">Yongfeng Zhang, Min Zhang, Yiqun Liu, Shaoping Ma, and Shi Feng. 2013. Localized Matrix Factorization for Recommendation Based on Matrix Block Diagonal Forms. In <em><em>Proceedings of the 22Nd International Conference on World Wide Web</em> (WWW ’13)</em>. ACM, 1511–1520.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class="link-inline force-break" href="http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf">http://www.cs.toronto.edu/~tijmen/csc321/slides/lecture_slides_lec6.pdf</a></p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&nbsp;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&nbsp;4.0 License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href="https://doi.org/10.1145/3178876.3186155">https://doi.org/10.1145/3178876.3186155</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
