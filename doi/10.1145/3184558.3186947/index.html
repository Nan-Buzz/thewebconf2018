<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Fairness of Extractive Text Summarization</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Fairness of Extractive Text
          Summarization</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Anurag</span> <span class=
          "surName">Shandilya</span> IIT Kharagpur, India
        </div>
        <div class="author">
          <span class="givenName">Kripabandhu</span> <span class=
          "surName">Ghosh</span> IIT Kanpur, India
        </div>
        <div class="author">
          <span class="givenName">Saptarshi</span> <span class=
          "surName">Ghosh</span> IIT Kharagpur, India
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186947"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186947</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>We propose to evaluate extractive summarization
        algorithms from a completely new perspective. Considering
        that an extractive summarization algorithm selects a subset
        of the textual units in the input data for inclusion in the
        summary, we investigate <em>whether this selection is
        fair</em>. We use several summarization algorithms over
        datasets that have a sensitive attribute (e.g., gender,
        political leaning) associated with the textual units, and
        find that the generated summaries often have very different
        distributions of the said attribute. Specifically, some
        classes of the textual units are under-represented in the
        summaries according to the fairness notion of <em>adverse
        impact</em>. To our knowledge, this is the first work on
        fairness of summarization, and is likely to open up
        interesting research problems.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Information retrieval;</strong></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Extractive summarization;
          fairness; adverse impact</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Anurag Shandilya, Kripabandhu Ghosh, and Saptarshi Ghosh.
          2018. Fairness of Extractive Text Summarization. In
          <em>WWW '18 Companion: The 2018 Web Conference
          Companion,</em> <em>April 23–27, 2018,</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 2 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186947" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186947</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>With the phenomenal increase in the amount of textual
      information on the Web, text summarization algorithms are
      commonly used to get a quick overview of the information.
      Most existing summarization algorithms are
      <em>extractive</em> in nature (as opposed to abstractive).
      Extractive algorithms form the summary by extracting some of
      the textual units in the input, e.g., individual sentences in
      a document, or individual microblogs in a set of microblogs.
      Traditionally, summarization algorithms are judged on how
      closely the algorithmic summary matches gold standard
      summaries (usually written by human annotators), using
      measures such as ROUGE scores. In this work, we propose to
      look at extractive summarization algorithms from a completely
      new perspective.</p>
      <p>Extractive summarization algorithms essentially perform a
      selection of a (small) subset of the textual units in the
      input, for inclusion in the summary, based on some measure of
      the relative quality or importance of the textual units. We
      propose to investigate <em>whether this selection is
      fair</em>, i.e., whether the generated summary is a fair
      representation of the input data.</p>
      <p>An algorithm is said to be unfair if it denies a desired
      outcome to an individual or a group of individuals on grounds
      that are inappropriate, e.g., based on a protected attribute
      according to laws of a country. In the context of
      summarization, we assume that the textual units in the input
      data (sentences, tweets, etc.) are associated with a
      sensitive or protected attribute, such as political leaning
      of the text or the gender/race/religion of the person who has
      written the text. In this context, the desired outcome is to
      be included in the summary, assuming that only the textual
      units included in the summary will get visibility or be seen
      by humans (and not the entire input data). Our motivation for
      investigating the fairness of summarization algorithms comes
      from the concern that using a (inadvertently) ‘biased’
      summarization algorithm can further reduce the visibility of
      the voice / opinion of a certain group in the summary.</p>
      <p>Let the distribution of the sensitive/protected attribute
      in the input data be termed as the ‘input distribution’. The
      summary will contain a subset of the textual units, which
      will also have a distribution of the same sensitive
      attribute; let this distribution be the ‘summary
      distribution’. We propose to judge the fairness of a
      summarization algorithm by comparing the input distribution
      and the summary distribution, based on fairness notions such
      as ‘adverse impact’ of the U.S. Equal Employment Opportunity
      Commission&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>].</p>
      <p>We perform experiments with two datasets of
      microblogs/tweets, one which contain tweets written by users
      having known gender, and the other containing tweets having
      known political leaning. We apply several well-known
      summarization algorithms over these datasets. We find that
      the distribution of the sensitive attribute (gender or
      political leaning) is very different in the summaries
      generated by different algorithms. In fact, some specific
      classes of the textual units (tweets) are
      <em>under-represented</em> according to the ‘adverse impact’
      notion&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0001">1</a>]
      in the summaries generated by several algorithms. To our
      knowledge, this is the first study which demonstrates that
      applications of summarization algorithms need to consider the
      fairness of such algorithms, along with the standard measures
      of the quality of the summaries.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Experiments and
          Observations</h2>
        </div>
      </header>
      <p>We now describe the datasets and extractive summarization
      algorithms considered for the present work.</p>
      <p><span style=
      "text-decoration: underline;"><strong>Datasets:</strong></span>
      We considered two datasets in which every textual unit is
      annotated with a sensitive attribute (gender or political
      leaning).</p>
      <p><strong>(1) Claritin dataset, that contains tweets about
      the effects of the drug Claritin (details at</strong>
      <em>https://tinyurl.com/claritindataset</em> <strong>). Each
      tweet is annotated with the gender of the user who posted it.
      After removing exact duplicates, we have 4,111 tweets in
      total, out of which 1,556 (37.8%) are written by male users,
      while 2,555 (62.2%) are written by female users.</strong></p>
      <p><strong>(2) US Election dataset&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>], that contains tweets
      posted during the 2016 US Presidential elections. Each tweet
      is annotated as supporting or attacking one of the
      presidential candidates (Donald Trump and Hillary Clinton) or
      neutral or attacking both. For simplicity, we grouped the
      tweets into three classes: (i)&nbsp;</strong>
      <em>Pro-Republican</em> <strong>: tweets which support Trump
      and / or attack Clinton, (ii)&nbsp;</strong>
      <em>Pro-Democratic</em> <strong>: tweets which support
      Clinton and / or attack Trump, and (iii)&nbsp;</strong>
      <em>Neutral</em> <strong>: tweets which are neutral or attack
      both candidates. After removing exact duplicates, we have
      3,450 tweets in total, out of which 2,115 (61.3%) are
      Pro-Republican, 1,114 (32.3%) are Pro-Democratic, and 221
      (6.4%) are Neutral tweets.</strong></p>
      <p><strong><span style=
      "text-decoration: underline;">Summarization
      algorithms:</span></strong></p>
      <p><strong>We consider seven well-known extractive
      summarization algorithms. These algorithms generally estimate
      an importance score for each textual unit (sentence / tweet)
      in the input, and include the textual units in the summary in
      decreasing order of this score, until a pre-defined length of
      the summary is reached.</strong></p>
      <p><strong>(1)&nbsp;Cluster-rank [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>] which clusters the textual
      units to form a cluster-graph, and uses graph algorithms
      (e.g., PageRank) to compute the importance of each
      unit;</strong></p>
      <p><strong>(2)&nbsp;DSDR [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0007">7</a>] which measures the relationship
      between the textual units using linear combinations and
      reconstructions, and generates the summary by minimizing the
      reconstruction error;</strong></p>
      <p><strong>(3)&nbsp;Frequency Summarizer [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0004">4</a>], which assumes that if a
      textual unit contains the most frequent words, it is likely
      to be a good candidate for including in the
      summary;</strong></p>
      <p><strong>(4)&nbsp;LexRank [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>], which computes the importance of
      textual units using eigenvector centrality on a graph
      representation based on similarity of the units;</strong></p>
      <p><strong>(5)&nbsp;LSA [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0006">6</a>], which constructs a terms-by-units
      matrix, and estimates the importance of the textual units
      based on SVD on the matrix;</strong></p>
      <p><strong>(6)&nbsp;LUHN [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0008">8</a>], which derives a ‘significance
      factor’ for each textual unit based on occurrences and
      placements of frequent words within the unit;</strong></p>
      <p><strong>(7)&nbsp;SumBasic [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0009">9</a>], which uses frequency-based selection
      of textual units, and re-weighting of the word probabilities
      to minimize redundancy.</strong></p>
      <p><strong><span style=
      "text-decoration: underline;">Results:</span>We apply the
      summarization algorithms stated earlier on the two datasets,
      to obtain summaries of length 100 tweets each.
      Table&nbsp;<a class="tbl" href="#tab1">1</a> shows the
      results of summarizing the two datasets – shown are the
      numbers of tweets of the different classes in the whole
      dataset (first row), and in the summaries generated by the
      different summarization algorithms (subsequent
      rows).</strong></p>
      <p><strong>It is clear that summaries produced by different
      summarization algorithms vary considerably, and contain very
      different distributions of the gender / political leaning, as
      compared to the distribution in the whole input data. For
      instance, for the Claritin dataset, the LSA algorithm
      increases the proportion of tweets from female users in the
      summary, while FreqSum, DSDR, LexRank and LUHN increase the
      proportion of tweets from male users in their summaries. In
      fact, while the whole dataset contains pre-dominantly tweets
      from female users (only 37.8% from male), the summaries by
      LexRank and LUHN contain pre-dominantly tweets from male
      users (51% and 55% male). The observations are similar for
      the US Election dataset. While the whole dataset is
      predominantly pro-Republican (61.3%), all the algorithms
      reduce the proportion of pro-Republican tweets in the
      summary. Especially, the summaries generated by LexRank and
      LSA have the opposite majority, i.e., include more
      pro-Democratic tweets than pro-Republican tweets. Clearly, a
      person who reads the summaries might form a very different
      view or opinion, as compared to reading the whole
      dataset.</strong></p>
      <p><strong>We verify the fairness of the summarization using
      the notion of ‘adverse impact’ used by the U.S. Equal
      Employment Opportunity Commission to determine whether a
      company's hiring policy has any adverse impact on a
      demographic group&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>]. According to this policy, a
      particular class <em>c</em> is</strong>
      <em>under-represented</em> <strong>(disadvantaged) in the
      selected set (i.e., the summary), if the fraction of selected
      items belonging to class <em>c</em> is less than 80% of the
      fraction of selected items of that class which has the
      highest selection rate. Applying this rule, we find
      under-representation of particular classes of tweets in the
      summaries generated by many of the algorithms; these cases
      are marked with an asterisk (*) in Table&nbsp;<a class="tbl"
      href="#tab1">1</a>.</strong></p>
      <p><strong>We repeated the experiments for summaries of
      lengths other than 100 as well, such as for 50, 200, 300, …,
      500 (details omitted due to lack of space). We observed
      several cases where the same algorithm includes very
      different proportions of tweets of various classes, while
      generating summaries of different lengths. Hence, whether
      summarization is fair depends on several factors, including
      the particular algorithm used and the length of
      summary.</strong></p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title"><strong>Results of summarizing the
          (i)&nbsp;Claritin dataset, and (ii)&nbsp;US Election
          dataset: Number of tweets of different classes, in the
          whole dataset and the summaries of length100 tweets
          computed by different algorithms. * indicates
          under-representation of a class according to the fairness
          notion of ‘adverse impact’&nbsp;[<a class="bib"
          data-trigger="hover" data-toggle="popover"
          data-placement="top" href="#BibPLXBIB0001">1</a>]
          (details in text).</strong></span>
        </div>
        <table class="table">
          <tbody>
            <tr>
              <td style="text-align:center;">
              <strong>Method</strong></td>
              <td colspan="2" style="text-align:center;">
                <strong>Claritin dataset</strong>
                <hr />
              </td>
              <td colspan="3" style="text-align:center;">
                <strong>US Election dataset</strong>
                <hr />
              </td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">
              <strong>Male</strong></td>
              <td style="text-align:center;">
              <strong>Female</strong></td>
              <td style="text-align:center;">
              <strong>Pro-Rep</strong></td>
              <td style="text-align:center;">
              <strong>Pro-Dem</strong></td>
              <td style="text-align:center;">
              <strong>Neutral</strong></td>
            </tr>
            <tr>
              <td style="text-align:center;">Whole data</td>
              <td style="text-align:center;">1,556</td>
              <td style="text-align:center;">2,555</td>
              <td style="text-align:center;">2,115</td>
              <td style="text-align:center;">1,114</td>
              <td style="text-align:center;">221</td>
            </tr>
            <tr>
              <td style="text-align:center;"></td>
              <td style="text-align:center;">(37.8%)</td>
              <td style="text-align:center;">(62.2%)</td>
              <td style="text-align:center;">(61.3%)</td>
              <td style="text-align:center;">(32.3%)</td>
              <td style="text-align:center;">(6.4%)</td>
            </tr>
            <tr>
              <td style="text-align:center;">ClusterRank</td>
              <td style="text-align:center;">37</td>
              <td style="text-align:center;">63</td>
              <td style="text-align:center;">55<sup>*</sup></td>
              <td style="text-align:center;">38</td>
              <td style="text-align:center;">7</td>
            </tr>
            <tr>
              <td style="text-align:center;">DSDR</td>
              <td style="text-align:center;">43</td>
              <td style="text-align:center;">57</td>
              <td style="text-align:center;">56<sup>*</sup></td>
              <td style="text-align:center;">38</td>
              <td style="text-align:center;">6<sup>*</sup></td>
            </tr>
            <tr>
              <td style="text-align:center;">FreqSumm</td>
              <td style="text-align:center;">46</td>
              <td style="text-align:center;">54<sup>*</sup></td>
              <td style="text-align:center;">52<sup>*</sup></td>
              <td style="text-align:center;">43</td>
              <td style="text-align:center;">5<sup>*</sup></td>
            </tr>
            <tr>
              <td style="text-align:center;">LexRank</td>
              <td style="text-align:center;">51</td>
              <td style="text-align:center;">49<sup>*</sup></td>
              <td style="text-align:center;">48<sup>*</sup></td>
              <td style="text-align:center;">49</td>
              <td style="text-align:center;">3<sup>*</sup></td>
            </tr>
            <tr>
              <td style="text-align:center;">LSA</td>
              <td style="text-align:center;">32<sup>*</sup></td>
              <td style="text-align:center;">68</td>
              <td style="text-align:center;">42<sup>*</sup></td>
              <td style="text-align:center;">46<sup>*</sup></td>
              <td style="text-align:center;">12</td>
            </tr>
            <tr>
              <td style="text-align:center;">LUHN</td>
              <td style="text-align:center;">55</td>
              <td style="text-align:center;">45<sup>*</sup></td>
              <td style="text-align:center;">58</td>
              <td style="text-align:center;">35</td>
              <td style="text-align:center;">7</td>
            </tr>
            <tr>
              <td style="text-align:center;">SumBasic</td>
              <td style="text-align:center;">37</td>
              <td style="text-align:center;">63</td>
              <td style="text-align:center;">52<sup>*</sup></td>
              <td style="text-align:center;">44</td>
              <td style="text-align:center;">4<sup>*</sup></td>
            </tr>
          </tbody>
          <tfoot>
            <tr>
              <td><strong></strong></td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
              <td></td>
            </tr>
          </tfoot>
        </table>
      </div>
    </section>
    <section id="sec-6">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Conclusion and
          Future Directions</h2>
        </div>
      </header>
      <p><strong>This is the first work that introduces the concept
      of fairness of text summarization algorithms. We show that
      while summarizing datasets having an associated sensitive
      attribute, one needs to verify the fairness of the summary.
      Especially, with the advent of neural network-based
      summarization algorithms (which involve supervised learning),
      the question of fairness becomes even more critical. We
      believe that this work will open up interesting research
      problems, e.g., on developing algorithms that will ensure
      some degree of fairness in the summaries.</strong></p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Dan Biddle. 2006.
        <em><em>Adverse Impact and Test Validation: A
        Practitioner's Guide to Valid and Defensible Employment
        Testing</em></em> . Routledge.</li>
        <li id="BibPLXBIB0002" label="[2]">K. Darwish, W. Magdy,
        and Zanouda T.2017. Trump vs. Hillary: What Went Viral
        During the 2016 US Presidential Election. In <em><em>Proc.
        Social Informatics (SocInfo)</em></em> .</li>
        <li id="BibPLXBIB0003" label="[3]">Günes Erkan and
        Dragomir&nbsp;R. Radev. 2004. LexRank: Graph-based Lexical
        Centrality As Salience in Text Summarization. <em><em>J.
        Artif. Int. Res.</em></em> 22, 1 (2004).</li>
        <li id="BibPLXBIB0004" label="[4]">freqsum2014. Text
        summarization with NLTK. (2014).
        https://tinyurl.com/frequency-summarizer.</li>
        <li id="BibPLXBIB0005" label="[5]">Nikhil Garg and others.
        2009. Clusterrank: a graph based method for meeting
        summarization. In <em><em>Proc. INTERSPEECH</em></em> .
        ISCA.</li>
        <li id="BibPLXBIB0006" label="[6]">Yihong Gong and Xin Liu.
        2001. Generic Text Summarization Using Relevance Measure
        and Latent Semantic Analysis. In <em><em>Proc. ACM
        SIGIR</em></em> .</li>
        <li id="BibPLXBIB0007" label="[7]">Zhanying He and others.
        2012. Document Summarization Based on Data Reconstruction.
        In <em><em>Proc. AAAI Conference on Artificial
        Intelligence</em></em> .</li>
        <li id="BibPLXBIB0008" label="[8]">H.&nbsp;P. Luhn. 1958.
        The Automatic Creation of Literature Abstracts. <em><em>IBM
        J. Res. Dev.</em></em> 2, 2 (April 1958), 159–165.</li>
        <li id="BibPLXBIB0009" label="[9]">Ani Nenkova and Lucy
        Vanderwende. 2005. <em><em>The impact of frequency on
        summarization</em></em> . Technical Report. Microsoft
        Research.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186947">https://doi.org/10.1145/3184558.3186947</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
