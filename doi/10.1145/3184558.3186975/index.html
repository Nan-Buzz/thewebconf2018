<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>DARQL: Deep Analysis of SPARQL Queries⁎⁎This work is
  supported by the Deutsche Forschungsgemeinschaft dfg under Grant
  No: MA 4938/2–1 dfg .</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">DARQL: Deep Analysis of SPARQL
          Queries<a class="fn" href="#fn1" id=
          "foot-fn1"><sup>⁎</sup></a></span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Angela</span> <span class=
          "surName">Bonifati</span> Lyon 1 University, <a href=
          "mailto:angela.bonifati@univ-lyon1.fr">angela.bonifati@univ-lyon1.fr</a>
        </div>
        <div class="author">
          <span class="givenName">Wim</span> <span class=
          "surName">Martens</span> University of Bayreuth, <a href=
          "mailto:wim.martens@uni-bayreuth.de">wim.martens@uni-bayreuth.de</a>
        </div>
        <div class="author">
          <span class="givenName">Thomas</span> <span class=
          "surName">Timm</span> University of Bayreuth, <a href=
          "mailto:thomas.timm@uni-bayreuth.de">thomas.timm@uni-bayreuth.de</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3184558.3186975"
        target=
        "_blank">https://doi.org/10.1145/3184558.3186975</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3184558" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>In this demonstration, we showcase DARQL, the
        first tool for deep, large-scale analysis of SPARQL
        queries. We have harvested a large corpus of query logs
        with different lineage and sizes, from DBPedia to BioPortal
        and Wikidata, whose total number of queries amounts to
        180M. We ran a wide range of analyses on the corpus,
        spanning from simple tasks (keyword counts, triple counts,
        operator distributions), moderately deep tasks (projection
        test, query classification), and deep analysis (shape
        analysis, well-designedness, weakly well-designedness,
        hypertreewidth, and fractional edge cover). The key goal of
        our demonstration is to let the users dive into the SPARQL
        query logs of our corpus and let them discover the inherent
        characteristics of the queries.</small></p>
        <p><small>The entire corpus of SPARQL queries is stored in
        a DBMS. The tool has a GUI that allows users to ask
        sophisticated analytical queries on the SPARQL logs. These
        analytical queries can both be directly written in SQL or
        composed by a visual query builder tool. The results of the
        analytical queries are represented both textually (as
        SPARQL queries) and visually. The DBMS performs the
        searches within the corpus quite efficiently. To the best
        of our knowledge, this is the first demonstration of this
        kind on such a large corpus and with such a number of
        varied tests.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Information systems</strong> →
        <strong>Query languages for non-relational
        engines;</strong> <strong>Query log analysis;</strong> •
        <strong>Theory of computation</strong> → <em>Database query
        languages (principles);</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>RDF</small>,</span>
          <span class="keyword"><small>SPARQL</small>,</span>
          <span class="keyword"><small>Conjunctive
          Queries</small>,</span> <span class=
          "keyword"><small>Query Analysis</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Angela Bonifati, Wim Martens, and Thomas Timm. 2018.
          DARQL: Deep Analysis of SPARQL Queries. In <em>WWW '18
          Companion: The 2018 Web Conference Companion,</em>
          <em>April 23–27, 2018 (WWW ’18 Companion),</em> <em>Lyon,
          France. ACM, New York, NY, USA</em> 5 Pages. <a href=
          "https://doi.org/10.1145/3184558.3186975" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3184558.3186975</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>A plethora of SPARQL endpoints<a class="fn" href="#fn3"
      id="foot-fn3"><sup>1</sup></a> is proliferating on the
      Internet thus allowing ordinary users to specify their
      queries either via APIs or manually. This phenomenon is
      leading to a democratization of query formulation. The
      queries are collected into log files by their respective
      owners and represent a valuable resource for understanding
      users’ preferences and needs in terms of query specification,
      but also for guiding us in research on query language design,
      query evaluation and benchmarking [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0002">2</a>].</p>
      <p>Motivated by previous studies on SPARQL log analysis
      [<a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0003">3</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>], we could
      harvest a large and varied corpus of SPARQL query logs
      amounting to a total of 180M queries [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>], which is several orders
      of magnitude more than earlier analytical studies on SPARQL
      query logs. In this demonstration, we showcase , a tool for
      deep and fast analysis of large SPARQL query logs. The tool
      comes equipped with an extensive set of pre-defined tests,
      including simple tasks (keyword counts, triple counts,
      operator distributions), moderately deep tasks (projection
      test, query classification), and deep analysis (shape
      analysis, well-designedness, weakly well-designedness,
      hypertreewidth, and fractional edge cover). The primary goal
      of our demonstration is to let the users dive into the SPARQL
      query logs of our corpus and let them discover the inherent
      characteristics of the queries. A secondary goal is to
      advertise as an easy-to-use tool for SPARQL query analysis in
      the research community. Out of the box, analyzes 62
      properties per query. We believe that will give researchers
      who want to dive into query log analysis a significant head
      start. Indeed, in our former analytical study [<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>] we only scratched the
      surface when it comes to finding correlations between query
      properties.</p>
      <p>We will demonstrate our tool by using queries from the
      same corpus considered in [<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>]. Since many of these queries are not
      publicly available, the demonstration may give visitors a
      unique opportunity to get an idea of how queries actually
      look like in practice. We release the tool itself at
      <a class="link-inline force-break" href=
      "https://github.com/PoDMr/darql">https://github.com/PoDMr/darql</a>.</p>
    </section>
    <section id="sec-5">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> System and Main
          Components</h2>
        </div>
      </header>
      <p>A query builder lets the user modify the features of the
      queries under scrutiny to respectively enlarge or restrict
      the scope of the analyzed portion of the corpus. Since our
      tool is deployed on top of a relational DBMS with a web-based
      front end, each search on the corpus corresponds to an SQL
      query issued on the database. The user can also manually
      modify this query and rerun a deeper or coarser analysis at
      will.</p>
      <p>Internally, the system consists of the following main
      components:</p>
      <ul class="list-no-style">
        <li id="list1" label="•">a batch processing system (for
        loading and analyzing query logs, writing to
        files),<br /></li>
        <li id="list2" label="•">a PostgreSQL 10 database,
        and<br /></li>
        <li id="list3" label="•">a GUI served from a connecting
        back-end.<br /></li>
      </ul>
      <p>We refer to Figure&nbsp;<a class="fig" href="#fig1">1</a>
      for a slightly more detailed overview. We describe the main
      components next.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186975/images/www18companion-215-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">General architecture of the system.</span>
        </div>
      </figure>
      <p></p>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span> Loading and
            Analyzing Query Logs</h3>
          </div>
        </header>
        <p>We release as open source project and therefore also
        discuss some aspects that will not be part of the demo,
        such as the batch processing system. Before the user can
        start exploring queries through the database interface, the
        queries need to be analyzed, deduplicated, and stored into
        the system. To this end, we provide scripts that can handle
        three main log formats: CLF-based logs (Common Log Format)
        used by most web servers, delimiter-based line log formats
        (e.g. CSV, TSV), or multi-line delimited formats. Every
        query in the log is parsed and stored into the database.
        For queries that do not parse, we record this in the
        database and do not perform further analysis. Every query
        that parses is run through an extensive set of analytical
        tests: 1 parse test, 31 keyword tests (query type,
        operators, solution modifiers, aggregation operators, ...),
        8 simple structural tests (property path, projection, ...),
        4 well-designedness tests, 3 classifications into different
        kinds of conjunctive queries, 11 complex shape tests for
        the structure of conjunctive queries (chain, star, cycle,
        tree, flower,...), and 4 value tests (number of triples of
        the queries, hypertreewidth, fractional edge cover, and the
        origin of the query logs). These tests include (but are not
        limited to) all those that have been used in [<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0001">1</a>].</p>
        <p>Prior to analysis, we test for duplicates in the query
        logs. We use SHA-256 for hashing to detect potential
        duplicates. The strings of queries are normalized by
        outputting them with the Jena parser, which normalizes
        whitespace and formats the queries in a readable form for
        our GUI. For some of the logs, we also need to add implicit
        prefixes explicitly to make queries valid as standalone
        queries. If we discover duplicate queries we do not re-run
        query analysis, but simply record its occurrence by
        referring to the first occurrence and store the new origin
        (log file and line number). As such, our system can
        display, for each query, how many duplicates were found and
        where.</p>
        <p>The batch processing system can write output to either
        files or databases. It also allows logs to be rewritten in
        different formats, and deduplicated in a normalized form.
        The analysis output can be transformed to spreadsheets.</p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span>
            Database</h3>
          </div>
        </header>
        <p>The database is a PostgreSQL 10 system that stores the
        results of each analysis for every query. For duplicate log
        entries it stores the origin of each entry. We tested the
        database with the dataset of roughly 180M SPARQL queries
        from [<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0001">1</a>]. This dataset is about two orders
        of magnitude larger than those found in earlier studies. We
        used PostgreSQL 10 in order to utilize new parallelization
        features for queries and joins, which after appropriate
        tuning resulted in much faster query times compared to
        PostgreSQL 9.6.</p>
        <p>We tested the system through a Web interface on our
        server<a class="fn" href="#fn4" id=
        "foot-fn4"><sup>2</sup></a> and noticed that most queries
        that our Query Builder generates are done in less than a
        second (e.g. 200ms).<a class="fn" href="#fn5" id=
        "foot-fn5"><sup>3</sup></a> Count queries are generally
        more expensive in PostgreSQL and take between 2–4 seconds
        on large sets (50–100 million queries) and are faster if
        subsets are smaller. In order to achieve this performance,
        we created a set of indexes. Index creation can take up to
        2–4 minutes.</p>
        <p>All non-duplicate queries are stored in a single table
        <tt>Queries</tt> with the information of analysis results
        as boolean or numeric columns. Each query is assigned a
        unique id. Additionally, it contains the string of the
        query, a hash of this string, and its origin (data set,
        filename, line number in file). Duplicates are stored in a
        table <tt>Duplicates</tt>, containing an id for the
        duplicate, a reference id to the original query, and the
        origin of the duplicate.</p>
      </section>
      <section id="sec-8">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> User
            Interface</h3>
          </div>
        </header>
        <p>The user interface consists of several components that
        are connected. The main ones are: (1) a query builder, (2)
        an SQL text editor, (3) a query visualizer, (4) a SPARQL
        text display, and (5) a query result table display.</p>
        <p>In a typical usage scenario of the tool, the user
        employs the query builder to select properties that she is
        interested in. Figure&nbsp;2(a) shows a partial screenshot
        of the query builder. Under “Query Types”, one can click if
        one wants to search for SELECT, ASK, CONSTRUCT, or DESCRIBE
        queries. Likewise, we have categories for “Operators”,
        “Solution Modifiers”, “Well Designedness”, “Shapes”, etc.
        As Figure&nbsp;2(a) shows, groups of properties can be
        folded and unfolded at need. Boolean properties can be set
        to True, False, or N/A (don't care). For numerical
        properties (e.g., number of triples, hypertreewidth), we
        simply allow that the user enters the desired number or
        range.</p>
        <p>When the user clicks “Run”, the GUI automatically
        generates an SQL query that fetches up to ten queries from
        our query logs and displays it in the SQL text editor, see
        Figure&nbsp;2(b) for an example query. The SQL query in the
        editor is fully editable, in case the user wants to refine
        the search.</p>
        <p>Upon executing the SQL query, several things happen at
        once:</p>
        <ul class="list-no-style">
          <li id="list4" label="•">its results are shown in the
          query result table display (shown in
          Figure&nbsp;3(a));<br /></li>
          <li id="list5" label="•">the first result is displayed in
          the SPARQL text display (shown in Figure&nbsp;3(b))
          and<br /></li>
          <li id="list6" label="•">the first result is visualized
          in the query visualizer (shown in Figure&nbsp;3(c) and
          3(d)).<br /></li>
        </ul>
        <p>The entries in the result table display are clickable,
        so the user can immediately select a query she is
        interested in (e.g., the second entry in Figure&nbsp;3(a)).
        When clicking a query, it is shown in the SPARQL editor and
        visualized (e.g., Figure&nbsp;3(b) –3(d)) show the
        highlighted query from Figure&nbsp;3(a) and two different
        visualizations). Furthermore, the query visualiser has
        links for going to the previous and next query. Additional
        properties of the currently displayed query can be shown in
        a “Details” panel (for which we did not provide a
        screenshot).</p>
        <figure id="fig2">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186975/images/www18companion-215-fig2.jpg"
          class="img-responsive" alt="Figure 2" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 2:</span>
            <span class="figure-title">Query Builder and SQL text
            editor</span>
          </div>
        </figure>
        <figure id="fig3">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186975/images/www18companion-215-fig3.jpg"
          class="img-responsive" alt="Figure 3" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 3:</span>
            <span class="figure-title">Partial screenshots of our
            query viewer and visualizer for an example
            query.</span>
          </div>
        </figure>
        <p></p>
        <p>Figure&nbsp;3(b) shows a query from the DBPedia 2016
        (Jan. 17th) log file, corresponding to one of the results
        of the SQL query in Figure&nbsp;2(b) on our database,
        fetching the queries of hypertreewidth two. The visualizer
        automatically renders a visualization of the query and can
        be configured to render the graph- and hypergraph
        structure. For the graph structure, we only consider the
        subject-object parts of SPARQL triples (and ignore the
        “predicate” part). Although the graph structure of a query
        is usually rather simple, it is often not sufficient to
        convey the full complexity of the query. For instance, the
        graph in Figure&nbsp;3(c) ignores the variables ?p1, ?p2,
        and ?p3 from Figure&nbsp;3(b) .</p>
        <p>The hypergraph structure captures this complexity more
        accurately. Since we only had graph render libraries at our
        disposal, we display a hyperedge {<em>s</em>, <em>p</em>,
        <em>o</em>} coming from SPARQL triple (<em>s</em>,
        <em>p</em>, <em>o</em>) as a new node <em>h</em>
        (representing the identity of the hyperedge) which we
        connect to nodes <em>s</em>, <em>p</em>, and <em>o</em>. We
        use the edges <em>s</em> → <em>h</em> and <em>h</em> →
        <em>o</em> (in blue) and <em>h</em> → <em>p</em> (in
        orange). Figure&nbsp;3(d) has such a visualization for the
        SPARQL query in Figure&nbsp;3(b) .</p>
        <p>The visualizer currently uses several graph layout
        algorithms (cose, cose-bilkent, concentric, breadthfirst,
        grid, and circle) and can readily switch between them. This
        gives users a quick idea of the query's structure.</p>
        <p>Finally, we provide a datasets panel (shown in
        Figure&nbsp;<a class="fig" href="#fig4">4</a>), which shows
        general statistics of the data that is currently in the
        database, ordered by origin. The panel contains four
        columns: name of the dataset (“originMajor”), total number
        of queries (“total”), number of unique queries (“unique”),
        and finally the number of unique queries that can be parsed
        (“unique_valid”). Duplicates are tested globally, so we may
        classify a query from dbpedia_15 as a duplicate if it
        already occured in dbpedia_12.</p>
        <p>Of course, the GUI is such that it can show (and resize)
        all these panels at the same time, in order to give the
        user a complete overview. It also allows to hide the panels
        the user is currently not interested in. The panels can be
        used as stacked tabs or as split views with advanced layout
        capabilities found in IDEs.</p>
        <figure id="fig4">
          <img src=
          "../../../data/deliveryimages.acm.org/10.1145/3190000/3186975/images/www18companion-215-fig4.jpg"
          class="img-responsive" alt="Figure 4" longdesc="" />
          <div class="figure-caption">
            <span class="figure-number">Figure 4:</span>
            <span class="figure-title">Data sets panel from the
            GUI, showing the currently loaded queries.</span>
          </div>
        </figure>
        <p></p>
      </section>
    </section>
    <section id="sec-9">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> Demo
          Overview</h2>
        </div>
      </header>
      <p>We believe that many visitors of the demo will be
      interested to see how queries from actual SPARQL log files
      (DBPedia, BioPortal, LGD, OpenBioMed, Semantic Web Dog Food,
      British Museum) look like, since many of these log files are
      not publically available. Since is very flexible, and
      immediately shows visualization such as in Figures&nbsp;3(c)
      and 3(d), we can do this interactively with visitors of the
      demo. Nevertheless, we will also prepare the following
      specific scenarios to stimulate such interaction.</p>
      <p><em>Search for Complex Queries</em>. Besides searching
      simple queries in our large corpus, the tool allows us to
      quickly search for queries by size (so the largest ones can
      be found quickly), with complex structures (e.g., cyclic
      queries) and with advanced keywords. Since our corpus
      encompasses a varied set of SPARQL log files coming from
      disparate SPARQL endpoints, lets the users access the lineage
      of the queries under inspection and have a perception of what
      logs contain queries with certain complex
      characteristics.</p>
      <p><em>Shape-Driven Exploration</em>. Here we start by
      selecting a specific shape (e.g., “star”) and show visitors
      on the visualizer how star-shaped queries in the log files
      actually look like. This gives an impression on the size,
      complexity, branching, and diameter of such queries. The tool
      supports many different shapes to start from, among which
      star, tree, chain, forest, flower, and cycle. We can also
      start from a given hypertreewidth. (Queries with
      hypertreewidth three are already quite complex and rare in
      the query logs.)</p>
      <p>In fact we already used the front-end extensively for our
      study of shapes in [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0001">1</a>]: we gradually implemented more and
      more shape tests and then visually inspected queries that
      were not classified by any known shapes. So, this part of the
      tool has already been heavily used behind-the-scenes in our
      own research.</p>
      <p><em>Getting Statistics</em>. We will demonstrate how
      statistics such as “How many of the SELECT queries are
      conjunctive queries?” or “How many of the construct-queries
      do a non-trivial insertion?” can be computed. For every query
      that can be constructed with the query builder, we can toggle
      if the tool should count the number of the results, or if it
      should produce a sample of the answers. Therefore, such
      statistics can be easily computed.</p>
      <p><em>Most Popular Queries</em>. Statistics on the logs can
      be easily computed and lead to identify for instance the most
      (or the least) occurring queries in the entire corpus or in a
      single log file or data source. We can thus access the most
      (or less) popular queries in the logs with a breakdown view
      on each individual log file, on each individual data source
      (e.g., Wikidata, DBPedia, BioPortal etc.) or on the entire
      corpus.</p>
      <p><em>Expert Search in SQL</em>. The predefined user
      interface only generated SQL queries to the database that
      test conjunctions of conditions. Using the direct SQL
      interface, we show that if the user wants, also more advanced
      conditions can be queried. For example, one can search the
      union of all queries that have a minimum size and those that
      have cycles.</p>
      <p>Furthermore, it is possible to explore queries in the
      context of time. Although only some logs have timestamps,
      most log files have names that indicate a date.
      Figure&nbsp;3(a) shows five queries from January 17th, 2016,
      for example. Using the SQL editor, queries coming from a
      specific date (or month, or year) can be found as well. With
      this date, we could perform complex interesting queries with
      a temporal aspect. For instance, we could inspect how many
      queries were submitted on the same day, or try to find days
      or time spans that have the most or least queries. Or, we
      could calculate the time span (first and last occurrence) for
      duplicates of a query. A user can design her own very complex
      log searches by formulating them as queries in SQL.</p>
      <p>To conclude, we want to give visitors of the demo the
      opportunity to browse in our repository consisting of 180
      million SPARQL queries, gathered from 2007 to 2017 and
      ranging over a wide range of sources. We also want to
      advertise itself as a useful tool for analysing SPARQL logs.
      We believe that the tool is indeed useful for analyzing large
      logs. The demo will show that most queries run very fast,
      i.e., faster than a second.</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Angela Bonifati, Wim
        Martens, and Thomas Timm. 2017. An Analytical Study of
        Large SPARQL Query Logs. <em><em>PVLDB</em></em> 11, 2
        (2017), 149–161.</li>
        <li id="BibPLXBIB0002" label="[2]">Mark Kaminski and
        Egor&nbsp;V. Kostylev. 2016. Beyond Well-designed SPARQL.
        In <em><em>International Conference on Database Theory
        (ICDT)</em></em> . 5:1–5:18.</li>
        <li id="BibPLXBIB0003" label="[3]">Francois Picalausa and
        Stijn Vansummeren. 2011. What Are Real SPARQL Queries Like?
        In <em><em>SWIM</em></em> . ACM, New York, NY, USA, Article
        7, 6&nbsp;pages.</li>
        <li id="BibPLXBIB0004" label="[4]">Muhammad Saleem,
        Muhammad&nbsp;Intizar Ali, Aidan Hogan, Qaiser Mehmood, and
        Axel-Cyrille Ngonga Ngomo. 2015. LSQ: The Linked SPARQL
        Queries Dataset. In <em><em>International Semantic Web
        Conference (ISWC)</em></em> . 261–269.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>⁎</sup></a>This work is
    supported by the Deutsche Forschungsgemeinschaft dfg under
    Grant No: MA 4938/2–1 dfg .</p>
    <p id="fn3"><a href="#foot-fn3"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.w3.org/wiki/SparqlEndpoints">https://www.w3.org/wiki/SparqlEndpoints</a></p>
    <p id="fn4"><a href="#foot-fn4"><sup>2</sup></a>A 2-CPU Intel
    Xeon E5-2630v2 2.6 GHz server with 128GB RAM and running Ubuntu
    16.04 LTS.</p>
    <p id="fn5"><a href="#foot-fn5"><sup>3</sup></a>We discuss the
    Query Builder in more detail in Section&nbsp;<a class="sec"
    href="#sec-8">2.3</a>.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23-27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License. ACM ISBN 978-1-4503-5640-4/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3184558.3186975">https://doi.org/10.1145/3184558.3186975</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
