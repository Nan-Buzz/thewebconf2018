<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
  <meta name="generator" content=
  "HTML Tidy for HTML5 for Linux version 5.7.16" />
  <title>Estimating Rule Quality for Knowledge Base Completion with
  the Relationship between Coverage Assumption</title>
  <!-- Copyright (c) 2010-2015 The MathJax Consortium -->
  <meta http-equiv="Content-Type" content=
  "text/html; charset=utf-8" />
  <meta name="viewport" content=
  "width=device-width; initial-scale=1.0;" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css" />
  <link media="screen, print" rel="stylesheet" href=
  "../../../data/dl.acm.org/pubs/lib/css/main.css" />
  <script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js"
  type="text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type=
  "text/javascript"></script>
  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type=
  "text/javascript"></script>
  <script type="text/javascript" src=
  "https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>
  <script type="text/x-mathjax-config">
  <![CDATA[
  MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});
  ]]>
  </script>
</head>
<body id="main">
  <section class="front-matter">
    <section>
      <header class="title-info">
        <div class="journal-title">
          <h1><span class="title">Estimating Rule Quality for
          Knowledge Base Completion with the Relationship between
          Coverage Assumption</span><br />
          <span class="subTitle"></span></h1>
        </div>
      </header>
      <div class="authorGroup">
        <div class="author">
          <span class="givenName">Kaja</span> <span class=
          "surName">Zupanc</span>, University of Ljubljana, Faculty
          of Computer and Information Science, Ljubljana, Slovenia,
          <a href=
          "mailto:kaja.zupanc@fri.uni-lj.si">kaja.zupanc@fri.uni-lj.si</a>
        </div>
        <div class="author">
          <span class="givenName">Jesse</span> <span class=
          "surName">Davis</span>, KU Leuven, Department of Computer
          Science, Leuven, Belgium, <a href=
          "mailto:jesse.davis@cs.kuleuven.be">jesse.davis@cs.kuleuven.be</a>
        </div>
      </div><br />
      <div class="pubInfo">
        <p>DOI: <a href="https://doi.org/10.1145/3178876.3186006"
        target=
        "_blank">https://doi.org/10.1145/3178876.3186006</a><br />
        WWW '18: <a href="https://doi.org/10.1145/3178876" target=
        "_blank">Proceedings of The Web Conference 2018</a>, Lyon,
        France, April 2018</p>
      </div>
      <div class="abstract">
        <p><small>Currently, there are many large, automatically
        constructed knowledge bases (KBs). One interesting task is
        learning from a knowledge base to generate new knowledge
        either in the form of inferred facts or rules that define
        regularities. One challenge for learning is that KBs are
        necessarily open world: we cannot assume anything about the
        truth values of tuples not included in the KB. When a KB
        only contains facts (i.e., true statements), which is
        typically the case, we lack negative examples, which are
        often needed by learning algorithms. To address this
        problem, we propose a novel score function for evaluating
        the quality of a first-order rule learned from a KB. Our
        metric attempts to include information about the tuples not
        in the KB when evaluating the quality of a potential rule.
        Empirically, we find that our metric results in more
        precise predictions than previous approaches.</small></p>
      </div>
      <div class="CCSconcepts">
        <p><small><span style="font-weight:bold;">CCS
        Concepts:</span> • <strong>Computing methodologies</strong>
        → <strong>Rule learning;</strong> <em>Knowledge
        representation and reasoning;</em> <em>Logical and
        relational learning;</em> • <strong>Information
        systems</strong> → <em>Association rules;</em></small></p>
      </div>
      <div class="classifications">
        <div class="author">
          <span style=
          "font-weight:bold;"><small>Keywords:</small></span>
          <span class="keyword"><small>Rule mining</small>,</span>
          <span class="keyword"><small>Knowledge
          base</small>,</span> <span class=
          "keyword"><small>ILP</small>,</span> <span class=
          "keyword"><small>Open world assumption</small></span>
        </div><br />
        <div class="AcmReferenceFormat">
          <p><small><span style="font-weight:bold;">ACM Reference
          Format:</span><br />
          Kaja Zupanc and Jesse Davis. 2018. Estimating Rule
          Quality for Knowledge Base Completion with the
          Relationship between Coverage Assumption. In <em>WWW
          2018: The 2018 Web Conference,</em> <em>April 23–27,
          2018, Lyon, France</em>. ACM, New York, NY, USA, 9 pages.
          <a href="https://doi.org/10.1145/3178876.3186006" class=
          "link-inline force-break" target=
          "_blank">https://doi.org/10.1145/3178876.3186006</a></small></p>
        </div>
      </div>
    </section>
  </section>
  <section class="body">
    <section id="sec-3">
      <header>
        <div class="title-info">
          <h2><span class="section-number">1</span>
          Introduction</h2>
        </div>
      </header>
      <p>Knowledge bases (KBs) store structured relational data
      such as “Aaron Rodgers plays for the Green Bay Packers” and
      “the Green Bay Packers are a football team.” Some of the most
      prominent KBs are YAGO&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0022">22</a>], Wikidata,<a class="fn" href="#fn1"
      id="foot-fn1"><sup>1</sup></a> DBpedia&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0001">1</a>], NELL&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0005">5</a>], Freebase&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0004">4</a>], Google
      Knowledge Graph&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0030">30</a>] and Cyc&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0011">11</a>]. Typically, these KBs are
      automatically populated by mining text found on the Web. Due
      to the variable quality of Web text and the desire to have
      accurate KBs, these approaches typically are biased towards
      ensuring that only high-quality tuples (i.e., those likely to
      be correct) are included in the KB. These KBs are constantly
      growing in size, as mining is an iterative and ongoing
      process and current KBs can involve millions of different
      entities and contain hundreds of millions of facts.</p>
      <p>One interesting task is mining a constructed knowledge
      base for rules such as the following:</p>
      <div class="table-responsive" id="eq1">
        <div class="display-equation">
          <span class="tex mytex">\begin{eqnarray} \mathtt
          {isPoliticianOf(x,y)} \wedge \mathtt {diedIn(x,y)}
          \Rightarrow \mathtt {livesIn(x,y)}.
          \end{eqnarray}</span><br />
          <span class="equation-number">(1)</span>
        </div>
      </div>This is interesting from several perspectives. First,
      the rules capture regularities in the data, and are therefore
      an interesting source of knowledge in and of themselves.
      Second, while these KBs are quite large, they are necessarily
      incomplete. The rules can be used for inferring additional
      facts to help complete the KB (e.g., as done
      in&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0005">5</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0007">7</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0020">20</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>]). Such
      rules need to be learned from data (that is, the KB). This
      can be naturally posed in the typical inductive logic
      programming setup&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>], where we want to learn rules that
      as many correct predictions as possible and no (or few)
      incorrect ones. However, the vast majority of work for
      learning such rules relies on having access to labeled
      positive and negative examples, whereas in this setting we
      only have access to positive examples.
      <p></p>
      <p>Several approaches have attempted to tackle this problem.
      The most obvious approach is to make the closed-world
      assumption, and simply presume that any tuple not in the KB
      is a negative example. This is clearly false, and has been
      shown to perform poorly in practice&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0012">12</a>]. As an alternative,
      Galarraga et al.&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0012">12</a>] proposed assuming that only certain
      unobserved tuples are false while making no assumptions about
      the truth values of the remaining ones. Another solution is
      to randomly sample some unobserved tuples to serve as
      negative examples, which is often done in conjunction with
      domain knowledge. For example, the initial version of NELL
      employed a semi-supervised approach that exploited mutual
      exclusivity constraints on certain argument types for a given
      relation&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0005">5</a>]. Finally, people have also
      investigated a variety of metrics that only rely on using
      positive examples to evaluate such rules&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0023">23</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0024">24</a>, <a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0028">28</a>].</p>
      <p>This paper proposes to look at learning such rules from
      the perspective of learning from both positive and unlabeled
      data&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0008">8</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>]. This
      learning framework assumes that a learner has access to
      positive examples and unlabeled data, where the unlabeled
      data contains a mix of positive and negative examples. We
      propose a novel confidence metric for evaluating candidate
      rules that reasons about the unlabeled data. Empirically, we
      compare our metric to Galarraga et al.’s
      metric&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0012">12</a>]
      on three data sets and to Gardner et al.’s
      approach&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0013">13</a>] on the NELL KB and find that we
      achieve superior performance.</p>
    </section>
    <section id="sec-4">
      <header>
        <div class="title-info">
          <h2><span class="section-number">2</span> Background</h2>
        </div>
      </header>
      <p>Table&nbsp;<a class="tbl" href="#tab1">1</a> presents a
      small knowledge base that is used as a running example to
      illustrate the important concepts.</p>
      <div class="table-responsive" id="tab1">
        <div class="table-caption">
          <span class="table-number">Table 1:</span> <span class=
          "table-title">Example KB.</span>
        </div>
        <table class="table">
          <thead>
            <tr>
              <th style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt
              {livesIn}$</span></span></th>
              <th style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt
              {isPoliticianOf}$</span></span></th>
              <th><span class="inline-equation"><span class=
              "tex">$\mathtt {diedIn}$</span></span></th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt {(Ava,
              Paris)}$</span></span></td>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt {(Ava,
              Paris)}$</span></span></td>
              <td><span class="inline-equation"><span class=
              "tex">$\mathtt {(Ava, Paris)}$</span></span></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt {(Emily,
              London)}$</span></span></td>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt {(Bob,
              Newyork)}$</span></span></td>
              <td><span class="inline-equation"><span class=
              "tex">$\mathtt {(Bob, Newyork)}$</span></span></td>
            </tr>
            <tr>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt {(Emily,
              Paris)}$</span></span></td>
              <td style="text-align:center;"><span class=
              "inline-equation"><span class="tex">$\mathtt {(Ava,
              Newyork)}$</span></span></td>
              <td><span class="inline-equation"><span class=
              "tex">$\mathtt {(Emma, Lisbon)}$</span></span></td>
            </tr>
          </tbody>
        </table>
      </div>
      <section id="sec-5">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.1</span>
            Representation</h3>
          </div>
        </header>
        <p>We will use a KB representation based on a subset of
        first-order logic. A <em>constant</em> (e.g., <span class=
        "inline-equation"><span class="tex">$\mathtt
        {Emily}$</span></span> ) refers to a specific entity in the
        domain and starts with an uppercase letter.
        <em>Variables</em> (e.g., <span class=
        "inline-equation"><span class="tex">$\mathtt
        {x}$</span></span> ) range over entities in the domain and
        are denoted by lowercase letters. A <em>predicate</em> or
        relation <span class="inline-equation"><span class=
        "tex">$\mathtt {R}/n$</span></span> , where <em>n</em> is
        the arity, represents a relationship among entities in the
        domain. In this paper, we only consider binary or arity two
        predicates, and will refer to the first argument as the
        <em>subject</em> and the second argument as the
        <em>object</em>. A <em>binary atom</em> is of the form
        <span class="inline-equation"><span class="tex">$\mathtt
        {R(t_1,t_2)}$</span></span> , where each of <span class=
        "inline-equation"><span class="tex">$\mathtt
        {t_1}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathtt
        {t_2}$</span></span> may be either a constant or a
        variable. A <em>literal</em> is an atom or its negation. A
        <em>ground atom</em> is an atom where both <span class=
        "inline-equation"><span class="tex">$\mathtt
        {t_1}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathtt
        {t_2}$</span></span> are constants. A ground atom can be
        either true or false. A true ground atom is called a
        <em>fact</em>. For the KB in Table&nbsp;<a class="tbl"
        href="#tab1">1</a>, an example of a ground fact is
        <span class="inline-equation"><span class="tex">$\mathtt
        {livesIn(Emily, London)}$</span></span> . For a given KB
        <em>K</em>, we use <span class=
        "inline-equation"><span class="tex">$\text{K}_\mathtt
        {R}$</span></span> to refer to the set of all ground facts
        for relation <span class="inline-equation"><span class=
        "tex">$\mathtt {R}$</span></span> that appear in
        <em>K</em>. We use <span class=
        "inline-equation"><span class="tex">$\mathbb {X}_\mathtt
        {R}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathbb {Y}_\mathtt
        {R}$</span></span> to denote the set of constants that
        appear as a subject or object in relation <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> in <em>K</em>, and they are defined as
        follows:</p>
        <div class="table-responsive" id="eq2">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} \mathbb
            {X}_\mathtt {R} &amp;=&amp; \lbrace \mathtt
            {x}~|~\exists \mathtt {y}:\mathtt {R(x,y)} \in
            K_\mathtt {R} \rbrace , \end{eqnarray}</span><br />
            <span class="equation-number">(2)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} \mathbb
            {Y}_\mathtt {R} &amp;=&amp; \lbrace \mathtt {y}
            ~|~\exists \mathtt {x}:\mathtt {R(x,y)} \in K_\mathtt
            {R} \rbrace . \end{eqnarray}</span><br />
            <span class="equation-number">(3)</span>
          </div>
        </div>
        <p></p>
        <p>A Datalog <em>rule</em> can be written as an implication
        <span class="inline-equation"><span class="tex">$\mathtt {B
        \Rightarrow H}$</span></span> . The body <span class=
        "inline-equation"><span class="tex">$\mathtt
        {B}$</span></span> , consists of a conjunction of literals,
        whereas the head <span class="inline-equation"><span class=
        "tex">$\mathtt {H}$</span></span> is a single literal. The
        following are examples of rules:</p>
        <div class="table-responsive" id="eq4">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathtt
            {livesIn(x,y)} \Rightarrow \mathtt {diedIn(x,y)},~
            \end{align}</span><br />
            <span class="equation-number">(4)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq5">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathtt
            {livesIn(x,y)} \Rightarrow \mathtt
            {isPoliticianOf(x,y)},~ \end{align}</span><br />
            <span class="equation-number">(5)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq6">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \mathtt
            {isPoliticianOf(x,y)} \wedge \mathtt {diedIn(x,y)}
            \Rightarrow \mathtt {livesIn(x,y)}~ .
            \end{align}</span><br />
            <span class="equation-number">(6)</span>
          </div>
        </div>
        <p></p>
        <p>A rule is <em>grounded</em> or <em>instantiated</em> if
        all variables have been replaced by constants. An
        instantiation of Rule&nbsp;(<a class="eqn" href=
        "#eq6">6</a>) is</p>
        <div class="table-responsive" id="eq7">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} \mathtt
            {diedIn(Ava, Paris)} \wedge \mathtt
            {isPoliticianOf(Ava,Paris)} &amp; &amp; \nonumber \\
            ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\Rightarrow \mathtt
            {livesIn(Ava,Paris)} \end{eqnarray}</span><br />
            <span class="equation-number">(7)</span>
          </div>
        </div>The head of an instantiated rule is a
        <em>prediction</em> if all the literals in
        therule'sbodyappearin<em>K</em>.In our runningexample,
        <p></p>
        <p><span class="inline-equation"><span class="tex">$\mathtt
        {livesIn(Ava, Paris)}$</span></span> is a prediction
        because <span class="inline-equation"><span class=
        "tex">$\mathtt {diedIn(Ava, Paris)}$</span></span> and
        <span class="inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf(Ava,Paris)}$</span></span> both appear in
        the KB shown in Table&nbsp;<a class="tbl" href=
        "#tab1">1</a>. The symbol <span class=
        "inline-equation"><span class="tex">$\mathbb {P}_{\mathtt
        {B \Rightarrow H}}$</span></span> denotes the set of
        predictions made by a rule <span class=
        "inline-equation"><span class="tex">$\mathtt {B \Rightarrow
        H}$</span></span> when applied to the facts in a knowledge
        base <em>K</em>.</p>
      </section>
      <section id="sec-6">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.2</span> Evaluation
            Metrics for Rules</h3>
          </div>
        </header>
        <p>Many metrics exist to evaluate the quality of a rule.
        Two of the most common ones are support and confidence.</p>
        <p>A rule's <em>support</em> or <em>coverage</em> is
        defined as the number of predictions made by the rule that
        appear in a given KB <em>K</em>:</p>
        <div class="table-responsive" id="eq8">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} supp(\mathtt
            {B \Rightarrow R}) = |\text{KP}_{\mathtt {B \Rightarrow
            R}}| = |\mathbb {P}_{\mathtt {B \Rightarrow R}} \cap
            \text{K}_\mathtt {R}|. \end{eqnarray}</span><br />
            <span class="equation-number">(8)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\text{KP}_{\mathtt {B \Rightarrow R}}$</span></span>
        represents the set of “known positive” (KP) examples, that
        is, the predictions that already appear in <em>K</em>.
        Galarraga et al.&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>] prefer this definition as it is
        monotonically decreasing, which enables applying many
        standard pruning techniques that can substantially improve
        the run time efficiency of rule learning.
        <p></p>
        <p>A rule's <em>confidence</em> or precision is defined as
        the proportion of its predictions that are correct.
        However, KBs that are automatically populated from Web
        usually only contain facts (i.e., ground atoms that are
        true) and are open world, meaning that the truth value of
        any ground atom not in the KB is unknown. Therefore, if
        <span class="inline-equation"><span class="tex">$\mathbb
        {P}_{\mathtt {B \Rightarrow R}}$</span></span> contains any
        ground atoms that are not in the KB, the confidence cannot
        be computed. One standard approach to this problem is to
        make a closed-world assumption (CWA) and assume that all
        ground atoms not in the KB are false. This yields the
        following definition:</p>
        <div class="table-responsive" id="Xeq1">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation}
            Conf_{\text{CWA}}(\mathtt {B \Rightarrow R}) =
            \frac{supp(\mathtt {B \Rightarrow R})}{|\mathbb
            {P}_{\mathtt {B \Rightarrow R}}|}.
            \end{equation}</span><br />
            <span class="equation-number">(9)</span>
          </div>
        </div>Clearly, the CWA is false when KBs are automatically
        populated from the Web, and hence using it is suboptimal.
        <p></p>
        <p>Galarraga et al.&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>] addressed this by proposing the
        partial completeness assumption (PCA), which assumes that
        if we know one <span class="inline-equation"><span class=
        "tex">$\mathtt {y}$</span></span> for a given <span class=
        "inline-equation"><span class="tex">$\mathtt
        {x}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> , that is, <span class=
        "inline-equation"><span class="tex">$\mathtt {R(x,y)} \in
        K_\mathtt {R}$</span></span> , we know all the <span class=
        "inline-equation"><span class="tex">$\mathtt
        {y}$</span></span> s for that <span class=
        "inline-equation"><span class="tex">$\mathtt
        {x}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> . Effectively, this allows them to infer
        the following set of negative examples for a KB
        <em>K</em>:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray*}
            \text{IN}_{\mathtt {R}} = \lbrace \mathtt
            {R(x,y^{\prime })}~|~\mathtt {R(x,y^{\prime })} \notin
            K_\mathtt {R} \wedge y^{\prime } \in \mathbb
            {Y}_\mathtt {R} \wedge \exists \mathtt {y}:\mathtt
            {R(x,y)} \in K_\mathtt {R} \rbrace
            \nonumber\end{eqnarray*}</span><br />
          </div>
        </div>These negative examples can be used to compute a
        rule's confidence, yielding the definition:
        <div class="table-responsive" id="eq9">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray}
            Conf_{\text{PCA}}(\mathtt {B \Rightarrow R}) =
            \frac{supp(\mathtt {B \Rightarrow R})}{supp(\mathtt {B
            \Rightarrow R})+ |\text{IN}_{\mathtt {R}} \cap \mathbb
            {P}_{\mathtt {B \Rightarrow R}}| }.
            \end{eqnarray}</span><br />
            <span class="equation-number">(10)</span>
          </div>
        </div>The PCA assumption is suitable for relations that act
        like functions and have at most one object for every
        subject (e.g. <span class="inline-equation"><span class=
        "tex">$\mathtt {diedIn}$</span></span> ), presuming the
        knowledge base is accurate. However, the assumption may be
        violated whenever a relation may associate multiple objects
        with each subject. Most relations fall into this category.
        In this case, its viability will solely depend on how
        complete the knowledge base is. Another potential weakness
        is that the PCA Confidence ignores the number of the
        predictions made by the rule.
        <p></p>
      </section>
      <section id="sec-7">
        <header>
          <div class="title-info">
            <h3><span class="section-number">2.3</span> Rule
            Generation</h3>
          </div>
        </header>
        <p>Approaches for generating first-order definite clauses
        have been extensively studied in the inductive logic
        programming literature&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0019">19</a>]. Our goal is not to revisit how
        rules are constructed; we are simply modifying the score
        function used to evaluate the quality of each rule. Hence,
        we make use of the highly efficient rule generation
        strategy employed in the AMIE+ system&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0012">12</a>]. The implementation
        employs a variety of techniques from the database community
        to achieve good scalability. At a high level, it generates
        rules as follows: As input, the user provides a support
        threshold and maximum clause length. The algorithm
        maintains a queue of rules, which initially contains one
        rule for each relation with an empty body (i.e.,
        <span class="inline-equation"><span class=
        "tex">$\Rightarrow \mathtt {R}$</span></span> ). Rules are
        removed from the queue and refined by adding literals to
        the body according to a language bias that defines legal
        rules (e.g., maximum length, etc.). It then checks the
        support of the refined rule, and if it exceeds the support
        threshold, the rule is returned. Furthermore, the modified
        rule is added to the queue for possible further
        refinement.</p>
      </section>
    </section>
    <section id="sec-8">
      <header>
        <div class="title-info">
          <h2><span class="section-number">3</span> RC Confidence
          Score</h2>
        </div>
      </header>
      <p>We propose a novel confidence score for evaluating rules
      in open-world problems. From a learning and evaluation
      perspective, each prediction made by an individual rule
      <span class="inline-equation"><span class="tex">$\mathtt {B
      \Rightarrow R}$</span></span> either falls in the set of
      known positive examples KP <sub><em>B</em>⇒<em>R</em></sub>
      (that is, it appears in the given knowledge base <em>K</em>)
      or belongs to a set <span class=
      "inline-equation"><span class="tex">$\text{U}_\mathtt {B
      \Rightarrow R}$</span></span> of unlabeled examples (that is,
      it does not appear in <em>K</em>). Any ground atom not in
      <em>K</em> could either be true or false. Thus, conceptually,
      these unlabeled examples can be subdivided into two sets</p>
      <ul class="list-no-style">
        <li id="uid16" label="Unknown positives">This is the set of
        ground atoms that are true but do not appear in <em>K</em>.
        They are denoted by <span class=
        "inline-equation"><span class="tex">$\text{UP}_{\mathtt {B
        \Rightarrow R}}$</span></span> .<br /></li>
        <li id="uid17" label="Unknown negatives">This is the set of
        the ground atoms that are false and do not appear in
        <em>K</em>. They are denoted by <span class=
        "inline-equation"><span class="tex">$\text{UN}_{\mathtt {B
        \Rightarrow R}}$</span></span> .<br /></li>
      </ul>
      <p>Figure&nbsp;<a class="fig" href="#fig1">1</a> illustrates
      this subdivision of the predictions.</p>
      <figure id="fig1">
        <img src=
        "../../../data/deliveryimages.acm.org/10.1145/3190000/3186006/images/www2018-15-fig1.jpg"
        class="img-responsive" alt="Figure 1" longdesc="" />
        <div class="figure-caption">
          <span class="figure-number">Figure 1:</span> <span class=
          "figure-title">The set of predictions <span class=
          "inline-equation"><span class="tex">$\mathbb {P}_{\mathtt
          {B \Rightarrow R}}$</span></span> can be divided into
          labeled (<span class="inline-equation"><span class=
          "tex">$\text{KP}_{\mathtt {B \Rightarrow
          R}}$</span></span> ) and unlabeled (<span class=
          "inline-equation"><span class="tex">$\text{U}_\mathtt {B
          \Rightarrow R}$</span></span> ) examples. Furthermore,
          <span class="inline-equation"><span class=
          "tex">$\text{U}_\mathtt {B \Rightarrow R}$</span></span>
          can be subdivided into the unknown positives
          (<span class="inline-equation"><span class=
          "tex">$\text{UP}_{\mathtt {B \Rightarrow
          R}}$</span></span> ) and unknown negatives (<span class=
          "inline-equation"><span class="tex">$\text{UN}_{\mathtt
          {B \Rightarrow R}}$</span></span> ).</span>
        </div>
      </figure>
      <p></p>
      <p>Based on the above division, a rule's confidence could
      then be calculated as:</p>
      <div class="table-responsive" id="Xeq2">
        <div class="display-equation">
          <span class="tex mytex">\begin{equation}
          \frac{supp(\mathtt {B \Rightarrow R}) +
          |\text{UP}_{\mathtt {B \Rightarrow R}}|}{|\mathbb
          {P}_\mathtt {B \Rightarrow R}|}
          \end{equation}</span><br />
          <span class="equation-number">(11)</span>
        </div>
      </div>Our insight is that estimating a rule's confidence only
      requires knowing the size of the set <span class=
      "inline-equation"><span class="tex">$\text{UP}_{\mathtt {B
      \Rightarrow R}}$</span></span> . That is, computing a
      confidence score does not require knowing precisely which
      ground atoms that do not appear in the knowledge base
      <em>K</em> are true, just how many of them are true. Thus,
      computing the confidence of a rule reduces to estimating the
      size of this set.
      <p></p>
      <p>We propose a novel way to estimate the size of
      <span class="inline-equation"><span class=
      "tex">$\text{UP}_{\mathtt {B \Rightarrow R}}$</span></span>
      that only relies on information available in the KB and
      subsequently incorporate it when evaluating rules. There are
      two key questions we need to address to estimate this set
      size, which are:</p>
      <ol class="list-no-style">
        <li id="list1" label="(1)"><strong>What is the relationship
        between the proportion of known and unknown positives
        covered by a rule?</strong> We address this by making the
        relationship between coverage assumption, which is outlined
        in Subsection 3.1.<br /></li>
        <li id="list2" label="(2)"><strong>What percentage of the
        unlabeled data for a given relation is expected to be
        positive?</strong> We begin in Subsection 3.2 by discussing
        two ways to compute this quantity, which we refer to as
        <em>β</em>. However, in Subsection 3.3 we detail several
        subtleties that we must contend with when using <em>β</em>
        to arrive at the final estimate for the number of unknown
        positives.<br /></li>
      </ol>
      <section id="sec-9">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.1</span>
            Relationship between Coverage Assumption</h3>
          </div>
        </header>
        <p>Our key assumption is that the proportion of positive
        examples covered by a rule is the same for both the labeled
        and unlabeled examples. We call this the relationship
        between coverage assumption (<em>rc</em>), and it is
        defined as follows:</p>
        <div class="table-responsive" id="eq10">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} rc(\mathtt {B
            \Rightarrow R}) = \frac{supp(\mathtt {B \Rightarrow
            R})}{ |K_\mathtt {R}|} = \frac{|\text{UP}_{\mathtt {B
            \Rightarrow R}}|}{ |\text{UP}_{\mathtt {R}}|},
            \end{equation}</span><br />
            <span class="equation-number">(12)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\text{UP}_{\mathtt {R}}$</span></span> represents
        the set of all true facts for relation <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> that do not appear in the KB <em>K</em>.
        Intuitively, this would hold if the instantiations of
        <span class="inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> that appear in <em>K</em> were selected
        completely at random from the set of all true facts for
        <span class="inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> .<a class="fn" href="#fn2" id=
        "foot-fn2"><sup>2</sup></a>
        <p></p>
        <p>Our goal is to estimate the size of <span class=
        "inline-equation"><span class="tex">$\text{UP}_{\mathtt {B
        \Rightarrow R}}$</span></span> , which we could do if we
        know <span class="inline-equation"><span class=
        "tex">$rc(\mathtt {B \Rightarrow R})$</span></span> and
        <span class="inline-equation"><span class=
        "tex">$|\text{UP}_{\mathtt {R}}|$</span></span> . Computing
        <em>rc</em> is straightforward. Computing <span class=
        "inline-equation"><span class="tex">$|\text{UP}_{\mathtt
        {R}}|$</span></span> is trickier.</p>
      </section>
      <section id="sec-10">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.2</span> Determining
            <em>β</em></h3>
          </div>
        </header>
        <p>We use <em>β</em> to denote the proportion of all the
        unlabeled examples that belong to the positive class. This
        true value of <em>β</em>, which is unknown, is:</p>
        <div class="table-responsive" id="eq11">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \beta =
            \frac{|\text{UP}_\mathtt {R}|}{|\text{U}_\mathtt {R}|},
            \end{equation}</span><br />
            <span class="equation-number">(13)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\text{U}_\mathtt {R}$</span></span> represents the
        set of all facts for relation <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> that do not appear in <em>K</em>. Now we
        discuss several options for estimating this proportion.
        <p></p>
        <p>One obvious solution for estimating <em>β</em> is to
        randomly sample a number of groundings for <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> , check which ones are true and use this
        to estimate <em>β</em>. This would likely entail
        significant work, as manual labels would need to be
        acquired for each relation. Thus, we propose two different,
        data-driven ways to estimate <em>β</em> that only rely on
        information guaranteed to be available in the KB (e.g., do
        not rely on the availability of type constraints).</p>
        <p>One possibility is to estimate <em>β</em> by making the
        partial completeness assumption. This yields the following
        definition:</p>
        <div class="table-responsive" id="eq12">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \beta
            _\text{PCA} = \frac{|K_\mathtt {R}|}{|\mathbb
            {X}_\mathtt {R}| \times |\mathbb {Y}_\mathtt {R}|},
            \end{equation}</span><br />
            <span class="equation-number">(14)</span>
          </div>
        </div>However, this is likely to be an overestimate in many
        cases. Consider, the relation <span class=
        "inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf}$</span></span> . Relatively speaking,
        there are very few people who are politicians. Using
        Equation&nbsp;<a class="eqn" href="#eq12">14</a> will
        assume politicians represent the entire set of people, and
        will therefore overestimate the proportion of the overall
        population who are politicians.
        <p></p>
        <p>Thus, a better possibility is to define <em>β</em> on a
        per-rule basis as:</p>
        <div class="table-responsive" id="Xeq3">
          <div class="display-equation">
            <span class="tex mytex">\begin{equation} \beta
            _{\mathtt {B \Rightarrow R}} = \frac{supp(\mathtt {B
            \Rightarrow R})}{|\mathbb {X}_\mathtt {B \Rightarrow
            R}| \times |\mathbb {Y}_\mathtt {B \Rightarrow R}|},
            \end{equation}</span><br />
            <span class="equation-number">(15)</span>
          </div>
        </div>where <span class="inline-equation"><span class=
        "tex">$\mathbb {X}_\mathtt {B \Rightarrow R}$</span></span>
        and <span class="inline-equation"><span class=
        "tex">$\mathbb {Y}_\mathtt {B \Rightarrow R}$</span></span>
        are defined as:
        <div class="table-responsive" id="eq13">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} \mathbb
            {X}_\mathtt {B \Rightarrow R} &amp;=&amp; \lbrace
            \mathtt {x}~|~\exists \mathtt {y}:\mathtt {R(x,y)} \in
            \mathbb {P}_{\mathtt {B \Rightarrow R}} \rbrace ,
            \end{eqnarray}</span><br />
            <span class="equation-number">(16)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq14">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} \mathbb
            {Y}_\mathtt {B \Rightarrow R} &amp;=&amp; \lbrace
            \mathtt {y} ~|~\exists \mathtt {x}:\mathtt {R(x,y)} \in
            \mathbb {P}_{\mathtt {B \Rightarrow R}} \rbrace .
            \end{eqnarray}</span><br />
            <span class="equation-number">(17)</span>
          </div>
        </div>Setting <em>β</em> in this way considers a larger set
        of constants (without relying on type constraints) and
        hence avoids the aforementioned drawback of <span class=
        "inline-equation"><span class="tex">$\beta
        _\text{PCA}$</span></span> .
        <p></p>
      </section>
      <section id="sec-11">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.3</span> Computing
            <span class="inline-equation"><span class=
            "tex">$|\text{UP}_{\mathtt {R}}|$</span></span></h3>
          </div>
        </header>
        <p>Our goal is to only use information that is guaranteed
        to be available to compute <span class=
        "inline-equation"><span class="tex">$|\text{UP}_{\mathtt
        {R}}|$</span></span> . Therefore, we do not wish to use
        type constraints, as they may be unknown. Thus, a first
        thought for computing <span class=
        "inline-equation"><span class="tex">$|\text{UP}_{\mathtt
        {R}}|$</span></span> is to consider the number of
        instantiations of <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> that are not in the KB that could be
        constructed using the observed constants in <span class=
        "inline-equation"><span class="tex">$\mathbb {P}_\mathtt {B
        \Rightarrow R}$</span></span> . Then we could multiply this
        number by <em>β</em>, leading to the following
        calculation:</p>
        <div class="table-responsive" id="eq15">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray}
            |\text{UP}_{\mathtt {R}}| = \beta \times | \lbrace
            \mathtt {R(x,y)}~|~\mathtt {x} \in \mathbb {X}_\mathtt
            {B \Rightarrow R},~\mathtt {y} \in \mathbb {Y}_\mathtt
            {B \Rightarrow R} \rbrace \setminus K_\mathtt {R}|.
            \end{eqnarray}</span><br />
            <span class="equation-number">(18)</span>
          </div>
        </div>
        <p></p>
        <p>However, Equation&nbsp;(<a class="eqn" href=
        "#eq15">18</a>) ignores the subtlety that each relation
        behaves differently in terms of the number of objects we
        would expect to be associated with each subject and vice
        versa. For example, the <span class=
        "inline-equation"><span class="tex">$\mathtt
        {bornIn}$</span></span> relation is a function because each
        person is only born in one city. However, a person can have
        multiple nationalities, although we would expect this
        number to be bounded by a small constant. To see how this
        issue affects computing <span class=
        "inline-equation"><span class="tex">$|\text{UP}_{\mathtt
        {R}}|$</span></span> , consider partitioning <span class=
        "inline-equation"><span class="tex">$\text{U}_\mathtt {B
        \Rightarrow R}$</span></span> into the following four
        subsets:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray*}
            X_{old}Y_{old} &amp; = &amp; \lbrace \mathtt
            {R(x,y)}~|~\mathtt {x} \in (\mathbb {X}_\mathtt {B
            \Rightarrow R} \cap \mathbb {X}_\mathtt {R}), ~\mathtt
            {y} \in (\mathbb {Y}_\mathtt {B \Rightarrow R} \cap
            \mathbb {Y}_\mathtt {R})\rbrace , \\X_{old}Y_{new}
            &amp; = &amp; \lbrace \mathtt {R(x,y)}~|~\mathtt {x}
            \in (\mathbb {X}_\mathtt {B \Rightarrow R} \cap \mathbb
            {X}_\mathtt {R}), ~\mathtt {y} \in (\mathbb {Y}_\mathtt
            {B \Rightarrow R} \setminus \mathbb {Y}_\mathtt {R})
            \rbrace , \\X_{new}Y_{old} &amp; = &amp; \lbrace
            \mathtt {R(x,y)}~|~\mathtt {x} \in (\mathbb {X}_\mathtt
            {B \Rightarrow R} \setminus \mathbb {X}_\mathtt {R}),
            ~\mathtt {y} \in (\mathbb {Y}_\mathtt {B \Rightarrow R}
            \cap \mathbb {Y}_\mathtt {R}) \rbrace ,
            \\X_{new}Y_{new} &amp; = &amp; \lbrace \mathtt
            {R(x,y)}~|~\mathtt {x} \in (\mathbb {X}_\mathtt {B
            \Rightarrow R} \setminus \mathbb {X}_\mathtt {R}),
            ~\mathtt {y} \in (\mathbb {Y}_\mathtt {B \Rightarrow R}
            \setminus \mathbb {Y}_\mathtt {R}) \rbrace
            .\end{eqnarray*}</span><br />
          </div>
        </div>For example, if <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R}$</span></span> only associates one object with each
        subject (i.e., it is a function), then all the new
        predictions (i.e., those not in the KB) that fall into
        <em>X<sub>old</sub>Y<sub>old</sub></em> and
        <em>X<sub>old</sub>Y<sub>new</sub></em> will be false.
        <p></p>
        <p>Therefore, we need to estimate the number of unknown
        positives separately for each subset. The two trickiest
        subsets are <em>X<sub>old</sub>Y<sub>new</sub></em> and
        <em>X<sub>new</sub>Y<sub>old</sub></em> , as we need to
        scale back the estimated number of positives examples in
        these subsets. To see why, consider
        <em>X<sub>old</sub>Y<sub>new</sub></em> . Here, we need to
        account for the fact that each subject already appears in
        <span class="inline-equation"><span class="tex">$K_\mathtt
        {R}$</span></span> with some objects, so we would expect it
        to associate with fewer new objects (on average) than if
        the subject did not appear in <span class=
        "inline-equation"><span class="tex">$K_\mathtt
        {R}$</span></span> . We do this by considering for
        <em>X<sub>old</sub>Y<sub>new</sub></em>
        (<em>X<sub>new</sub>Y<sub>old</sub></em> ) the proportion
        of possible new objects (new subjects) that are associated
        with each old subject (old object). This is captured by the
        functionality and inverse functionality of a
        relation&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>]. We use a slight modification from
        past work&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>],<a class="fn" href="#fn3" id=
        "foot-fn3"><sup>3</sup></a> and define <span class=
        "inline-equation"><span class="tex">$f_\mathtt {X}(\mathtt
        {R})$</span></span> and <span class=
        "inline-equation"><span class="tex">$f_\mathtt {Y}(\mathtt
        {R})$</span></span> as:</p>
        <div class="table-responsive" id="eq16">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} f_\mathtt
            {X}(\mathtt {R}) &amp;=&amp; 1 - \text{HM}_{\mathtt {x}
            \in \mathbb {X}_\mathtt {R}} \bigg (\frac{1}{|\lbrace
            \mathtt {y} | \mathtt {R(x,y)} \in K|\rbrace }\bigg)
            \end{eqnarray}</span><br />
            <span class="equation-number">(19)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq17">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} f_\mathtt
            {Y}(\mathtt {R}) &amp;=&amp; 1 - \text{HM}_{\mathtt {y}
            \in \mathbb {Y}_\mathtt {R}} \bigg (\frac{1}{|\lbrace
            \mathtt {x} | \mathtt {R(x,y)} \in K|\rbrace }\bigg)
            \end{eqnarray}</span><br />
            <span class="equation-number">(20)</span>
          </div>
        </div>where HM is the harmonic mean. The functions return a
        value between zero and one. The value of <span class=
        "inline-equation"><span class="tex">$f_\mathtt {X}(\mathtt
        {R})$</span></span> (<span class=
        "inline-equation"><span class="tex">$f_\mathtt {Y}(\mathtt
        {R})$</span></span> ) is zero if the relation is a function
        (inverse function).
        <p></p>
        <p>Finally, like the PCA, we ignore
        <em>X<sub>old</sub>Y<sub>old</sub></em> and assume that all
        of these predictions are false. We use this data to compute
        the information needed to derive the size of <span class=
        "inline-equation"><span class="tex">$\text{UP}_{\mathtt {B
        \Rightarrow R}}$</span></span> , so any estimates on this
        subsample of the data will likely be overly optimistic. We
        estimate the unknown positives for the remaining three
        subsets as:</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray*}
            |\text{UP}_{X_{old}Y_{new}}|
            &amp;=&amp;|X_{old}Y_{new}| \times \beta _{\mathtt {B
            \Rightarrow R}} \times f_\mathtt {X}(\mathtt {R})
            \times rc(\mathtt {B \Rightarrow R}),\\
            |\text{UP}_{X_{new}Y_{old}}| &amp;=&amp;
            |X_{new}Y_{old}| \times \beta _{\mathtt {B \Rightarrow
            R}} \times f_\mathtt {Y}(\mathtt {R}) \times rc(\mathtt
            {B \Rightarrow R}),\\ |\text{UP}_{X_{new}Y_{new}}|
            &amp;=&amp; |X_{new}Y_{new}| \times \beta _{\mathtt {B
            \Rightarrow R}} \times rc(\mathtt {B \Rightarrow
            R}).~~~~~~~~~~~~\end{eqnarray*}</span><br />
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-12">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.4</span> The Final
            RC Metric</h3>
          </div>
        </header>
        <p>This leads to our rule confidence score, which for a
        rule <span class="inline-equation"><span class=
        "tex">$r:\mathtt {B \Rightarrow R}$</span></span> is
        defined as:</p>
        <div class="table-responsive" id="eq18">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} &amp;
            Conf_\text{RC}(r) = \nonumber \\ &amp; \frac{supp(r) +
            |\text{UP}_{X_{old}Y_{new}}| +
            |\text{UP}_{X_{new}Y_{old}}| +
            |\text{UP}_{X_{new}Y_{new}}|}{|\mathbb {P}_r|}.
            \end{align}</span><br />
            <span class="equation-number">(21)</span>
          </div>
        </div>
        <p></p>
      </section>
      <section id="sec-13">
        <header>
          <div class="title-info">
            <h3><span class="section-number">3.5</span> Comparing
            Different Evaluation Measures</h3>
          </div>
        </header>
        <p>To illustrate the differences between the CWA, PCA, and
        RC, we will use our KB from Table&nbsp;<a class="tbl" href=
        "#tab1">1</a> and Rule&nbsp;(<a class="eqn" href=
        "#eq5">5</a>):</p>
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray*} r:~\mathtt
            {livesIn(x,y)} \Rightarrow \mathtt
            {isPoliticianOf(x,y)}.\nonumber\end{eqnarray*}</span><br />
          </div>
        </div>This rule produces three predictions: <span class=
        "inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf(Ava,Paris)}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf(Emily,London)}$</span></span> and
        <span class="inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf(Emily}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\mathtt
        {Paris)}$</span></span> . Only one of these appears in the
        KB. Hence, the <span class="inline-equation"><span class=
        "tex">$supp(\mathtt {livesIn(x,y)} \Rightarrow \mathtt
        {isPoliticianOf(x,y)}) = 1$</span></span> . Now, let us
        consider the various confidence scores for this rule.
        <p></p>
        <p><strong>CWA.</strong> Because <span class=
        "inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf(Emily,London)}$</span></span> and</p>
        <p><span class="inline-equation"><span class="tex">$\mathtt
        {isPoliticianOf(Emily,Paris)}$</span></span> do not appear
        in the KB in Table&nbsp;<a class="tbl" href="#tab1">1</a>,
        the CWA means that these are assumed not to be true. Thus,
        the confidence with this assumption is:</p>
        <div class="table-responsive" id="eq19">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} Conf_{CWA}(r) =
            \frac{1}{3} = 0.33 \end{align}</span><br />
            <span class="equation-number">(22)</span>
          </div>
        </div>
        <p></p>
        <p><strong>PCA.</strong> The PCA assumption would assume
        that <span class="inline-equation"><span class=
        "tex">$\mathtt {Ava}$</span></span> and <span class=
        "inline-equation"><span class="tex">$\mathtt
        {Bob}$</span></span> are not politicians in any other city.
        However, because <span class="inline-equation"><span class=
        "tex">$\mathtt {Emily}$</span></span> does not appear in
        the <span class="inline-equation"><span class=
        "tex">$\mathtt {isPoliticianOf}$</span></span> relation, it
        assumes nothing about whether <span class=
        "inline-equation"><span class="tex">$\mathtt
        {Emily}$</span></span> is a politician. Because the PCA
        does not produce any negative examples that are in the
        prediction set, the PCA confidence measure is:</p>
        <div class="table-responsive" id="eq20">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} Conf_{PCA}(r)
            =\frac{1}{1 + 0} = 1 \end{align}</span><br />
            <span class="equation-number">(23)</span>
          </div>
        </div>The PCA Confidence for this rule is 1.0, even though
        clearly not every resident of a city is also a politician
        in that city.
        <p></p>
        <p><strong>RC.</strong> When calculating the <em>RC</em>
        confidence we would like to estimate how many of the
        remaining 2 unlabeled predictions are also positive. The
        <span class="inline-equation"><span class="tex">$\mathbb
        {X}_{r}$</span></span> has two elements (<span class=
        "inline-equation"><span class="tex">$\mathtt
        {Ava}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\mathtt
        {Emily}$</span></span> ) and <span class=
        "inline-equation"><span class="tex">$\mathbb
        {Y}_r$</span></span> has as well two elements (<span class=
        "inline-equation"><span class="tex">$\mathtt
        {Paris}$</span></span> , <span class=
        "inline-equation"><span class="tex">$\mathtt
        {London}$</span></span> ) and therefore the estimate of the
        proportion of groundings that are true is:</p>
        <div class="table-responsive" id="eq21">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} \beta _{r} =
            \frac{1}{2 \times 2} = \frac{1}{4}
            \end{align}</span><br />
            <span class="equation-number">(24)</span>
          </div>
        </div>Next, we divide <em>U<sub>r</sub></em> into 4
        subsets:
        <div class="table-responsive">
          <div class="display-equation">
            <span class="tex mytex">\begin{align*} X_{old}Y_{old}
            &amp;= \lbrace \mathtt {(Ava, Paris)}\rbrace
            ,\\X_{old}Y_{new} &amp;= \lbrace \mathtt {(Ava,
            London)}\rbrace ,\\X_{new}Y_{old} &amp;= \lbrace
            \mathtt {(Emily, Paris)}\rbrace ,\\X_{new}Y_{new}
            &amp;= \lbrace \mathtt {(Emily, London)}\rbrace
            ,\\\end{align*}</span><br />
          </div>
        </div>The calculation must also take into account the
        relation's properties, where
        <div class="table-responsive" id="eq22">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} f_\mathtt
            {X}(\mathtt {isPoliticianOf}) = 1 - \frac{2}{3} =
            \frac{1}{3} \end{align}</span><br />
            <span class="equation-number">(25)</span>
          </div>
        </div>shows that a person can be a politician in different
        cities. Similarly we also calculate <span class=
        "inline-equation"><span class="tex">$f_\mathtt {Y}(\mathtt
        {isPoliticianOf}) = \frac{1}{3}$</span></span> . Using the
        definition of the <em>rc</em> assumption we calculate the
        relationship to be:
        <div class="table-responsive" id="eq23">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} rc(r) =
            \frac{supp(r)}{ |K_{\mathtt {isPoliticianOf}}|} =
            \frac{1}{3}, \end{align}</span><br />
            <span class="equation-number">(26)</span>
          </div>
        </div>In the next step, we calculate the estimated number
        of positives in each of the subsets:
        <div class="table-responsive" id="eq24">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            |UP_{X_{old}Y_{new}}| &amp;= 1 \times \frac{1}{4}
            \times \frac{1}{3} \times \frac{1}{3} =
            \frac{1}{36},\end{align}</span><br />
            <span class="equation-number">(27)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq25">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            |UP_{X_{new}Y_{old}}| &amp;= 1 \times \frac{1}{4}
            \times \frac{1}{3} \times \frac{1}{3} =
            \frac{1}{36},\end{align}</span><br />
            <span class="equation-number">(28)</span>
          </div>
        </div>
        <div class="table-responsive" id="eq26">
          <div class="display-equation">
            <span class="tex mytex">\begin{align}
            |UP_{X_{new}Y_{new}}| &amp;= 1 \times \frac{1}{4}
            \times \frac{1}{3} = \frac{1}{12}.
            \end{align}</span><br />
            <span class="equation-number">(29)</span>
          </div>
        </div>Finally, we can compute the confidence measure for
        the rule:
        <div class="table-responsive" id="eq27">
          <div class="display-equation">
            <span class="tex mytex">\begin{align} Conf_{RC}(r) =
            \frac{1 + (\frac{1}{36} + \frac{1}{36} +
            \frac{1}{12})}{3} = 0.38 \end{align}</span><br />
            <span class="equation-number">(30)</span>
          </div>
        </div>
        <p></p>
      </section>
    </section>
    <section id="sec-14">
      <header>
        <div class="title-info">
          <h2><span class="section-number">4</span> Empirical
          Evaluation</h2>
        </div>
      </header>
      <p>We will evaluate our proposed confidence metric in the
      context of inferring novel facts to include in a KB.
      Specifically, our goal is to address the following
      questions:</p>
      <ol class="list-no-style">
        <li id="list3" label="(1)">When using the AMIE+ system,
        does employing our proposed RC confidence score result in
        more accurate predictions than using the PCA confidence
        score?<br /></li>
        <li id="list4" label="(2)">What is the effect of type
        constraints on the precision of the predictions?<br /></li>
        <li id="list5" label="(3)">How well do the confidence
        measures estimate the precision of a rule?<br /></li>
        <li id="list6" label="(4)">How does <em>β<sub>r</sub></em>
        compare to <span class="inline-equation"><span class=
        "tex">$\beta _\text{PCA}$</span></span> ?<br /></li>
        <li id="list7" label="(5)">How does the proposed approach
        compare to the Subgraph Feature Extraction approach
        (SFE)&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0013">13</a>]?<br />
        </li>
      </ol>
      <p>In order to answer these five questions, we will compare
      the following algorithms:</p>
      <ul class="list-no-style">
        <li id="uid55"><strong>RC + types</strong>: RC confidence
        using <span class="inline-equation"><span class=
        "tex">$\beta _{\mathtt {B \Rightarrow R}}$</span></span>
        and RDF type constraints<br /></li>
        <li id="uid56"><strong>PCA + types</strong>: PCA confidence
        and RDF type constraints<br /></li>
        <li id="uid57"><strong>RC</strong>: RC confidence using
        <span class="inline-equation"><span class="tex">$\beta
        _{\mathtt {B \Rightarrow R}}$</span></span> and no type
        constraints<br /></li>
        <li id="uid58"><strong>PCA</strong>: PCA confidence and no
        type constraints<br /></li>
        <li id="uid59"><strong>RC_PCA + types</strong>: RC
        confidence using <span class="inline-equation"><span class=
        "tex">$\beta _\text{PCA}$</span></span> and RDF type
        constraints<br /></li>
      </ul>
      <section id="sec-15">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.1</span>
            Methodology</h3>
          </div>
        </header>
        <p>We used the AMIE+&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>] system to learn rules (see
        Sect.&nbsp;<a class="sec" href="#sec-7">2.3</a>) on the
        YAGO2&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0016">16</a>] and Wikidata KBs. We employed the
        same parameter settings from the original paper and set
        <em>minHC = 0.01</em> (support threshold) and <em>maxLen =
        3</em>, which resulted in 137 and 1515 learned rules,
        respectively. After finding all rules that meet the support
        threshold, the AMIE+ system only retains those that also
        satisfy a threshold on the PCA confidence, which we set to
        0.1 for the YAGO2 KB as in the AMIE+ paper. Consequently,
        the final rule set for the YAGO2 KB consisted of the 69
        rules from the initial 137 that met this threshold. As far
        more rules met the support threshold in the Wikidata KB, we
        employed a higher PCA confidence threshold of 0.6, which
        resulted in 456 rules from the initial set of 1515 being
        selected.</p>
        <p>When using our proposed confidence score, it would also
        be natural only to consider those rules that meet both a
        support threshold and a threshold on the RC confidence
        score. However, when comparing the PCA and RC confidences,
        we want to avoid any differences in performance arising due
        to the fact that each confidence score led to a different
        number of rules being selected. Therefore, in order to
        ensure a fair comparison between the two confidence
        metrics, instead of using a threshold on the RC confidence,
        we selected rules using it as follows. We ordered the list
        of rules that met the support threshold (137 for YAGO2 and
        1515 for Wikidata) according to the RC confidence metric.
        Then, we selected the top 69 rules for the YAGO2 KB and top
        456 rules on the Wikidata KB according the RC confidence.
        Hence, the rule sets selected using each confidence score
        contain different rules in them, but are of the same
        size.</p>
        <p>Using the selected rules, we generated all predictions
        and assigned a confidence to each prediction. As some
        predictions can be derived from multiple, different rules,
        we calculated the confidence score for each ground atom
        using Galarraga et al.’s method&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0012">12</a>]:</p>
        <div class="table-responsive" id="eq28">
          <div class="display-equation">
            <span class="tex mytex">\begin{eqnarray} score(\mathtt
            {R(x,y)}) = 1 - \prod _{i=1}^{n} (1 - {Conf}_{*}(r_i)),
            \end{eqnarray}</span><br />
            <span class="equation-number">(31)</span>
          </div>
        </div>where <em>r<sub>i</sub></em> , <em>i</em> ∈ {1, …,
        <em>n</em>} is the set of rules that predict <span class=
        "inline-equation"><span class="tex">$\mathtt
        {R(x,y)}$</span></span> and <em>Conf</em> <sub>*</sub> is
        the appropriate confidence measure. This formula assigns a
        higher confidence value to ground atoms derived from
        multiple rules, which intuitively makes sense. In the cases
        where we consider type constraints, we used the
        <em>rdf:type</em> constraints from the YAGO3
        KB&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0022">22</a>], type constraints for Wikidata
        properties, and type-checking NELL constraints. In this
        case, any prediction that violates the constraints is
        discarded.
        <p></p>
        <p>To evaluate the quality of the predictions, we generated
        a single ranking over all relations based on <span class=
        "inline-equation"><span class="tex">$score(\mathtt
        {R(x,y)})$</span></span> . Based on the confidence scores,
        we divided the predictions into buckets of width 0.01
        (e.g., the first bucket contains the predictions with
        scores between 1 and 0.99, the second bucket contains the
        predictions with scores between 0.99 and 0.98, etc.). We
        calculated the precision for the first bucket such that,
        cumulatively, 10&nbsp;thousand predictions were made. Then,
        we evaluated every subsequent bucket such that an
        additional 50&nbsp;thousand predictions were made. Then,
        like Galarraga et al.&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0012">12</a>], we estimated the cumulative
        precision, by randomly sampling 100 unlabeled predictions
        from any bucket which represents a range of confidence
        scores that are equal to or higher than the current bucket.
        For predictions on the YAGO2 KB, we checked if each
        selected prediction appeared in the YAGO3 KB, and if so we
        labeled it as correct. If it did not appear in the YAGO3
        KB, we manually checked the fact. We manually labeled all
        selected facts on the Wikidata KB.</p>
        <p>To compare our approach with the Subgraph Feature
        Extraction approach (SFE)&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0013">13</a>], we used the NELL
        KB&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
        "popover" data-placement="top" href=
        "#BibPLXBIB0005">5</a>]. SFE, which is an enhancement of
        the Path Ranking Algorithm (PRA)&nbsp;[<a class="bib"
        data-trigger="hover" data-toggle="popover" data-placement=
        "top" href="#BibPLXBIB0017">17</a>], learns a separate
        model for each relation, and generates predictions
        separately for each relation. We used the same data as used
        in Gardner and Mitchell&nbsp;[<a class="bib" data-trigger=
        "hover" data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0013">13</a>] and Gardner et al.&nbsp;[<a class=
        "bib" data-trigger="hover" data-toggle="popover"
        data-placement="top" href="#BibPLXBIB0014">14</a>], which
        focuses on the 10 relations in the NELL KB with the largest
        number of known instances. We obtained the set of
        predictions for all 10 relations using the SFE algorithm
        and placed them into buckets based on the SFE probability
        measure&nbsp;[<a class="bib" data-trigger="hover"
        data-toggle="popover" data-placement="top" href=
        "#BibPLXBIB0014">14</a>]. We also ran the AMIE+ system on
        the NELL data using the same support threshold as
        previously mentioned. Then, we only kept the 139 rules that
        had one of the 10 considered relations as the head. We
        ranked all rules according to both the PCA and RC
        confidences, and employed the previously procedure to make
        predictions with each rule set.</p>
        <p>The characteristics for the YAGO2, Wikidata, and the
        used subset of the NELL KBs are provided in
        Table&nbsp;<a class="tbl" href="#tab2">2</a>.</p>
        <div class="table-responsive" id="tab2">
          <div class="table-caption">
            <span class="table-number">Table 2:</span> <span class=
            "table-title">Characteristics of each knowledge base
            considered in the experimental evaluation.</span>
          </div>
          <table class="table">
            <thead>
              <tr>
                <th style="text-align:left;">KB</th>
                <th style="text-align:right;"># facts</th>
                <th style="text-align:center;"># relations</th>
              </tr>
            </thead>
            <tbody>
              <tr>
                <td style="text-align:left;">YAGO2</td>
                <td style="text-align:right;">948K</td>
                <td style="text-align:center;">33</td>
              </tr>
              <tr>
                <td style="text-align:left;">Wikidata</td>
                <td style="text-align:right;">8.4M</td>
                <td style="text-align:center;">430</td>
              </tr>
              <tr>
                <td style="text-align:left;">NELL</td>
                <td style="text-align:right;">3.4M</td>
                <td style="text-align:center;">520</td>
              </tr>
            </tbody>
          </table>
        </div>
      </section>
      <section id="sec-16">
        <header>
          <div class="title-info">
            <h3><span class="section-number">4.2</span>
            Results</h3>
          </div>
        </header>
        <section id="sec-17">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.1</span> RC vs.
              PCA Confidence</h4>
            </div>
          </header>
          <p>To address the first question, we compare the rule
          sets selected using the RC and PCA confidences with and
          without type constraints on the YAGO2 and Wikidata KBs.
          Figure&nbsp;<a class="fig" href="#fig2">2</a> shows the
          precision for all approaches. The RC confidence achieves
          a superior precision regardless of the number of
          predictions made and regardless of the KB. At the
          beginning, when only a few predictions are made, both
          approaches perform similarly. However, larger differences
          begin to emerge as more facts are predicted.</p>
          <figure id="fig2">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186006/images/www2018-15-fig2.jpg"
            class="img-responsive" alt="Figure 2" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 2:</span>
              <span class="figure-title">Comparing the effect of
              using type constraints on the precision of the
              predictions as a function of the number of
              predictions made for both the RC and PCA confidence
              measures.</span>
            </div>
          </figure>
          <p></p>
          <p>We will now discuss the results on the YAGO2 KB in
          more detail. The precision when using the RC confidence
          increases after the first point in the plot. This metric
          ranks the rule</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\[ \mathtt
              {isMarriedTo(x,y)}\wedge \mathtt
              {hasChild(x,z)}\Rightarrow \mathtt {hasChild(y,z)}
              \]</span><br />
            </div>
          </div>first. While this rule is logical and relevant, the
          YAGO2 KB is fairly complete with respect to the
          <span class="inline-equation"><span class="tex">$\mathtt
          {hasChild}$</span></span> relations. Hence, this rule
          ends up making a number of predictions that describe the
          stepchild relation, which leads to slightly lower
          precision at the top of the ranked list of predictions.
          <p></p>
          <p>The precision when using the PCA confidence decreases
          as more predictions are made. This metric ranks the
          following rules second and third:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray*} \mathtt
              {diedIn(x,y)} \wedge \mathtt {isLocatedIn(y,z)}
              \Rightarrow \mathtt {isPoliticianOf(x,y)}~ \\\mathtt
              {livesIn(x,y)} \wedge \mathtt {isLocatedIn(y,z)}
              \Rightarrow \mathtt
              {isPoliticianOf(x,z)}.\end{eqnarray*}</span><br />
            </div>
          </div>Generally speaking, these rules do not hold and
          hence create a large number of highly ranked, yet
          incorrect predictions.
          <p></p>
        </section>
        <section id="sec-18">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.2</span> Effect
              of Type Constraints</h4>
            </div>
          </header>
          <p>As expected, Figure&nbsp;<a class="fig" href=
          "#fig2">2</a> shows that including type constraints
          improves performance for both metrics. However, using the
          RC confidence without type constraints results in
          equivalent or slightly better performance than PCA with
          type constraints on the YAGO2 KB and better performance
          on the Wikidata KB. Furthermore, the RC confidence
          without type constraints is better than PCA without type
          constraints. These results give additional evidence that
          reasoning about the unlabeled data in a more
          sophisticated manner, as the RC confidence measure does,
          can be beneficial.</p>
        </section>
        <section id="sec-19">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.3</span>
              Rule-by-rule comparison</h4>
            </div>
          </header>
          <p>We selected nine rules learned on YAGO2 KB and
          compared the PCA and RC (both with type constraints)
          confidences to the confidence of the rule as estimated on
          manually labeled data. The chosen rules are ranked highly
          according to at least one of the score functions or
          reflect the differences between them. We estimated the
          confidence by randomly sampling 100 predictions for each
          rule. Predictions that appeared in the YAGO3 KB were
          labeled as correct and the others were assessed
          manually.</p>
          <p>The results are presented in Table&nbsp;<a class="tbl"
          href="#tab3">3</a>. For each rule, the table gives both
          the PCA and RC confidence measures and the rule's rank
          among all learned rules. It also includes the number of
          predictions made by each rule that did not appear in the
          KB (i.e., the size of <span class=
          "inline-equation"><span class="tex">$\text{U}_\mathtt {B
          \Rightarrow R}$</span></span> ) and the precision as
          estimated on the manually labeled data.</p>
          <div class="table-responsive" id="tab3">
            <div class="table-caption">
              <span class="table-number">Table 3:</span>
              <span class="table-title">For nine rules learned on
              the YAGO2 KB, we report the PCA and RC confidence
              scores (together with each rule's rank) as well as an
              estimate of the rule's true precision. <span class=
              "inline-equation"><span class=
              "tex">$|\text{U}_{\mathtt {B \Rightarrow
              R}}|$</span></span> reports the number of predictions
              that are unlabeled.</span>
            </div>
            <table class="table">
              <thead>
                <tr>
                  <th style="text-align:right;"></th>
                  <th style="text-align:center;"></th>
                  <th colspan="2" style="text-align:center;">
                  PCA</th>
                  <th colspan="2" style="text-align:center;">
                  RC</th>
                  <th style="text-align:center;"></th>
                </tr>
                <tr>
                  <th style="text-align:right;">Rule</th>
                  <th style="text-align:center;"><span class=
                  "inline-equation"><span class=
                  "tex">$|\text{U}_{\mathtt {B \Rightarrow
                  R}}|$</span></span></th>
                  <th style="text-align:right;">Conf</th>
                  <th style="text-align:center;">Rank</th>
                  <th style="text-align:right;">Conf</th>
                  <th style="text-align:center;">Rank</th>
                  <th style="text-align:center;">Precision</th>
                </tr>
              </thead>
              <tbody>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {isMarriedTo(x,y)} \Rightarrow \mathtt
                  {isMarriedTo(y,x)}$</span></span>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                  <td style="text-align:center;">5635</td>
                  <td style="text-align:right;">0.92</td>
                  <td style="text-align:center;">1</td>
                  <td style="text-align:right;">0.59</td>
                  <td style="text-align:center;">2</td>
                  <td style="text-align:center;">1.00</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {diedIn(x,y)} \wedge \mathtt {isLocatedIn(y,z)}
                  \Rightarrow \mathtt
                  {isPoliticianOf(x,z)}$</span></span></td>
                  <td style="text-align:center;">13038</td>
                  <td style="text-align:right;">0.85</td>
                  <td style="text-align:center;">2</td>
                  <td style="text-align:right;">0.03</td>
                  <td style="text-align:center;">67</td>
                  <td style="text-align:center;">0.36</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {created(x,y)} \wedge \mathtt {produced(x,y)}
                  \Rightarrow \mathtt
                  {directed(x,y)}\,$</span></span>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                  <td style="text-align:center;">1018</td>
                  <td style="text-align:right;">0.59</td>
                  <td style="text-align:center;">8</td>
                  <td style="text-align:right;">0.50</td>
                  <td style="text-align:center;">5</td>
                  <td style="text-align:center;">0.13</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {isMarriedTo(x,y)} \wedge \mathtt {hasChild(x,z)}
                  \Rightarrow \mathtt
                  {hasChild(y,z)}\,$</span></span>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                  <td style="text-align:center;">2643</td>
                  <td style="text-align:right;">0.58</td>
                  <td style="text-align:center;">9</td>
                  <td style="text-align:right;">0.59</td>
                  <td style="text-align:center;">1</td>
                  <td style="text-align:center;">0.51</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {bornIn(x,y)} \wedge \mathtt {isLocatedIn(y,z)}
                  \Rightarrow \mathtt
                  {isPoliticianOf(x,z)}$</span></span></td>
                  <td style="text-align:center;">33559</td>
                  <td style="text-align:right;">0.57</td>
                  <td style="text-align:center;">11</td>
                  <td style="text-align:right;">0.01</td>
                  <td style="text-align:center;">83</td>
                  <td style="text-align:center;">0.33</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {hasChild(x,y)} \wedge \mathtt {hasChild(z,y)}
                  \Rightarrow \mathtt
                  {isMarriedTo(x,z)}$</span></span>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                  <td style="text-align:center;">1971</td>
                  <td style="text-align:right;">0.41</td>
                  <td style="text-align:center;">20</td>
                  <td style="text-align:right;">0.50</td>
                  <td style="text-align:center;">4</td>
                  <td style="text-align:center;">0.86</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {livesIn(x,y)} \Rightarrow \mathtt
                  {isPoliticianOf(x,y)}$</span></span></td>
                  <td style="text-align:center;">14515</td>
                  <td style="text-align:right;">0.29</td>
                  <td style="text-align:center;">32</td>
                  <td style="text-align:right;">0.01</td>
                  <td style="text-align:center;">85</td>
                  <td style="text-align:center;">0.29</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {dealsWith(x,y)} \wedge \mathtt {dealsWith(y,z)}
                  \Rightarrow \mathtt
                  {dealsWith(x,z)}\,$</span></span>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                  <td style="text-align:center;">1121</td>
                  <td style="text-align:right;">0.28</td>
                  <td style="text-align:center;">33</td>
                  <td style="text-align:right;">0.29</td>
                  <td style="text-align:center;">8</td>
                  <td style="text-align:center;">0.94</td>
                </tr>
                <tr>
                  <td style="text-align:right;"><span class=
                  "inline-equation"><span class="tex">$\mathtt
                  {dealsWith(x,y)} \Rightarrow \mathtt
                  {dealsWith(y,x)}\,$</span></span>
                  &nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;</td>
                  <td style="text-align:center;">595</td>
                  <td style="text-align:right;">0.18</td>
                  <td style="text-align:center;">47</td>
                  <td style="text-align:right;">0.15</td>
                  <td style="text-align:center;">17</td>
                  <td style="text-align:center;">1.00</td>
                </tr>
              </tbody>
            </table>
          </div>
          <p>The RC confidence tends to systematically
          underestimate the precision of rules. Perhaps this occurs
          because <span class="inline-equation"><span class=
          "tex">$\beta _{\mathtt {B \Rightarrow R}}$</span></span>
          is an underestimate of the true proportion of the true
          facts in the unlabeled data. When estimating the
          precision using the RC confidence, we divide the set of
          unlabeled predictions into subsets based on the previous
          appearance of subjects and objects in the KB. The RC
          confidence tends to rank rules where one (or more) of
          those subsets is empty, higher. Empty subsets can arise
          when <span class="inline-equation"><span class=
          "tex">$f_\mathtt {X}(\mathtt {R})$</span></span> and
          <span class="inline-equation"><span class=
          "tex">$f_\mathtt {Y}(\mathtt {R})$</span></span> equal
          zero. Considering the characteristics of each relation
          allows the RC confidence to perform better. The RC
          confidence also tends to rank rules that make fewer
          predictions that fall outside the KB higher.</p>
          <p>In contrast, the PCA confidence over- or
          under-estimates the precision based on the
          characteristics of the rule. For example, when
          calculating the confidence for a relation that has many
          objects for each subject (e.g., <span class=
          "inline-equation"><span class="tex">$\mathtt
          {dealsWith}$</span></span> ), the PCA confidence tends to
          underestimate the precision. Its estimates of the
          precision tend to be better when a subject has close to
          (or exactly) one object for each subject.</p>
          <p>While the confidence estimates may not be well
          calibrated, they do tend to produce good rankings. There
          is an extensive literature on calibrating estimates via
          post processing (e.g. [<a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0006">6</a>, <a class="bib" data-trigger=
          "hover" data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0033">33</a>]), and it would be possible to
          adapt these techniques to our setting.</p>
        </section>
        <section id="sec-20">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.4</span>
              Comparing Different βs</h4>
            </div>
          </header>
          <p>Figure&nbsp;<a class="fig" href="#fig3">3</a> shows a
          comparison between using <span class=
          "inline-equation"><span class="tex">$\beta _\mathtt {B
          \Rightarrow R}$</span></span> and <span class=
          "inline-equation"><span class="tex">$\beta
          _\text{PCA}$</span></span> on the YAGO2 KB. Clearly,
          using <span class="inline-equation"><span class=
          "tex">$\beta _\mathtt {B \Rightarrow R}$</span></span>
          results in better performance. The primary issue with
          using <span class="inline-equation"><span class=
          "tex">$\beta _\text{PCA}$</span></span> is that it tends
          to way overestimate the confidence in several cases.
          Namely, it struggles when the relation in the rule head
          contains one argument where only a small subset of the
          entities that could appear in that argument position will
          appear in a true ground atom that involves that the
          relation, such as the following rule:</p>
          <div class="table-responsive">
            <div class="display-equation">
              <span class="tex mytex">\begin{eqnarray*} \mathtt
              {livesIn(x,y)} \wedge \mathtt {isLocatedIn(y,z)}
              \Rightarrow \mathtt
              {isPoliticianOf(x,z)}.\end{eqnarray*}</span><br />
            </div>
          </div>
          <figure id="fig3">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186006/images/www2018-15-fig3.jpg"
            class="img-responsive" alt="Figure 3" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 3:</span>
              <span class="figure-title">The effect of using
              <span class="inline-equation"><span class=
              "tex">$\beta _\mathtt {B \Rightarrow
              R}$</span></span> versus <span class=
              "inline-equation"><span class="tex">$\beta
              _\text{PCA}$</span></span> on the RC confidence
              measure on the YAGO2 KB. The plot shows the precision
              of the predictions as a function of the number of
              predictions made.</span>
            </div>
          </figure>
          <p></p>
        </section>
        <section id="sec-21">
          <header>
            <div class="title-info">
              <h4><span class="section-number">4.2.5</span>
              Comparison to SFE</h4>
            </div>
          </header>
          <p>Finally, we compare AMIE+ with both the PCA and RC
          confidence measures to the Subgraph Feature Extraction
          (SFE) approach&nbsp;[<a class="bib" data-trigger="hover"
          data-toggle="popover" data-placement="top" href=
          "#BibPLXBIB0013">13</a>], which is recent, well-known
          approach to KB completion. Figure&nbsp;<a class="fig"
          href="#fig4">4</a> shows a comparison between the SFE
          algorithm and AMIE+ using the PCA and RC confidence on
          the described subset of the NELL KB. SFE achieves the
          highest performance when only a very small number of
          predictions are made, however, its performance quickly
          degrades. After around 1500 predictions, the rules
          learned by AMIE+ regardless of which confidence measure
          is used perform much better. Furthermore, the RC
          confidence measure again outperforms the PCA
          confidence.</p>
          <figure id="fig4">
            <img src=
            "../../../data/deliveryimages.acm.org/10.1145/3190000/3186006/images/www2018-15-fig4.jpg"
            class="img-responsive" alt="Figure 4" longdesc="" />
            <div class="figure-caption">
              <span class="figure-number">Figure 4:</span>
              <span class="figure-title">Comparison of the RC, PCA
              and SFE approaches on the considered subset of the
              NELL KB. The plot shows the precision of the
              predictions as a function of the number of
              predictions made.</span>
            </div>
          </figure>
          <p></p>
        </section>
      </section>
    </section>
    <section id="sec-22">
      <header>
        <div class="title-info">
          <h2><span class="section-number">5</span> Related
          Work</h2>
        </div>
      </header>
      <p>We now discuss how our work is related to learning
      definite clauses, KB completion, and learning from positive
      and unlabeled data (PU Learning).</p>
      <p><strong>Learning Definite Clauses.</strong> Inductive
      logic programming (ILP) [<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0019">19</a>] is the standard approach for
      learning definite clauses. Typical ILP systems require both
      positive and negative examples to evaluate the score of an
      individual rule. However, there have been several attempts to
      define score functions based only on positive examples and no
      unlabeled data. Muggleton&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0024">24</a>] and the LIME system&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0023">23</a>] developed
      an approach that is based on a Bayesian estimate.
      Effectively, these approaches work by randomly generating
      some negative examples. The SHERLOCK system&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0028">28</a>] employs a
      score function based on statistical relevance and statistical
      significance to identify interesting rules. We differ from
      this approach in that we explicitly attempt to make use of
      the unlabeled data.</p>
      <p><strong>Knowledge Base Completion.</strong> There are many
      approaches to infer additional facts to include in a
      KB&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0014">14</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0015">15</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0018">18</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0025">25</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0031">31</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0032">32</a>]. One set
      of approaches&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0018">18</a>] work by using ideas from ILP and
      relational learning to convert the KB into a graph. Then,
      they construct features based on paths in the graph and build
      one classifier (e.g., logistic regression) per relation.
      Another set of approaches use matrix factorization to obtain
      new relations. Either by putting positive facts into the rows
      of a matrix, and inference rules into the
      columns&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0032">32</a>] or by putting relations into rows
      and entity pairs into the columns of the
      matrix&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0015">15</a>],
      these systems, using scalable matrix factorization
      approaches, learn new facts to complete KBs. Recently, there
      has been interest in neural network
      approaches&nbsp;[<a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover"
      data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0031">31</a>] to this problem. These approaches
      encode entities in a vector representation and attempt to
      predict if a given relation holds for a pair of input
      entities. One of these approaches&nbsp;[<a class="bib"
      data-trigger="hover" data-toggle="popover" data-placement=
      "top" href="#BibPLXBIB0029">29</a>] works in an open-world
      setting, but requires a text corpus to do so. A primary
      advantage to learning rules over the neural network based
      approaches is that the rules are easy for humans to
      interpret, can give an explanation for why a prediction was
      made, and represent new knowledge themselves.</p>
      <p><strong>PU Learning</strong> Our work is clearly related
      to the field of learning from positive and unlabeled
      data&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0008">8</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0010">10</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0021">21</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>]. In this
      setting, a learner only has access to positive examples and a
      (large) set of unlabeled data. The assumption is that the
      unlabeled data contains both positive and negative examples.
      This paper differs from past work in three important ways.
      One, we propose a method for estimating the fraction of
      unlabeled examples that belongs to the positive class. Two,
      we define a novel score function based on this proportion and
      the behavior of the target relation. Three, we focus on
      relational data, whereas the other work on PU learning that
      we are aware of work on propositional data.</p>
      <p>The idea for the relationship between coverage assumption
      (that is, the computation of <em>rc</em> in
      Equation&nbsp;<a class="eqn" href="#eq10">12</a>), is similar
      in spirit to the technique employed by Denis et
      al.&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0008">8</a>]
      for decision tree learning and Ritter et al.&nbsp;[<a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0026">26</a>] for
      logistic regression and Naïve Bayes. However, these works
      assume <em>β</em> was given. Several approaches
      (e.g.,&nbsp;[<a class="bib" data-trigger="hover" data-toggle=
      "popover" data-placement="top" href="#BibPLXBIB0002">2</a>,
      <a class="bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0009">9</a>, <a class=
      "bib" data-trigger="hover" data-toggle="popover"
      data-placement="top" href="#BibPLXBIB0010">10</a>] propose
      methods for estimating this quantity, but from propositional
      data. One of these methods&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0003">3</a>] was recently extended to the
      relational setting, but only focused on predicting unary
      predicates. Ritter et al.&nbsp;[<a class="bib" data-trigger=
      "hover" data-toggle="popover" data-placement="top" href=
      "#BibPLXBIB0027">27</a>] introduce penalties in the relation
      extraction approach for missing data in the text and KB. They
      estimate penalties based on the assumption that extractions
      for popular entities are more likely to be negative than
      those involving rare entities.</p>
    </section>
    <section id="sec-23">
      <header>
        <div class="title-info">
          <h2><span class="section-number">6</span> Conclusions and
          Future Work</h2>
        </div>
      </header>
      <p>In this paper, we have proposed a new confidence measure
      to rank first-order rules learned from an open-world KB.
      Taking inspiration from the learning from positive and
      unlabeled data setting, our metric attempts to incorporate
      information about the unlabeled data into our metric. Our key
      insight is that we only need to reason about number of
      “positive” examples that are not in the KB. We discussed
      several factors that must be accounted for when estimating
      this number and our approach requires no additional
      information apart from what is in the KB. Empirically, our
      metric results in a better ranking than the state-of-the-art
      PCA metric. Furthermore, our proposed approach performs
      better than SFE, which is another well-known approach for KB
      completion. In the future, we will explore different ways to
      estimate <em>β</em>, as well as other ways to reduce the
      consistent underestimate of our confidence measure.
      Additionally, we believe our metric is generally applicable
      and could be used to evaluate rules learned from an
      open-world KBs using other rule learners such as Aleph or
      FOIL and would like to investigate this. Finally, we want to
      evaluate our metric using additional KBs.</p>
    </section>
    <section id="sec-24">
      <header>
        <div class="title-info">
          <h2>Acknowledgments</h2>
        </div>
      </header>
      <p>We would like to thank Jessa Bekker, Hendrik Blockeel and
      the anonymous reviewers for their very helpful feedback on
      this paper. JD is partially supported by the KU Leuven
      Research Fund (C14/17/070, C22/15/015, C32/17/036) and
      FWO-Vlaanderen (G.0356.12, SBO-150033).</p>
    </section>
  </section>
  <section class="back-matter">
    <section id="ref-001">
      <header>
        <div class="title-info">
          <h2 class="page-brake-head">REFERENCES</h2>
        </div>
      </header>
      <ul class="bibUl">
        <li id="BibPLXBIB0001" label="[1]">Sören Auer, Christian
        Bizer, Georgi Kobilarov, Jens Lehmann, Richard Cyganiak,
        and Zachary Ives. 2007. DBpedia: A nucleus for a Web of
        open data. <em><em>The Semantic Web</em></em> 4825(2007),
        722–735.</li>
        <li id="BibPLXBIB0002" label="[2]">Jessa Bekker and Jesse
        Davis. 2018. Estimating the class prior in positive and
        unlabeled data through decision tree induction. In
        <em><em>Proceedings of the 32nd AAAI Conference on
        Artificial Intelligence</em></em> .</li>
        <li id="BibPLXBIB0003" label="[3]">Jessa Bekker and Jesse
        Davis. 2018. Positive and unlabeled relational
        classification through label frequency estimation. In
        <em><em>Proceedings of the 27th International Conference on
        Inductive Logic Programming (ILP)</em></em> .</li>
        <li id="BibPLXBIB0004" label="[4]">Kurt Bollacker, Colin
        Evans, Praveen Paritosh, Tim Sturge, and Jamie Taylor.
        2008. Freebase: a collaboratively created graph database
        for structuring human knowledge. In <em><em>Proceedings of
        the 2008 ACM SIGMOD international conference on Management
        of data</em></em> . Vancouver, Canada, 1247–1250.</li>
        <li id="BibPLXBIB0005" label="[5]">Andrew Carlson, Justin
        Betteridge, and Bryan Kisiel. 2010. Toward an Architecture
        for Never-Ending Language Learning.. In <em><em>Proceedings
        of the 24th Conference on Artificial Intelligence
        (AAAI’10)</em></em> . 1306–1313. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1002/ajp.20927" target=
        "_blank">https://doi.org/10.1002/ajp.20927</a>
        </li>
        <li id="BibPLXBIB0006" label="[6]">Rich Caruana and
        Alexandru Niculescu-Mizil. 2006. An empirical comparison of
        supervised learning algorithms. In <em><em>Proceedings of
        the 23rd international conference on Machine learning
        (ICML’06)</em></em> . 161–168.</li>
        <li id="BibPLXBIB0007" label="[7]">Luc De&nbsp;Raedt, Anton
        Dries, Ingo Thon, Guy Van&nbsp;den Broeck, and Mathias
        Verbeke. 2015. Inducing Probabilistic Relational Rules from
        Probabilistic Examples.. In <em><em>IJCAI</em></em> .
        1835–1843.</li>
        <li id="BibPLXBIB0008" label="[8]">François Denis, Rémi
        Gilleron, and Fabien Letouzey. 2005. Learning from positive
        and unlabeled examples. <em><em>Theor. Comput.
        Sci.</em></em> 348, 1 (2005), 70–83. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1016/j.tcs.2005.09.007" target=
        "_blank">https://doi.org/10.1016/j.tcs.2005.09.007</a>
        </li>
        <li id="BibPLXBIB0009" label="[9]">
        Marthinus&nbsp;Christoffel du Plessis, Gang Niu, and
        Masashi Sugiyama. 2017. Class-prior estimation for learning
        from positive and unlabeled data. <em><em>Machine
        Learning</em></em> 106, 4 (2017), 463–492.</li>
        <li id="BibPLXBIB0010" label="[10]">C. Elkan and K. Noto.
        2008. Learning Classifiers from Only Positive and Unlabeled
        Data. In <em><em>Proceedings of the 14th ACM SIGKDD
        International Conference on Knowledge Discovery and Data
        Mining (KDD 2008)</em></em> . 213–220.</li>
        <li id="BibPLXBIB0011" label="[11]">D. Foxvog. 2010. Cyc.
        In <em><em>Theory and Applications of Ontology: Computer
        Applications</em> (1 ed.)</em>, Roberto Poli, Michael
        Healy, and Kameas Achilles (Eds.). Springer Netherlands,
        259–278.</li>
        <li id="BibPLXBIB0012" label="[12]">Luis Galárraga,
        Christina Teflioudi, Katja Hose, and Fabian&nbsp;M
        Suchanek. 2015. Fast Rule Mining in Ontological Knowledge
        Bases with AMIE +. <em><em>The VLDB Journal</em></em> 24, 6
        (2015), 707–730.</li>
        <li id="BibPLXBIB0013" label="[13]">Matt Gardner and Tom
        Mitchell. 2015. Efficient and Expressive Knowledge Base
        Completion Using Subgraph Feature Extraction. In
        <em><em>Proceedings of EMNLP’15</em></em> . 1488–1498.
        <a class="link-inline force-break" href=
        "https://doi.org/10.1016/j.artint.2016.09.003" target=
        "_blank">https://doi.org/10.1016/j.artint.2016.09.003</a>
        </li>
        <li id="BibPLXBIB0014" label="[14]">Matt Gardner, Partha
        Talukdar, Jayant Krishnamurthy, and Tom Mitchell. 2014.
        Incorporating vector space similarity in random walk
        inference over knowledge bases. In <em><em>Proceedings of
        EMNLP</em></em> .</li>
        <li id="BibPLXBIB0015" label="[15]">Wenqiang He, Yansong
        Feng, Lei Zou, and Dongyan Zhao. 2015. Knowledge Base
        Completion Using Matrix Factorization. In <em><em>Web
        Technologies and Applications - 17th Asia-PacificWeb
        Conference (APWeb’15)</em></em> . 768–780.</li>
        <li id="BibPLXBIB0016" label="[16]">Johannes Hoffart,
        Fabian&nbsp;M. Suchanek, Klaus Berberich, and Gerhard
        Weikum. 2013. YAGO2: A spatially and temporally enhanced
        knowledge base from Wikipedia. <em><em>Artificial
        Intelligence</em></em> 194, Artificial Intelligence,
        Wikipedia and Semi-Structured Resources (2013), 28–61.
        <a class="link-inline force-break" href=
        "https://doi.org/10.1016/j.artint.2012.06.001" target=
        "_blank">https://doi.org/10.1016/j.artint.2012.06.001</a>
        </li>
        <li id="BibPLXBIB0017" label="[17]">Ni Lao, Tom Mitchell,
        and William&nbsp;W Cohen. 2011. Random walk inference and
        learning in a large scale knowledge base. In
        <em><em>Proceedings of the Conference on Empirical Methods
        in Natural Language Processing</em></em> . Association for
        Computational Linguistics, 529–539.</li>
        <li id="BibPLXBIB0018" label="[18]">Ni Lao, Amarnag
        Subramanya, Fernando Pereira, and William&nbsp;W Cohen.
        2012. Reading the web with learned syntactic-semantic
        inference rules. In <em><em>Proceedings of the 2012 Joint
        Conference on Empirical Methods in Natural Language
        Processing and Computational Natural Language
        Learning</em></em> . Association for Computational
        Linguistics, 1017–1026.</li>
        <li id="BibPLXBIB0019" label="[19]">N. Lavrac and S.
        Dzeroski (Eds.). 2001. <em><em>Relational Data
        Mining</em></em> . Springer-Verlag, Berlin.</li>
        <li id="BibPLXBIB0020" label="[20]">Dekang Lin and Patrick
        Pantel. 2001. DIRT@ SBT@ discovery of inference rules from
        text. In <em><em>Proceedings of the seventh ACM SIGKDD
        international conference on Knowledge discovery and data
        mining</em></em> . ACM, 323–328.</li>
        <li id="BibPLXBIB0021" label="[21]">Bing Liu, Yang Dai,
        Xiaoli Li, Wee&nbsp;Sun Lee, and Philip&nbsp;S. Yu. 2003.
        Building Text Classifiers Using Positive and Unlabeled
        Examples. In <em><em>Proceedings of the Third IEEE
        International Conference on Data Mining</em></em> (ICDM
        ’03). IEEE Computer Society, Washington, DC, USA,
        179–.</li>
        <li id="BibPLXBIB0022" label="[22]">Farzaneh Mahdisoltani,
        Joanna Biega, and Fabian&nbsp;M Suchanek. 2015. YAGO3 : A
        Knowledge Base from Multilingual Wikipedias. In
        <em><em>Proceedings of the Conference on Innovative Data
        Systems Research, CIDR ’15</em></em> .</li>
        <li id="BibPLXBIB0023" label="[23]">Eric McCreath and Arun
        Sharma. 1997. ILP with Noise and Fixed Example Size: A
        Bayesian Approach. In <em><em>Proceedings of the Fifteenth
        International Joint Conference on Artifical
        Intelligence</em></em> . Morgan Kaufmann Publishers Inc.,
        San Francisco, CA, USA, 1310–1315.</li>
        <li id="BibPLXBIB0024" label="[24]">Stephen Muggleton.
        1997. Learning from Positive Data. In <em><em>Selected
        Papers from the 6th International Conference on Inductive
        Logic Programming</em></em> (ILP ’96). Springer-Verlag,
        London, UK, UK, 358–376.</li>
        <li id="BibPLXBIB0025" label="[25]">Arvind Neelakantan,
        Benjamin Roth, and Andrew McCallum. 2015. Compositional
        vector space models for knowledge base completion. In
        <em><em>Proceedings of the Annual Meeting of the
        Association for Computational Linguistics</em></em> .</li>
        <li id="BibPLXBIB0026" label="[26]">Alan Ritter, Evan
        Wright, William Casey, and Tom Mitchell. 2015. Weakly
        Supervised Extraction of Computer Security Events from
        Twitter. In <em><em>The 24th ACM International World Wide
        Web Conference (WWW’15)</em></em> . 896–905. <a class=
        "link-inline force-break" href=
        "https://doi.org/10.1145/2736277.2741083" target="_blank">
          https://doi.org/10.1145/2736277.2741083</a>
        </li>
        <li id="BibPLXBIB0027" label="[27]">Alan Ritter, Luke
        Zettlemoyer, Mausam, and Oren Etzioni. 2013. Modeling
        Missing Data in Distant Supervision for Information
        Extraction. <em><em>Transactions of the Association for
        Computational Linguistics</em></em> 1 (2013), 367–37.</li>
        <li id="BibPLXBIB0028" label="[28]">Stefan Schoenmackers,
        Oren Etzioni, Daniel&nbsp;S Weld, and Jesse Davis. 2010.
        Learning first-order horn clauses from web text. In
        <em><em>Proceedings of the 2010 Conference on Empirical
        Methods in Natural Language Processing</em></em> .
        Association for Computational Linguistics, 1088–1098.</li>
        <li id="BibPLXBIB0029" label="[29]">Baoxu Shi and Tim
        Weninger. 2018. Open-World Knowledge Graph Completion. In
        <em><em>Proceedings of 32nd AAAI Conference on Artificial
        Intelligence</em></em> .</li>
        <li id="BibPLXBIB0030" label="[30]">Amit Singhal. 2012.
        Introducing the Knowledge Graph: things, not strings.
        (2012).</li>
        <li id="BibPLXBIB0031" label="[31]">Richard Socher, Danqi
        Chen, Christopher&nbsp;D Manning, and Andrew Ng. 2013.
        Reasoning with neural tensor networks for knowledge base
        completion. In <em><em>Advances in neural information
        processing systems</em></em> . 926–934.</li>
        <li id="BibPLXBIB0032" label="[32]">William&nbsp;Yang Wang
        and William&nbsp;W Cohen. 2016. Learning First-Order Logic
        Embeddings via Matrix Factorization. In <em><em>Proceedings
        of the 25th International Joint Conference on Artificial
        Intelligence (IJCAI’16)</em></em> . 2132–2138.</li>
        <li id="BibPLXBIB0033" label="[33]">Bianca Zadrozny and
        Charles Elkan. 2001. Obtaining calibrated probability
        estimates from decision trees and naive Bayesian
        classifiers. In <em><em>Proceedings of the Eighteenth
        International Conference on Machine Learning
        (ICML’01)</em></em> . 609–616.</li>
      </ul>
    </section>
  </section>
  <section id="foot-001" class="footnote">
    <header>
      <div class="title-info">
        <h2>FOOTNOTE</h2>
      </div>
    </header>
    <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a><a class=
    "link-inline force-break" href=
    "https://www.wikidata.org">https://www.wikidata.org</a></p>
    <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>This is the
    standard assumption made in PU learning&nbsp;[<a class="bib"
    data-trigger="hover" data-toggle="popover" data-placement="top"
    href="#BibPLXBIB0010">10</a>].</p>
    <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>The only change
    is inserting the 1 − in front of the harmonic mean, which we do
    simplify the notation later on.</p>
    <div class="bibStrip">
      <p>This paper is published under the Creative Commons
      Attribution 4.0 International (CC-BY&nbsp;4.0) license.
      Authors reserve their rights to disseminate the work on their
      personal and corporate Web sites with the appropriate
      attribution.</p>
      <p><em>WWW '18, April 23–27, 2018, Lyon, France</em></p>
      <p>© 2018; IW3C2 (International World Wide Web Conference
      Committee), published under Creative Commons CC-BY&nbsp;4.0
      License.<br />
      ACM ISBN 978-1-4503-5639-8/18/04.<br />
      DOI: <a class="link-inline force-break" target="_blank" href=
      "https://doi.org/10.1145/3178876.3186006">https://doi.org/10.1145/3178876.3186006</a></p>
    </div>
  </section>
  <div class="pubHistory">
    <p></p>
  </div>
</body>
</html>
