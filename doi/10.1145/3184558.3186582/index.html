<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Automated Extractions for Machine Generated Mail</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">
<div>
<p style='font-size: 75%; color #444'>
This is a HTML copy of <a href='https://doi.org/10.1145/3184558.3186582'>https://doi.org/10.1145/3184558.3186582</a> 
originally published by ACM, 
redistributed under the terms of 
<a href='https://creativecommons.org/licenses/by/4.0/'>Creative Commons Attribution 4.0 (CC BY 4.0)</a>.
The <a href='https://github.com/usable-oa/thewebconf2018/tree/master/scripts'>modifications</a> 
from the original are solely to improve HTML accessability, compatibility, 
augmenting HTML metadata and avoiding ACM trademark.
To reference this HTML version, use:
</p><p>
<strong>Permalink:</strong>
<a href='https://w3id.org/oa/10.1145/3184558.3186582'>https://w3id.org/oa/10.1145/3184558.3186582</a>
</p></div>
<hr>

  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Automated Extractions for Machine Generated Mail</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Dotan Di</span>     <span class="surName">Castro</span>     Yahoo Research, Haifa, Israel, <a href="mailto:dot@oath.com">dot@oath.com</a>    </div>    <div class="author">     <span class="givenName">Iftah</span>     <span class="surName">Gamzu</span>     Amazon Research, Haifa, Israel, <a href="mailto:iftah@amazon.com">iftah@amazon.com</a>    </div>    <div class="author">     <span class="givenName">Irena</span>     <span class="surName">Grabovitch-Zuyev</span>     Yahoo Research, Haifa, Israel, <a href="mailto:iragz@oath.com">iragz@oath.com</a>    </div>    <div class="author">     <span class="givenName">Liane</span>     <span class="surName">Lewin-Eytan</span>     Amazon Research, Haifa, Israel, <a href="mailto:lliane@amazon.com">lliane@amazon.com</a>    </div>    <div class="author">     <span class="givenName">Abhinav</span>     <span class="surName">Pundir</span>     Yahoo Inc., Sunnyvale, CA, USA, <a href="mailto:abhin@oath.com">abhin@oath.com</a>    </div>    <div class="author">     <span class="givenName">Nil Ratan</span>     <span class="surName">Sahoo</span>     Yahoo Inc., Sunnyvale, CA, USA, <a href="mailto:nilratan@oath.com">nilratan@oath.com</a>    </div>    <div class="author">     <span class="givenName">Michael</span>     <span class="surName">Viderman</span>     Yahoo Research, Haifa, Israel, <a href="mailto:viderman@oath.com">viderman@oath.com</a>    </div>                                </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3184558.3186582" target="_blank">https://doi.org/10.1145/3184558.3186582</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3184558" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Mail extraction is a critical task whose objective is to extract valuable data from the content of mail messages. This task is key for many types of applications including re-targeting, mail search, and mail summarization, which utilize the important personal data pieces in mail messages to achieve their objectives. We focus on machine generated traffic, which comprises most of the Web mail traffic today, and use its structured and large-scale repetitive nature to devise a fully automated extraction method. Our solution builds on an advanced structural clustering technique previously presented by some of the authors of this work. The heart of our solution is an offline process that leverages the structural mail-specific characteristics of the clustering, and automatically creates extraction rules that are later applied online for each new arriving message. We provide of a full description of our process, which has been productized in Yahoo mail backend. We complete our work with large-scale experiments carried over real Yahoo mail traffic, and evaluate the performance of our automatic extraction method.</small>    </p>    </div>    <div class="classifications">    <div class="author">     <span style="font-weight:bold;">      <small>Keywords:</small>     </span>     <span class="keyword">      <small>Mail Extraction</small>, </span>     <span class="keyword">      <small> Automated Extraction</small>, </span>     <span class="keyword">      <small> Machine Generated Mail</small>, </span>     <span class="keyword">      <small> Mail Clustering</small>     </span>    </div>    <br/>    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Dotan Di Castro, Iftah Gamzu, Irena Grabovitch-Zuyev, Liane Lewin-Eytan, Abhinav Pundir, Nil Ratan Sahoo, and Michael Viderman. 2018. Automated Extractions for Machine Generated Mail. In <em>WWW '18 Companion: The 2018 Web Conference Companion,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 8 Pages. <a href="https://doi.org/10.1145/3184558.3186582" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3184558.3186582</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-8">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>    </div>    </header>    <p>During the last few years, there has been a surge of interest in analyzing machine generated mail. Machine generated messages are commonly created by scripts on behalf of commercial entities or organizations, and comprises more than 90% of non-spam Web mail traffic&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>]. There are numerous examples of such messages, including purchase receipts, travel reservations, events and social notifications, and more.</p>    <p>Most of the work that has been done in this context utilizes the special characteristics of this traffic. Arguably, the two main characteristics of such messages are that they are highly-structured, and that similar messages are sent at large scale across the users population. The identical message structure and repetitive parts of messages generated by the same script allow a dedicated analysis, and enable the application of automated data mining and learning methods at scale. Several purposes for such methods have been presented, including mail classification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>], mail anonymization&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>], and mail extraction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>].</p>    <p>In this work, we focus on mail extraction, where the goal is to extract valuable information from the body of mail messages. More specifically, we concentrate on developing a fully automated method for identifying and extracting valuable data parts from machine generated traffic. Note that although machine generated messages are created by scripts, they typically include personal data pieces. These are usually the pieces of information we wish to extract. Examples of such might be the item that was purchased, its date of delivery, or the travel details in an itinerary.</p>    <p>Mail extraction is critical for many applications like re-targeting, mail search, and mail summarization used by various user facing features. For example, re-targeting can be optimized using the analysis of users interests based on their transactions; mail search can be improved by properly indexing extracted data to enrich search features, and mail summarization can be better performed after identifying the important parts of messages.</p>    <p>The general task of data extraction has been a major challenge in the Web domain, and many solutions that consider Web documents have been developed. However, these solutions either require types of similarities to which mail messages do not conform (e.g., semantic or statistical term similarities&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]), or, as they tend to be rather generic, do not exploit valuable mail-specific characteristics that can greatly improve the quality of extractions (e.g., compare with the Web pages structural methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>]). There are also continuous efforts to formalize schemes for structured data on the Internet, including web pages, mail messages and others. The well-known <em>schema.org</em> defines a full hierarchy of data vocabulary for different mail types (e.g., &#x201C;FlightReservation&#x201D;, &#x201C;ParcelDelivery&#x201D;) with the goal of creating a standard that would be supported by major services to power extensible experiences. Despite these efforts, only a small part of mail traffic adheres to this schema<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>, and thus, it does not provide sufficient coverage of the traffic.</p>    <p>Mail data extractions can be performed most efficiently at scale when based on a clustering of the traffic, rather than on a single message. The objective in clustering of mail traffic is to group together messages with similar structure and intent, so that extraction rules can be defined and applied for the entire cluster. Intuitively, such a clustering aims to capture the scope of the generating scripts. Different clustering techniques have been proposed in the context of mail, starting from clusters based on the message header&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>] to more advanced techniques based on the message structure&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>]. Some of the authors of this work have recently introduced new structural clustering methods that can be conducted at different levels of granularity, using strict or flexible matching constraints&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>]. That work focuses on clustering and considers the mail extraction task as a primary use case. However, the extraction method employed in that work is based on manually defined rules.</p>    <p>In the current work, we propose a fully-automated extraction solution. The progress that was made in clustering machine generated traffic underlies this new process. We utilize the structural clustering solution&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>], and exploit its characteristics in the development of our solution. The heart of our solution is a fully-automated way for creation of extraction rules. The rules are generated in an offline manner per cluster, and are applied online upon an arrival of new mail messages. We present the complete solution, and analyze its performance using both professional editorial evaluation, and an offline extraction evaluation on a real large scale data-set of Yahoo mail service. The editorial evaluation is used to assess the quality of the extraction rules created automatically by our process, while the large scale offline evaluation is used to estimate the quality of the final extraction output. Using both types of evaluation, we derive some insights on the strengths and challenges of information extraction in the Web mail domain.</p>    <p>The rest of this paper is organized as follows. Section <a class="sec" href="#sec-9">2</a> covers some related work in the context of information extraction. Section&#x00A0;<a class="sec" href="#sec-10">3</a> describes the automated mail extraction approach and the main steps of the rules creation process. Section <a class="sec" href="#sec-15">4</a> presents our system architecture as productized in Yahoo mail backend, along a traffic analysis. In Section <a class="sec" href="#sec-18">5</a>, we present our evaluation, focusing on the travel domain, and considering the different phases of our automatic extraction process. Finally, we conclude in Section <a class="sec" href="#sec-25">6</a>.</p>   </section>   <section id="sec-9">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> Related Work</h2>    </div>    </header>    <p>Information extraction is a major research challenge at Web scale. Traditional techniques range from <em>source-centric</em>, in which data is extract from an explicit source (like a specific website), to <em>web-centric</em>, where data is extracted from the entire Web&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>]. A web-centric approach is commonly confined to extracting simple entities (such as tables and lists) as well as some relational data. This approach finds it difficult to attach semantics to entities. As opposed, a source-centric or domain-centric approach, which considers a collection of sources, allows utilization of specific schemas to be populated (e.g., restaurant recommendations), as well as supervision at the domain level&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>]. In this sense, information extraction adapted to machine-generated mail traffic is much closer to the domain-centric approach, as it collects data according to domain-specific types. The programs that extract information are called <em>extractors</em> or <em>wrappers</em>, and the reader is referred to surveys&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] for further reading about these concepts.</p>    <p>The task of information extraction processes online documents which are semi-structured and generated automatically by server-side applications. As such, it usually applies machine learning and pattern mining techniques to exploit the syntactical patterns or layout structures of the template-based documents. In a sense, most techniques seek to separate between the layout and the data. There is an abundant amount of research on extraction techniques, where some of the main ones build on Web page structure&#x00A0; [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>], pattern recognition&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>], tree methods&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>], repetitive information identification&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>], vision techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>], and more. These techniques often build on clustering of the underlying documents, focusing mainly on HTML-based pages. Common approaches for identifying document similarity are based on the semantic or statistical term similarity of documents (see, e.g., [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] for an introduction of related techniques), or on methods for document reduction and canonical sequence representation&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>]). Although machine-generated emails are typically documents in rich HTML format, they do not easily conform to the above types of similarity when structural considerations (like the ones required for data extraction) are meaningful. Identifying document similarity based on its structure has been given a lot of consideration in the past&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0006">6</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. However, those techniques are not dedicated to the mail domain, and therefore, do not exploit meaningful mail signals.</p>    <p>An important part of our automatic extraction process comprises of the identification and annotation of different information parts. Example of such are names, places, products, and more. Generally, identification of entities in documents is known as name entity recognition&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>]. Some major methods in this field involve databases and statistical models, while others use machine learning or linguistic grammar-based techniques in order to identify entities using parts of speech. There are few papers that consider entity recognition in the context of informal documents such as mail messages, like the research on contacts information extraction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>], and the work of [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] that uses NLP methods to identify names. These works are not targeted to the mail domain, nor to machine-generated traffic. As a result, they do not make use of the ensemble of documents as provided by the clustering of this traffic and of the statistical power it provides. Our method allows treating any set of entities that are present in such traffic using a modular approach. This is partly achieved as our process is based on the horizontal analysis of each cluster of messages, rather than on a single message.</p>    <p>The only work we are aware of, which treats a similar problem as ours, is the work of Zhang et al. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] that considers the extraction of structured data from emails. Our work can be seen as complementary to their work in several aspects. Zhang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] describes a general approach for extracting and annotating product names. It concentrates on the learning algorithm and optimization behind the annotation process, rather than on the implementation details and the deployment of the approach. For example, their extraction process is based on abstracted entities (clusters and templates), but specific details about their computation or level of abstraction are not provided, making the approach irreproducible. One contribution of our work lies in the description of the complete solution, which is based on a known notion of structural clustering. As part of our solution, we precisely describe and analyze how the structural characteristics of these cluster entities are processed and exploited for the creation of extraction rules. Furthermore, in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], a separate learning model is computed per template, while in the current work, we compute one generic model which works for the entire set of clusters belonging to a specific category (e.g. travel, purchases, etc.). Note that the number of clusters is already in the low millions for travel-related domains. As scale is a crucial issue in a system processing email traffic, this is yet another key difference between the two works. This difference is also reflected in the traffic coverage of the experiments; Zhang et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>] considers about 20K messages (associated with 20 templates), while our work considers 10M messages (associated with 36K clusters). A final difference is that Zhang et al. focuses on the purchases domain while ours take the travel domain as an exemplary use case. These are indeed two different important business-consumer classes. Both of them have some support in all major Web mail services (e.g., the <em>Bulk Senders</em> category in AOL mail, the semantic <em>Bundles</em> in Inbox for Gmail, and the <em>Smart Views</em> in Yahoo mail).</p>   </section>   <section id="sec-10">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> The Mail Extraction Process</h2>    </div>    </header>    <p>The mail extraction process comprises two parts, as presented in Figure <a class="fig" href="#fig1">1</a>. The first part is an offline process during which extraction rules are automatically created. The second part is an online process where upon the arrival of a new message, the appropriate rules are identified and applied, resulting in extracted information. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186582/images/www18companion-136-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Flow logic of automatic mail extraction process.</span>     </div>    </figure>    </p>    <p>We focus on the automatic offline process. Our approach is modular, and can be applied to any class of mail messages such as travel, social or finance. The steps of the process that require adaptation to a specific class are mentioned along the section.</p>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Preliminaries</h3>     </div>    </header>    <p>An essential and first building block in our approach is a clustering method that groups together machine generated emails having the exact same structure. This method is known as <em>x-clustering</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>]. Without delving into technicalities, the method represents each message as an ordered list of all the XPath expressions in its DOM tree, and then applies a so-called Mail-Hash signature to that list. This implies a clustering of messages according to their resulting (structure-based) signature. The generated clusters are referred to as XPath clusters, or <em>x-clusters</em>.</p>    <p>Importantly, this structure-based clustering enables us to easily collect multiple (different) message instances that result from the same generative script. We process and analyze those messages to identify the concrete locations that hold information of interest, that is, information to be extracted. Typically, this kind of information is present in <em>variable XPaths</em>, which are XPaths whose values vary between messages in the same cluster. Those variable data pieces commonly hold the most valuable information of the message. Xpaths that hold fixed information over the entire cluster are called <em>constant XPaths</em>, and are typically of low value. Subsequently, we define rules that can extract variable information and annotate it. The resulting extraction rules are then used to extract data online, as mentioned earlier. Specifically, once a message arrives to the mail system, its x-cluster signature is computed, and the associated extraction rules are retrieved and applied to the message.</p>    </section>    <section id="sec-12">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Overview: Automatic Rules Creation</h3>     </div>    </header>    <p>Given a large corpus of email messages collected over several (not necessarily consecutive) days, our process begins by grouping together messages into x-clusters. Then, it processes each x-cluster independently to create extraction rules per cluster. The process applied to the x-clusters is described below.</p>    <p>     <strong>Input &#x0026; Output</strong> Given an x-cluster, we gather a random sample of email messages that are associated with the cluster, namely, messages having the exact same structure. We require the sample size to be sufficiently large, usually around several dozens of messages (parameterized by a threshold <em>&#x0393;</em>), to allow effective and precise identification of variable data. Note that the difference between constant and variable data is sometimes delicate and far from being absolute. For example, there are sometimes variations in what we consider constant data to account for identical message structure under different languages. This technical issue is of secondary importance, and therefore, we neglect it from the current discussion.</p>    <p>It is instructive to represent the sample data as a table whose rows correspond to different messages while its columns correspond to different XPaths. Such a tabular representation is called an <em>x-cluster table</em>, an example of which is presented in Table&#x00A0;<a class="tbl" href="#tab1">1</a>. Note that some of the XPaths in this example are variable (e.g., XPath<sub>1</sub>, XPath<sub>4</sub>, and XPath<sub>7</sub>), while others are constant (e.g., XPath<sub>2</sub>, XPath<sub>3</sub>, and XPath<sub>6</sub>). In particular, note that XPaths that consist of both constant and variable sub-parts (like, XPath<sub>1</sub>) are regarded as variable. Those mixed XPaths are usually the most challenging for defining extraction rules. In the remainder of this work, we concentrate only on extraction rules for variable XPaths.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">An example of an input table for the rules creation process.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:center;"/>       <td style="text-align:left;">XPath<sub>1</sub>       </td>       <td style="text-align:left;">XPath<sub>2</sub>       </td>       <td style="text-align:left;">XPath<sub>3</sub>       </td>       <td style="text-align:left;">XPath<sub>4</sub>       </td>       <td style="text-align:left;">XPath<sub>5</sub>       </td>       <td style="text-align:left;">XPath<sub>6</sub>       </td>       <td style="text-align:left;">XPath<sub>7</sub>       </td>       <td style="text-align:left;">...</td>       </tr>       <tr>       <td style="text-align:center;">msg 1</td>       <td style="text-align:left;">Thank you <strong>John</strong>!,</td>       <td style="text-align:left;">Below is ...</td>       <td style="text-align:left;">Upcoming Trip:</td>       <td style="text-align:left;">        <strong>07/08/16</strong> - <strong>Ontario</strong>       </td>       <td style="text-align:left;">...</td>       <td style="text-align:left;">AIR Confi...</td>       <td style="text-align:left;">        <strong>ABC123</strong>       </td>       <td style="text-align:left;">...</td>       </tr>       <tr>       <td style="text-align:center;">msg 2</td>       <td style="text-align:left;">Thank you <strong>Arya</strong>!,</td>       <td style="text-align:left;">Below is ...</td>       <td style="text-align:left;">Upcoming Trip:</td>       <td style="text-align:left;">        <strong>06/09/16</strong> - <strong>New York</strong>       </td>       <td style="text-align:left;">...</td>       <td style="text-align:left;">AIR Confi...</td>       <td style="text-align:left;">        <strong>Z5T8Q2</strong>       </td>       <td style="text-align:left;">...</td>       </tr>       <tr>       <td style="text-align:center;">msg 3</td>       <td style="text-align:left;">Thank you <strong>Khal</strong>!,</td>       <td style="text-align:left;">Below is ...</td>       <td style="text-align:left;">Upcoming Trip:</td>       <td style="text-align:left;">        <strong>23/08/16</strong> - <strong>Boston</strong>       </td>       <td style="text-align:left;">...</td>       <td style="text-align:left;">AIR Confi...</td>       <td style="text-align:left;">        <strong>ER46T1</strong>       </td>       <td style="text-align:left;">...</td>       </tr>       <tr>       <td style="text-align:center;">msg 4</td>       <td style="text-align:left;">Thank you <strong>Lucy</strong>!,</td>       <td style="text-align:left;">Below is ...</td>       <td style="text-align:left;">Upcoming Trip:</td>       <td style="text-align:left;">        <strong>13/08/16</strong> - <strong>Seattle</strong>       </td>       <td style="text-align:left;">...</td>       <td style="text-align:left;">AIR Confi...</td>       <td style="text-align:left;">        <strong>61FXC1</strong>       </td>       <td style="text-align:left;">...</td>       </tr>       <tr>       <td style="text-align:center;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">        <strong>.</strong>..</td>       <td style="text-align:left;">...</td>       </tr>      </tbody>     </table>    </div>    <p>As already noted, the output of the process are extraction rules for the underlying x-cluster. Each extraction rule is associated with some XPath of the x-cluster. At the most intuitive level, a rule provides a description of the way to extract variable data pieces from the XPath, and suggests an <em>annotation</em> for each of those pieces. An annotation is chosen from a pre-defined set of information types to be extracted, e.g., < passenger-name > , < confirmation-code > , < date-depart > . Figure&#x00A0;<a class="fig" href="#fig2">2</a> shows an example of an extraction rule that was generated for a specific XPath, based on a set of values collected from different messages. In this case, constant (repeating) sub-parts are identified, while variable sub-parts are given a placeholder and annotation. The concrete way by which this rule can be utilized for future extractions is a matter of implementation. One possibility is to interpret this rule as a regular expression by replacing the placeholders with regexes that can match the XPath values.</p>    <p>mybox = [line width=0.1mm, draw=black, rectangle, inner sep=2pt, inner ysep=2pt, minimum width=14cm, minimum height = 1cm] myarrows=[line width=0.2mm,draw=black,-triangle 45,postaction=draw, line width=1mm, shorten >=2mm, -] <figure id="fig2">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186582/images/www18companion-136-fig3.svg" class="img-responsive" alt="Figure 2"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 2:</span>       <span class="figure-title">Example of an extraction rule.</span>      </div>     </figure>    </p>    </section>    <section id="sec-13">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Step 1: Basic Rule Creation</h3>     </div>    </header>    <p>Our first objective is to create a basic rule that identifies constant and variable sub-parts within an XPath. We start by performing a first round of annotations of the tokens in the sample set. This first round is based on <em>light annotations</em> that are general and independent of the context. For instance, we may use a date annotation without a concrete contextual interpretation indicating that the date corresponds to a flight arrival, package delivery, service expiration, or else. Essentially, this views the set of annotations as a two-level hierarchy in which first level annotations are general (e.g., < date >), and second level annotations are refined (e.g, < date-arrive > for a flight). By replacing tokens with annotations, we are able to identify popular tokens that are part of variable XPaths, and are not supposed to be considered constants. For example, for an XPath with a large number of samples referring to SFO airport, the token &#x201C;SFO&#x201D; would be annotated as < airport > and identified as valuable, even if appearing in most of the samples.</p>    <p>The task of annotating tokens clearly depends on the set of supported annotations and the definition of the information that ought to be extracted. Many techniques can be used here. Examples range from brute-force dictionary search and regular expression patterns matching, to more involved machine learning based classification procedures. A dictionary comprises a set of words related to a specific subject. For instance, one can build a dictionary of personal names. Given a dictionary and a term (token), we search the dictionary, and replace any identified token with a respective annotation. Consider the text &#x201C;<tt>Hello Jack, Thank you for flying with us!</tt>&#x201D;. Using the dictionary method, this sentence will be annotated as &#x201C;<tt>Hello <name>, Thank you for flying with us!</tt>&#x201D;. Similarly, one can define regular expressions that identify annotations such as date or time. For example, given the text &#x201C;<tt>Departing at 9:00 AM (PST)</tt>&#x201D;, if we use time-related regexes, the sentence will be annotated as &#x201C;<tt>Departing at <time></tt>&#x201D;. In addition, it is likely one would need build more involved classification procedures for complicated annotation tasks such as identification of product names. The specific annotation types used for flights are specified in Table&#x00A0;<a class="tbl" href="#tab3">3</a>.</p>    <p>We note that a given technique will likely not be accurate enough to identify all tokens in all sampled phrases. Since our sample is large enough and chosen at random from the cluster, we are still expected to identify many (if not most) of the tokens. The approach described relies on the strength of the clustering phase, allowing successful annotations over a large number of similar messages, but not necessarily over the entire sample. This way, patterns can be identified even if for some cases the annotation fails.</p>    </section>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Step 2: Rule Refinement</h3>     </div>    </header>    <p>In the second step, we refine the light annotations from the first step to include contextual meaning. For example, we&#x0027;d like to understand whether an airport code stands for the departure airport or the arrival airport in a flight itinerary. This requires departing from the local XPath view, and considering connections between multiple XPaths, sometimes within the entire x-cluster. As annotations take context into consideration, the supported annotations are specifically tailored to the use case, e.g., annotations for flights would be different than for purchases. A date token in an itinerary may stand for a departure or arrival date, while it may represent the expected delivery date when the message is a shipping notification. We assume to know the classification of the underlying x-cluster, which can be generated using known techniques&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>].</p>    <p>Our approach is based on learning the context of each annotation by the use of machine learning. Given an x-cluster table of cluster <em>c</em>, we associate each light annotation of a variable XPath <em>XP<sub>i</sub>     </em> with a vector of features. These features are naturally based on the characteristics of <em>XP<sub>i</sub>     </em>, but also on the other XPaths in the x-cluster, i.e., {<em>XP<sub>k</sub>     </em>} where <em>k</em> = 1, &#x2026;, <em>i</em> &#x2212; 1, <em>i</em> + 1, &#x2026;<em>K</em>, <em>K</em> being the number of XPaths in <em>c</em>. The features used for can be divided into two main types: one includes the light annotations, and the other includes indicative words. For example, in the travel domain, light annotations could be < time > , < airport > or < name > , while indicative words could be &#x201C;passenger&#x201D;, &#x201C;depart&#x201D; or &#x201C;arrive&#x201D;. The collection of inductive words are identified using both human experts and automatic statistical techniques (e.g., tf-idf weighting schemes). The annotations and words considered relevant for some <em>XP<sub>i</sub>     </em> are those that can be found at its proximity. Proximity is defined in two ways. The first is the distance between XPaths according to the DOM tree representation of <em>c</em>. The second way by which proximity is defined is based on the visualization of the cluster&#x0027;s structure and uses the CSS<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> of the HTML. It can be specifically tailored to HTML tables, where a feature is associated with the relative position of its table entry with respect to <em>XP<sub>i</sub>     </em>.</p>    <p>Based on these features, we can classify each light annotation to a contextual annotation. We follow standard machine learning procedures for training the classifier, and use Random Forest&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0024">24</a>] after carefully comparing with other classifiers. Details regarding the train and test sets used for the classifier are presented in Section&#x00A0;<a class="sec" href="#sec-18">5</a>.</p>    </section>   </section>   <section id="sec-15">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> System &#x0026; Traffic Analysis</h2>    </div>    </header>    <p>The mail extraction process has been productized in Yahoo mail backend. We provide details regarding its deployment along with some traffic analysis, focusing on flight itineraries use case.</p>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.1</span> Deployment</h3>     </div>    </header>    <p>The architecture of the mail extraction system, as deployed in Yahoo mail backend, is described in Figure <a class="fig" href="#fig3">3</a>. The classification, clustering, sampling and rule creation modules are all part of the offline process, leading to the creation of the extraction rules. The rules creation module is the heart of our process, and is fully covered in Section&#x00A0;<a class="sec" href="#sec-10">3</a>. Sampling is performed for clusters of predefined classes that are supported by the extraction process (e.g. flights, coupons, purchases, etc.) as annotations are specific for each use case. The classification is attained using the method described in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. Thus, sampling is based on the output of the classification and clustering modules. The output of the sampling module are x-cluster tables for all x-clusters that have enough samples, based on the threshold <em>&#x0393;</em> set in the system. The concrete value of <em>&#x0393;</em> depends on the desired level of trade-off between quality and coverage, as discussed later on. The rules, the samples, and all information related to their respective x-clusters and annotations are stored in the <em>Rules Management Framework</em> (RMF). The RMF has a UI interface that allows human intervention for quality assurance purposes, or modification of certain rules in case of need. The online process is applied over the incoming messages stream. For each message, its x-cluster is first computed. Then, the respective rule is fetched from the RMF and applied to the message. Finally, the extraction data is stored in a dedicated storage, to be further used by various applications. <figure id="fig3">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186582/images/www18companion-136-fig5.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 3:</span>       <span class="figure-title">The architecture of the mail extraction system.</span>      </div>     </figure>    </p>    <p>The online extraction process is extremely light-weight by design. When it is applied to a message under consideration it is essentially instantaneous, that is, it takes only few milliseconds to complete in most cases. The more involved computational part of our approach is the offline rules creation process. This process is implemented in Hadoop MapReduce&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0017">17</a>]. This enables creating the extraction rules efficiently in parallel as an extraction rule for one x-clusters is independent of the rules generated for other x-clusters. The most computation-intensive step in our approach is the random sampling which requires going over the entire inbound mail corpus during some time period. Providing concrete time measure for each of the modules seems redundant as they depend on the number of messages that are taken into consideration, the number of computational units used in the parallel processing, additional load on the MapReduce system, and more. Still, we note that the time consumed by the automatic rules creation module is smaller by roughly an order of magnitude than the time of the preliminary steps, i.e., clustering, classification and sampling. For example, when the preliminary steps take few hours, the rules creation takes only few minutes.</p>    </section>    <section id="sec-17">    <header>     <div class="title-info">      <h3>       <span class="section-number">4.2</span> Traffic Analysis</h3>     </div>    </header>    <p>We analyze the characteristics of the traffic corresponding to flight itineraries with respect to the phases preceding the rules creation, namely, classification, clustering and sampling. The analysis was performed over one week of traffic during April 2017, leading to about 500 domains that contained x-clusters classified as flight itineraries. We first look at the distribution of traffic and x-clusters per domain, presented in Figure&#x00A0;<a class="fig" href="#fig4">4</a>. The domains in the x-axis are ordered according to decreasing number of x-clusters in each domain. Thus, the curve descending exponentially corresponds to the distribution of x-clusters number per domain. The second curve corresponds to the distribution of the traffic (number of messages) per domain, and is represented as percentage of the overall traffic classified as flight itineraries (that is, overall number of messages belonging to x-clusters classified as itineraries). The top-10 domains with respect to the number of x-clusters are listed in Table&#x00A0;<a class="tbl" href="#tab2">2</a>. For each domain, we specify the number of x-clusters and traffic coverage. As can be seen, there is a long tail of flight itinerary domains, where the number of x-clusters varies between only dozens, to hundreds or thousands, up to above 60K. It can also be seen that there is no correlation between this number and the traffic coverage of a domain. This leads to the understanding that there is a need for a robust method that can handle different &#x2019;types&#x2019; of domains, and can reach high coverage and extraction quality with large as well as small amounts of traffic per domain and x-cluster. This is further emphasized in the next figure as well as in Section&#x00A0;<a class="sec" href="#sec-24">5.3</a>. <figure id="fig4">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186582/images/www18companion-136-fig6.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 4:</span>       <span class="figure-title">Domains x-clusters number and traffic.</span>      </div>     </figure>    </p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Top-10 flight domains wrt. x-clusters number.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:left;">        <strong>Domain</strong>       </td>       <td style="text-align:left;">        <strong>x-clusters</strong>       </td>       <td style="text-align:left;">        <strong>coverage</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">united.com</td>       <td style="text-align:left;">67,835</td>       <td style="text-align:left;">4.37%</td>       </tr>       <tr>       <td style="text-align:left;">e.delta.com</td>       <td style="text-align:left;">34,804</td>       <td style="text-align:left;">5.68%</td>       </tr>       <tr>       <td style="text-align:left;">luv.southwest.com</td>       <td style="text-align:left;">30,926</td>       <td style="text-align:left;">8.02%</td>       </tr>       <tr>       <td style="text-align:left;">amadeus.com</td>       <td style="text-align:left;">22,169</td>       <td style="text-align:left;">2.01%</td>       </tr>       <tr>       <td style="text-align:left;">aa.globalnotifications.com</td>       <td style="text-align:left;">20,532</td>       <td style="text-align:left;">1.74%</td>       </tr>       <tr>       <td style="text-align:left;">cheapoair.com</td>       <td style="text-align:left;">16,473</td>       <td style="text-align:left;">0.73%</td>       </tr>       <tr>       <td style="text-align:left;">expediamail.com</td>       <td style="text-align:left;">12,638</td>       <td style="text-align:left;">1.88%</td>       </tr>       <tr>       <td style="text-align:left;">t.spiritairlines.com</td>       <td style="text-align:left;">12,332</td>       <td style="text-align:left;">4.70%</td>       </tr>       <tr>       <td style="text-align:left;">cs.tuiuk.com</td>       <td style="text-align:left;">10,504</td>       <td style="text-align:left;">0.45%</td>       </tr>       <tr>       <td style="text-align:left;">alaskaair.com</td>       <td style="text-align:left;">8,284</td>       <td style="text-align:left;">0.62%</td>       </tr>      </tbody>     </table>    </div>    <p>Figure&#x00A0;<a class="fig" href="#fig5">5</a> exhibits the coverage obtained for different threshold values for the sample size. That is, it represents the relative coverage percentage when taking into consideration only x-clusters that have more than <em>&#x0393;</em> messages. The three curves in the figures correspond to coverage in terms of number of x-clusters, number of mail messages, and number of domains. It is important to note that while the number of covered x-clusters drops drastically for values of <em>&#x0393;</em> &#x2265; 20, the coverage in terms of number of messages and domains remain close to or above 80%. This affirms the strength of our solution, which allows a relatively high coverage for a wide range of threshold values. Remark that for small values of <em>&#x0393;</em>, the coverage is higher, but the small number of samples might negatively influence the quality of the extractions. Notably, our solution can be easily adjusted to optimize between quality and coverage. This trade-off is further explored in Section&#x00A0;<a class="sec" href="#sec-24">5.3</a>. <figure id="fig5">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186582/images/www18companion-136-fig7.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 5:</span>       <span class="figure-title">Trade-off between sample size and coverage.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-18">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> Experimental Evaluation</h2>    </div>    </header>    <p>We demonstrate the value of our approach through an offline evaluation based on Yahoo Web mail traffic. As in Section <a class="sec" href="#sec-15">4</a>, we adapt our general approach, detailed in Section&#x00A0;<a class="sec" href="#sec-10">3</a>, to the travel domain, and more specifically, to flight itineraries. We divide our evaluation into two parts. In the first part (Section <a class="sec" href="#sec-19">5.1</a>), we evaluate the extraction rules, while in the second part (Section <a class="sec" href="#sec-23">5.2</a>) we consider our entire extraction process and evaluate its final output. The first part is further partitioned into two sub-parts, one for the evaluation of the basic rules, and the other for the evaluation of the refined rules.</p>    <p>The first evaluation part is based on a manually labeled set, allowing us to evaluate independently each phase of the rules creation. Conversely, the second part evaluates the final output of the end-to-end process, resulting from the consecutive application of all phases described over real Yahoo mail traffic at large scale. We further note that when considering the rules, the evaluation is performed at the XPath level, and when considering the output of the extractions, the evaluation is performed at the message level.</p>    <section id="sec-19">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Evaluation of the Extraction Rules</h3>     </div>    </header>    <section id="sec-20">     <p><em>5.1.1 Dataset.</em> The evaluation of this phase is based on a manually labeled set, referred to as <em>golden set</em>. The golden set was created by selecting 850 x-clusters belonging to 60 different domains that were classified as flights.</p>     <p>For each x-cluster, we presented three mail samples to professional quality assurance staff, referred to as <em>editors</em>, for manual labeling. We note that this process was conducted under full privacy preservation in accordance with Yahoo privacy policy. The manual labeling was performed at the XPath level. For each XPath, editors selected labels from a predefined set of contextual annotations as presented in Table <a class="tbl" href="#tab4">4</a>. Note that an XPath could be associated with more than a single label in case its value contains more than one annotation, e.g., an XPath containing both an airport name and a place. For constant XPaths, no labels were provided.</p>    </section>    <section id="sec-21">     <p><em>5.1.2 Evaluation of the Basic Rules.</em> We built several annotators for identifying light annotations for flight itineraries. Those annotators were computed using the two approaches presented in section&#x00A0;<a class="sec" href="#sec-13">3.3</a>, that is, dictionaries and regular expression patterns. We evaluated the basic (light annotation) rules as follows. For each XPath, we compared the light annotations resulting from the manual labeling, to the light annotation assigned by our automated method. The results are presented in Table&#x00A0;<a class="tbl" href="#tab3">3</a> in terms of precision, recall and <em>F</em>-measure (<em>F</em>      <sub>1</sub>-score).</p>     <p>Looking at the results, one can see that almost all light annotations achieve precision and recall above 90%. The single caveat is the recall of the place annotator. As it turns out, places have high variability both in absolute number and different ways to spell the (essentially) same place. Any dictionary-based annotator has high-dependency on the richness of the dictionary and its coverage of unpopular terms. In accordance, extending the places dictionary that underlies this annotator and adding more advanced canonization techniques are likely to significantly improve this metric.</p>     <div class="table-responsive" id="tab3">      <div class="table-caption">       <span class="table-number">Table 3:</span>       <span class="table-title">Evaluation of light annotations for flights.</span>      </div>      <table class="table">       <tbody>       <tr>        <td style="text-align:left;">         <strong>Annotation</strong>        </td>        <td style="text-align:left;">         <strong>Type</strong>        </td>        <td style="text-align:left;">         <strong>Precision</strong>        </td>        <td style="text-align:left;">         <strong>Recall</strong>        </td>        <td style="text-align:left;">         <strong>F-score</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">confirmation-code</td>        <td style="text-align:left;">Regex</td>        <td style="text-align:left;">100%</td>        <td style="text-align:left;">96%</td>        <td style="text-align:left;">0.98</td>       </tr>       <tr>        <td style="text-align:left;">airport</td>        <td style="text-align:left;">Dict.</td>        <td style="text-align:left;">97%</td>        <td style="text-align:left;">87%</td>        <td style="text-align:left;">0.92</td>       </tr>       <tr>        <td style="text-align:left;">date</td>        <td style="text-align:left;">Regex</td>        <td style="text-align:left;">96%</td>        <td style="text-align:left;">92%</td>        <td style="text-align:left;">0.95</td>       </tr>       <tr>        <td style="text-align:left;">name</td>        <td style="text-align:left;">Dict.</td>        <td style="text-align:left;">92%</td>        <td style="text-align:left;">90%</td>        <td style="text-align:left;">0.91</td>       </tr>       <tr>        <td style="text-align:left;">place</td>        <td style="text-align:left;">Dict.</td>        <td style="text-align:left;">98%</td>        <td style="text-align:left;">77%</td>        <td style="text-align:left;">0.86</td>       </tr>       <tr>        <td style="text-align:left;">time</td>        <td style="text-align:left;">Regex</td>        <td style="text-align:left;">99%</td>        <td style="text-align:left;">94%</td>        <td style="text-align:left;">0.97</td>       </tr>       </tbody>      </table>     </div>    </section>    <section id="sec-22">     <p><em>5.1.3 Evaluation of the Refined Rules.</em> In order to evaluate the rules refinement step, without any dependency on the preceding step of basic rules creation, we trained our model on the light annotations obtained from the manual labeling of the golden set. We trained a model for the classification of each type of light annotation into its optional contextual annotations, as described in Section&#x00A0;<a class="sec" href="#sec-14">3.4</a>. Our labeled set consisted of about 17K anonymized XPaths<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>. We applied cross-validation on the data with 5-folds in order to assess the classifiers performance. The evaluation results are presented in Table&#x00A0;<a class="tbl" href="#tab4">4</a>. As can be seen, the precision and recall of most refined annotations are above 90%.</p>     <div class="table-responsive" id="tab4">      <div class="table-caption">       <span class="table-number">Table 4:</span>       <span class="table-title">Evaluation of contextual annotations for flights.</span>      </div>      <table class="table">       <tbody>       <tr>        <td style="text-align:left;">         <strong>Annotation</strong>        </td>        <td style="text-align:left;">         <strong>Precision</strong>        </td>        <td style="text-align:left;">         <strong>Recall</strong>        </td>        <td style="text-align:left;">         <strong>F-score</strong>        </td>       </tr>       <tr>        <td style="text-align:left;">airport-depart</td>        <td style="text-align:left;">91%</td>        <td style="text-align:left;">96%</td>        <td style="text-align:left;">0.93</td>       </tr>       <tr>        <td style="text-align:left;">airport-arrive</td>        <td style="text-align:left;">95%</td>        <td style="text-align:left;">89%</td>        <td style="text-align:left;">0.92</td>       </tr>       <tr>        <td style="text-align:left;">date-booking</td>        <td style="text-align:left;">100%</td>        <td style="text-align:left;">78%</td>        <td style="text-align:left;">0.87</td>       </tr>       <tr>        <td style="text-align:left;">date-flight</td>        <td style="text-align:left;">90%</td>        <td style="text-align:left;">99%</td>        <td style="text-align:left;">0.94</td>       </tr>       <tr>        <td style="text-align:left;">name-passenger</td>        <td style="text-align:left;">97%</td>        <td style="text-align:left;">99%</td>        <td style="text-align:left;">0.98</td>       </tr>       <tr>        <td style="text-align:left;">name-user</td>        <td style="text-align:left;">96%</td>        <td style="text-align:left;">86%</td>        <td style="text-align:left;">0.91</td>       </tr>       <tr>        <td style="text-align:left;">place-depart</td>        <td style="text-align:left;">94%</td>        <td style="text-align:left;">95%</td>        <td style="text-align:left;">0.94</td>       </tr>       <tr>        <td style="text-align:left;">place-arrive</td>        <td style="text-align:left;">94%</td>        <td style="text-align:left;">93%</td>        <td style="text-align:left;">0.94</td>       </tr>       <tr>        <td style="text-align:left;">time-depart</td>        <td style="text-align:left;">91%</td>        <td style="text-align:left;">94%</td>        <td style="text-align:left;">0.93</td>       </tr>       <tr>        <td style="text-align:left;">time-arrive</td>        <td style="text-align:left;">93%</td>        <td style="text-align:left;">90%</td>        <td style="text-align:left;">0.91</td>       </tr>       </tbody>      </table>     </div>    </section>    </section>    <section id="sec-23">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Evaluation of the Extraction Output</h3>     </div>    </header>    <p>We evaluate the performance of the entire automatic extraction process by comparing its output to the output of the extraction process presented in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>]. This latter process was based on manually-defined rules, and hence, we refer to it as the <em>manual rules</em> process<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>. The process was applied to 10M messages received in December 2016 from 100 flight domains, and associated with 36K x-clusters. The intersection between those domains and the domains of the golden set described in Section <a class="sec" href="#sec-19">5.1</a> leads to 40 domains, allowing the validation of our process on domains that were not necessarily part of the training set.</p>    <p>As a first step, we gathered samples for the 36K x-clusters during a two weeks period in December 2016. Extraction rules were created using our automated process for each of those x-clusters. The rules created by our process were applied to the same traffic as the manual rules process. Note that we ignored x-clusters for which we could not gather enough samples, as defined by the threshold <em>&#x0393;</em> = 50.</p>    <p>The results of the comparison between the output of our automatic process and the output of the manual rules process are presented in Table <a class="tbl" href="#tab5">5</a>. Here, the comparison was performed over the set of extractions per message. An extraction is considered successful if the output strings match between the two processes and get the same annotation. The results are computed per type of annotation, considering both light and contextual annotations. As expected, the precision and recall for the entire process are usually lower compared to its rules creation sub-parts, and mostly range between 80%-90% for both precision and recall. Note that we present light as well as contextual annotations, even if considering only one type of contextual annotation (e.g., date and date-flight, where date-flight comprises a subset of date). This provides further understanding of the process, since for both annotation types, we now evaluate the quality of the actual output, rather than the quality of the rules as presented previously.</p>    <p>While our method could be further enhanced for specific annotations, there are other less obvious reasons for the performance decrease that ought to be discussed. First and foremost, the manual rules process is far from being perfect. It is noisy and not all its extractions are successful, and thus, it only approximates the ground truth and cannot be considered as truly clean. It is important to note that failures in the manual process translate to (false) failures accounted for our automatic process since our process usually does not make the same errors as the manual process. Second, our process was specifically tuned for samples in English, and therefore, had decreased performance metrics for messages in other languages. Lastly, our process was influenced by a skewness of the data that resulted in a decreased performance of the classifier for arrival time.</p>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Evaluation of entire extraction process for flights.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:left;">        <strong>Annotation</strong>       </td>       <td style="text-align:left;">        <strong>Precision</strong>       </td>       <td style="text-align:left;">        <strong>Recall</strong>       </td>       <td style="text-align:left;">        <strong>F-score</strong>       </td>       </tr>       <tr>       <td style="text-align:left;">airport</td>       <td style="text-align:left;">94%</td>       <td style="text-align:left;">94%</td>       <td style="text-align:left;">0.94</td>       </tr>       <tr>       <td style="text-align:left;">airport-depart</td>       <td style="text-align:left;">85%</td>       <td style="text-align:left;">91%</td>       <td style="text-align:left;">0.88</td>       </tr>       <tr>       <td style="text-align:left;">airport-arrive</td>       <td style="text-align:left;">88%</td>       <td style="text-align:left;">89%</td>       <td style="text-align:left;">0.88</td>       </tr>       <tr>       <td style="text-align:left;">confirmation-code</td>       <td style="text-align:left;">88%</td>       <td style="text-align:left;">88%</td>       <td style="text-align:left;">0.88</td>       </tr>       <tr>       <td style="text-align:left;">date</td>       <td style="text-align:left;">82%</td>       <td style="text-align:left;">96%</td>       <td style="text-align:left;">0.88</td>       </tr>       <tr>       <td style="text-align:left;">date-flight</td>       <td style="text-align:left;">82%</td>       <td style="text-align:left;">96%</td>       <td style="text-align:left;">0.88</td>       </tr>       <tr>       <td style="text-align:left;">name</td>       <td style="text-align:left;">90%</td>       <td style="text-align:left;">93%</td>       <td style="text-align:left;">0.91</td>       </tr>       <tr>       <td style="text-align:left;">name-passenger</td>       <td style="text-align:left;">75%</td>       <td style="text-align:left;">78%</td>       <td style="text-align:left;">0.76</td>       </tr>       <tr>       <td style="text-align:left;">place</td>       <td style="text-align:left;">94%</td>       <td style="text-align:left;">92%</td>       <td style="text-align:left;">0.93</td>       </tr>       <tr>       <td style="text-align:left;">place-depart</td>       <td style="text-align:left;">78%</td>       <td style="text-align:left;">84%</td>       <td style="text-align:left;">0.81</td>       </tr>       <tr>       <td style="text-align:left;">place-arrive</td>       <td style="text-align:left;">86%</td>       <td style="text-align:left;">88%</td>       <td style="text-align:left;">0.87</td>       </tr>       <tr>       <td style="text-align:left;">time</td>       <td style="text-align:left;">94%</td>       <td style="text-align:left;">96%</td>       <td style="text-align:left;">0.95</td>       </tr>       <tr>       <td style="text-align:left;">time-depart</td>       <td style="text-align:left;">91%</td>       <td style="text-align:left;">95%</td>       <td style="text-align:left;">0.93</td>       </tr>       <tr>       <td style="text-align:left;">time-arrive</td>       <td style="text-align:left;">73%</td>       <td style="text-align:left;">73%</td>       <td style="text-align:left;">0.73</td>       </tr>      </tbody>     </table>    </div>    </section>    <section id="sec-24">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Trade-off Between Quality and Coverage</h3>     </div>    </header>    <p>We investigate the trade-off between the quality of our process and the traffic coverage. Both measures rely primarily on the threshold <em>&#x0393;</em>, which defines the minimal size of an x-cluster, that is, the minimal number of samples required for creating extraction rules for an x-cluster. This value influences the quality of the light annotations step, as it is based on the horizontal analysis of each x-cluster and XPath, according to their samples. Conversely, the step of contextual annotations is based on learning, where for each annotation, training examples are gathered from our entire set of samples for all x-clusters, and thus does not rely on <em>&#x0393;</em>. As demonstrated in Section <a class="sec" href="#sec-17">4.2</a>, the threshold <em>&#x0393;</em> has also a direct influence on the coverage we can achieve, as extractions are not performed for x-clusters that do not meet the threshold.</p>    <p>Figure&#x00A0;<a class="fig" href="#fig6">6</a> presents the F-score of the light annotation step in our process for different values of <em>&#x0393;</em> when the process is applied to the mail data from the previous section. Recall that this mail corpus consisted of about 10M messages. As can be seen, the higher the value of <em>&#x0393;</em>, the higher the quality of the annotations. This is a natural outcome, as the more samples we have per x-cluster, the more values we have per XPath, and the more precise is our process. For example, annotations relying on dictionaries (airport, name, place) benefit from a large sample since unpopular terms have smaller influence. One can also observe that the improvement in quality becomes smaller when <em>&#x0393;</em> is larger than 20. This demonstrates that our process does not require a high number of samples per x-cluster to achieve high quality. <figure id="fig6">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186582/images/www18companion-136-fig9.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 6:</span>       <span class="figure-title">Extraction quality as a function of <em>&#x0393;</em>.</span>      </div>     </figure>    </p>    </section>   </section>   <section id="sec-25">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> Conclusions</h2>    </div>    </header>    <p>We presented a fully-automated approach for mail extraction of machine generated messages. We leveraged a structural clustering method for messages, and devised a modular automated method for creation of extraction rules at the cluster level. We presented the entire solution along its architecture, performance considerations, and an analysis of the traffic related to its different phases.</p>    <p>Then, we performed thorough offline experiments for evaluating our automated extraction method over real Yahoo mail traffic, considering both the quality of the extraction rules as well as the quality of the entire extraction pipeline. We adapted our solution to the travel domain, and specifically to flight itineraries, and provided clear evaluation. For the rules evaluation, we achieved a performance of above 90% in precision and recall, while for the extraction output evaluation, our numbers mostly range between 80%-90%. We further discussed some data insights and ideas for improvement. We believe that this work is an important step in the field of information extraction in the Web mail domain, and more specifically, for the task of reaching fully-automated mail extractions, adapted to the large scale of this domain.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Charu&#x00A0;C. Aggarwal and Cheng&#x00A0;Xiang Zhai. 2012. <em>      <em>Mining Text Data</em>     </em>. Springer Publishing Company, Incorporated.</li>    <li id="BibPLXBIB0002" label="[2]">Nir Ailon, Zohar&#x00A0;S. Karnin, Edo Liberty, and Yoelle Maarek. 2013. Threading Machine Generated Email. In <em>      <em>WSDM</em>     </em>. 405&#x2013;414.</li>    <li id="BibPLXBIB0003" label="[3]">Arvind Arasu and Hector Garcia-Molina. 2003. Extracting structured data from web pages. In <em>      <em>ACM SIGMOD</em>     </em>. 337&#x2013;348.</li>    <li id="BibPLXBIB0004" label="[4]">Noa Avigdor-Elgrabli, Mark Cwalinski, Dotan&#x00A0;Di Castro, Iftah Gamzu, Irena Grabovitch-Zuyev, Liane Lewin-Eytan, and Yoelle Maarek. 2016. Structural Clustering of Machine-Generated Mail. In <em>      <em>CIKM</em>     </em>. 217&#x2013;226.</li>    <li id="BibPLXBIB0005" label="[5]">Michele Banko, Michael&#x00A0;J. Cafarella, Stephen Soderland, Matt Broadhead, and Oren Etzioni. 2007. Open Information Extraction from the Web. In <em>      <em>20th IJCAI</em>     </em>. 2670&#x2013;2676.</li>    <li id="BibPLXBIB0006" label="[6]">Lorenzo Blanco, Nilesh Dalvi, and Ashwin Machanavajjhala. 2011. Highly efficient algorithms for structural clustering of large websites. In <em>      <em>20th WWW</em>     </em>. 437&#x2013;446.</li>    <li id="BibPLXBIB0007" label="[7]">Philip Bohannon, Nilesh Dalvi, Yuval Filmus, Nori Jacoby, Sathiya Keerthi, and Alok Kirpal. 2012. Automatic Web-scale Information Extraction. In <em>      <em>ACM SIGMOD</em>     </em>. 609&#x2013;612.</li>    <li id="BibPLXBIB0008" label="[8]">Andrei&#x00A0;Z Broder. 1997. On the resemblance and containment of documents. In <em>      <em>Compression and Complexity of Sequences</em>     </em>. 21&#x2013;29.</li>    <li id="BibPLXBIB0009" label="[9]">Andrei&#x00A0;Z. Broder. 2000. Identifying and Filtering Near-Duplicate Documents. In <em>      <em>11th CPM</em>     </em>. 1&#x2013;10.</li>    <li id="BibPLXBIB0010" label="[10]">Andrei&#x00A0;Z Broder, Steven&#x00A0;C Glassman, Mark&#x00A0;S Manasse, and Geoffrey Zweig. 1997. Syntactic clustering of the web. <em>      <em>Computer Networks and ISDN Systems</em>     </em>29, 8 (1997), 1157&#x2013;1166.</li>    <li id="BibPLXBIB0011" label="[11]">Michael&#x00A0;J. Cafarella, Alon Halevy, Daisy&#x00A0;Zhe Wang, Eugene Wu, and Yang Zhang. 2008. WebTables: Exploring the Power of Tables on the Web. <em>      <em>Proc. VLDB Endow.</em>     </em>1, 1 (2008), 538&#x2013;549.</li>    <li id="BibPLXBIB0012" label="[12]">Chia-Hui Chang, Mohammed Kayed, Moheb&#x00A0;Ramzy Girgis, and Khaled&#x00A0;F. Shaalan. 2006. A Survey of Web Information Extraction Systems. <em>      <em>IEEE Trans. on Knowl. and Data Eng.</em>     </em>18, 10 (Oct. 2006), 1411&#x2013;1428.</li>    <li id="BibPLXBIB0013" label="[13]">Chia-Hui Chang and Shao-Chen Lui. 2001. IEPAD: information extraction based on pattern discovery. In <em>      <em>10th WWW</em>     </em>. 681&#x2013;688.</li>    <li id="BibPLXBIB0014" label="[14]">Valter Crescenzi, Paolo Merialdo, and Paolo Missier. 2005. Clustering web pages based on their structure. <em>      <em>Data &#x0026; Knowledge Engineering</em>     </em>54, 3 (2005), 279&#x2013;299.</li>    <li id="BibPLXBIB0015" label="[15]">A. Culotta, R. Bekkerman, and A.McCallum. 2004. Extracting social networks and contact information from email and the Web. In <em>      <em>CEAS</em>     </em>.</li>    <li id="BibPLXBIB0016" label="[16]">Nilesh Dalvi, Philip Bohannon, and Fei Sha. 2009. Robust web extraction: an approach based on a probabilistic tree-edit model. In <em>      <em>ACM SIGMOD</em>     </em>. 335&#x2013;348.</li>    <li id="BibPLXBIB0017" label="[17]">Jeffrey Dean and Sanjay Ghemawat. 2008. MapReduce: simplified data processing on large clusters. <em>      <em>Commun. ACM</em>     </em>51, 1 (2008), 107&#x2013;113.</li>    <li id="BibPLXBIB0018" label="[18]">Dotan Di&#x00A0;Castro, Liane Lewin-Eytan, Yoelle Maarek, Ran Wolff, and Eyal Zohar. 2016. Enforcing k-anonymity in Web Mail Auditing. In <em>      <em>WSDM</em>     </em>.</li>    <li id="BibPLXBIB0019" label="[19]">Hazem Elmeleegy, Jayant Madhavan, and Alon Halevy. 2011. Harvesting Relational Tables from Lists on the Web. <em>      <em>The VLDB Journal</em>     </em>20, 2 (2011), 209&#x2013;226.</li>    <li id="BibPLXBIB0020" label="[20]">Wolfgang Gatterbauer, Paul Bohunsky, Marcus Herzog, Bernhard Kr&#x00FC;pl, and Bernhard Pollak. 2007. Towards Domain-independent Information Extraction from Web Tables. In <em>      <em>16th WWW</em>     </em>. 71&#x2013;80.</li>    <li id="BibPLXBIB0021" label="[21]">Mihajlo Grbovic, Guy Halawi, Zohar Karnin, and Yoelle Maarek. 2014. How many folders do you really need? Classifying email into a handful of categories. In <em>      <em>CIKM</em>     </em>.</li>    <li id="BibPLXBIB0022" label="[22]">Qiang Hao, Rui Cai, Yanwei Pang, and Lei Zhang. 2011. From one tree to a forest: a unified solution for structured web data extraction. In <em>      <em>34th ACM SIGIR</em>     </em>. 775&#x2013;784.</li>    <li id="BibPLXBIB0023" label="[23]">Monika Henzinger. 2006. Finding near-duplicate web pages: a large-scale evaluation of algorithms. In <em>      <em>29th ACM SIGIR</em>     </em>. 284&#x2013;291.</li>    <li id="BibPLXBIB0024" label="[24]">Tin&#x00A0;Kam Ho. 1995. Random decision forests. In <em>      <em>Document Analysis and Recognition, 1995</em>     </em>, Vol.&#x00A0;1. IEEE, 278&#x2013;282.</li>    <li id="BibPLXBIB0025" label="[25]">Mohammed Kayed and Chia-Hui Chang. 2010. FiVaTech: Page-level web data extraction from template pages. <em>      <em>Knowledge and Data Engineering, IEEE Transactions on</em>     </em>22, 2(2010), 249&#x2013;263.</li>    <li id="BibPLXBIB0026" label="[26]">Bing Liu, Robert Grossman, and Yanhong Zhai. 2003. Mining data records in Web pages. In <em>      <em>9th ACM SIGKDD</em>     </em>. 601&#x2013;606.</li>    <li id="BibPLXBIB0027" label="[27]">Wei Liu, Xiaofeng Meng, and Weiyi Meng. 2010. Vide: A vision-based approach for deep web data extraction. <em>      <em>IEEE Transactions on Knowledge and Data Engineering</em>     </em>22, 3(2010), 447&#x2013;460.</li>    <li id="BibPLXBIB0028" label="[28]">Christopher&#x00A0;D Manning, Prabhakar Raghavan, and Hinrich Sch&#x00FC;tze. 2008. <em>      <em>Introduction to information retrieval</em>     </em>. Vol.&#x00A0;1.</li>    <li id="BibPLXBIB0029" label="[29]">Einat Minkov, Richard&#x00A0;C Wang, and William&#x00A0;W Cohen. 2005. Extracting personal names from email: applying named entity recognition to informal text. In <em>      <em>HLT/EMNLP</em>     </em>. 443&#x2013;450.</li>    <li id="BibPLXBIB0030" label="[30]">David Nadeau and Satoshi Sekine. 2007. A survey of named entity recognition and classification. <em>      <em>Lingvisticae Investigationes</em>     </em>30, 1 (2007), 3&#x2013;26.</li>    <li id="BibPLXBIB0031" label="[31]">Andrew Nierman and HV Jagadish. 2002. Evaluating Structural Similarity in XML Documents.. In <em>      <em>WebDB</em>     </em>, Vol.&#x00A0;2. 61&#x2013;66.</li>    <li id="BibPLXBIB0032" label="[32]">Pierre Senellart, Avin Mittal, Daniel Muschick, R&#x00E9;mi Gilleron, and Marc Tommasi. 2008. Automatic Wrapper Induction from Hidden-web Sources with Domain Knowledge. In <em>      <em>10th WIDM</em>     </em>. 9&#x2013;16.</li>    <li id="BibPLXBIB0033" label="[33]">Hassan&#x00A0;A Sleiman and Rafael Corchuelo. 2013. Tex: An efficient and effective unsupervised web information extractor. <em>      <em>Knowledge-Based Systems</em>     </em>39 (2013), 109&#x2013;123.</li>    <li id="BibPLXBIB0034" label="[34]">Hassan&#x00A0;A Sleiman and Rafael Corchuelo. 2014. Trinity: on using trinary trees for unsupervised web data extraction. <em>      <em>IEEE Transactions on Knowledge and Data Engineering</em>     </em>26, 6(2014), 1544&#x2013;1556.</li>    <li id="BibPLXBIB0035" label="[35]">Wachirawut Thamviset and Sartra Wongthanavasu. 2014. Information extraction for deep web using repetitive subject pattern. <em>      <em>World Wide Web</em>     </em> (2014), 1109&#x2013;1139.</li>    <li id="BibPLXBIB0036" label="[36]">Jiying Wang and Fred&#x00A0;H Lochovsky. 2003. Data extraction and label assignment for web databases. In <em>      <em>12th WWW</em>     </em>. 187&#x2013;196.</li>    <li id="BibPLXBIB0037" label="[37]">James&#x00A0;B. Wendt, Michael Bendersky, Lluis Garcia-Pueyo, Vanja Josifovski, Balint Miklos, and Ivo Krka. 2016. Hierarchical Label Propagation and Discovery for Machine Generated Email. In <em>      <em>WSDM</em>     </em>.</li>    <li id="BibPLXBIB0038" label="[38]">Weinan Zhang, Amr Ahmed, Jie Yang, Vanja Josifovski, and Alex&#x00A0;J. Smola. 2015. Annotating Needles in the Haystack Without Looking: Product Information Extraction from Emails. In <em>      <em>21th ACM SIGKDD</em>     </em>. 2257&#x2013;2266.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>Less than 1% of the mail clusters use a valuable schema type. The notion of clusters for machine generated traffic is explained later on.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>Cascading Style Sheets (CSS) is a style sheet language used for describing the presentation of a document written in HTML or any other markup language.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>We emphasize that all the experiments reported here have been conducted on fully anonymized data, in accordance with Yahoo strict privacy policy.</p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>A note is added to avoid confusion between two manually created sets: (1) the golden set used for training the contextual annotation model, as well as for the evaluation in Section <a class="sec" href="#sec-19">5.1</a>; and (2) the manual process presented in [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"    href="#BibPLXBIB0004">4</a>] used for creating extraction rules, against which our automatic end-to-end process is evaluated.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW '18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5640-4/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3184558.3186582">https://doi.org/10.1145/3184558.3186582</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
