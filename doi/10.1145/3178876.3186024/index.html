<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en"> <head>  <title>Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script> </head> <body id="main">  <section class="front-matter">   <section>    <header class="title-info">    <div class="journal-title">     <h1>      <span class="title">Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning</span>      <br/>      <span class="subTitle"/>     </h1>    </div>    </header>    <div class="authorGroup">    <div class="author">     <span class="givenName">Meng</span>     <span class="surName">Qu</span>,     University of Illinois at Urbana-Champaign, IL, USA    </div>    <div class="author">     <span class="givenName">Xiang</span>     <span class="surName">Ren</span>,     University of Southern California, CA, USA    </div>    <div class="author">     <span class="givenName">Yu</span>     <span class="surName">Zhang</span>,     University of Illinois at Urbana-Champaign, IL, USA    </div>    <div class="author">     <span class="givenName">Jiawei</span>     <span class="surName">Han</span>,     University of Illinois at Urbana-Champaign, IL, USA    </div>                    </div>    <br/>    <div class="pubInfo">    <p>DOI: <a href="https://doi.org/10.1145/3178876.3186024" target="_blank">https://doi.org/10.1145/3178876.3186024</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">    <p>     <small>Extracting relations from text corpora is an important task with wide applications. However, it becomes particularly challenging when focusing on weakly-supervised relation extraction, that is, utilizing a few relation instances (i.e., a pair of entities and their relation) as seeds to extract from corpora more instances of the same relation. Existing distributional approaches leverage the corpus-level co-occurrence statistics of entities to predict their relations, and require a large number of labeled instances to learn effective relation classifiers. Alternatively, pattern-based approaches perform boostrapping or apply neural networks to model the local contexts, but still rely on a large number of labeled instances to build reliable models. In this paper, we study the integration of distributional and pattern-based methods in a weakly-supervised setting such that the two kinds of methods can provide complementary supervision for each other to build an effective, unified model. We propose a novel co-training framework with a distributional module and a pattern module. During training, the distributional module helps the pattern module <em>discriminate</em> between the informative patterns and other patterns, and the pattern module <em>generates</em> some highly-confident instances to improve the distributional module. The whole framework can be effectively optimized by iterating between improving the pattern module and updating the distributional module. We conduct experiments on two tasks: knowledge base completion with text corpora and corpus-level relation extraction. Experimental results prove the effectiveness of our framework over many competitive baselines.</small>    </p>    </div>    <div class="classifications">    <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Meng Qu, Xiang Ren, Yu Zhang, and Jiawei Han. 2018. Weakly-supervised Relation Extraction by Pattern-enhanced Embedding Learning. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France</em>. ACM, New York, NY, USA 11 Pages. <a href="https://doi.org/10.1145/3178876.3186024" class="link-inline force-break"       target="_blank">https://doi.org/10.1145/3178876.3186024</a></small>     </p>    </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-5">    <header>    <div class="title-info">     <h2>      <span class="section-number">1</span> INTRODUCTION</h2>    </div>    </header>    <p>Relation extraction is an important task in data mining and natural language processing. Given a text corpus, relation extraction aims at extracting a set of relation instances (i.e., a pair of entities and their relation) based on some given examples. Many efforts&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0007">7</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0022">22</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>] have been done on sentence-level relation extraction, where the goal is to predict the relation for a pair of entities mentioned in a sentence (e.g., predict the relation between &#x201C;<em>Beijing</em>&#x201D; and &#x201C;<em>China</em>&#x201D; in sentence 1 of Fig.&#x00A0;<a class="fig" href="#fig1">1</a>). Despite its wide applications, these studies usually require a large number of human-annotated sentences as training data, which are expensive to obtain. In many cases (e.g., knowledge base completion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>]), it is also desirable to extract a set of relation instances by consolidating evidences from multiple sentences in corpora, which cannot be directly achieved by these studies. Instead of looking at individual sentences, corpus-level relation extraction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0021">21</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>] identifies relation instances from text corpora using evidences from multiple sentences. This also makes it possible to apply weakly-supervised methods based on corpus-level statistics&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. Such weakly-supervised approaches usually take a few relation instances as seeds, and extract more instances by consolidating redundant information collected from large corpora. The extracted instances can serve as extra knowledge in various downstream applications, including knowledge base completion&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>], corpus-level relation extraction&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>], hypernym discovery&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>] and synonym discovery&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>]. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig1.jpg" class="img-responsive" alt="Figure 1"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Illustration of weakly-supervised relation extraction. Given a text corpus and a few relation instances as seeds, the goal is to extract more instances from the corpus.</span>     </div>    </figure>    </p>    <p>In this paper, we focus on corpus-level relation extraction in the <em>weakly-supervised setting</em>. There are broadly two types of weakly-supervised approaches for corpus-level relation extraction. Among them, pattern-based approaches predict the relation of an entity pair from multiple sentences mentioning both entities. To do that, traditional approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] extract textual patterns (e.g., tokens between a pair of entities) and new relation instances in a bootstrapping manner. However, many relations could be expressed in a variety of ways. Due to such diversity, these approaches often have difficulty matching the learned patterns to unseen contexts, leading to the problem of semantic drift&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] and inferior performance. For example, with the given instance &#x201C;<em>(Beijing, Capital of, China)</em>&#x201D; in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>, &#x201C;<em>[Head] , the capital of [Tail]</em>&#x201D; will be extracted as a textual pattern from sentence 1. But we have difficulty in matching the pattern to sentence 2 even though both sentences refer to the same relation &#x201C;<em>Capital of</em>&#x201D;. Recent approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>] try to overcome the sparsity issue of textual patterns by encoding textual patterns with neural networks, so that pattern matching can be replaced by similarity measurement between vector representations. However, these approaches typically rely on large amount of labeled instances to train effective models&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>], making it hard to deal with the weakly-supervised setting.</p>    <p>Alternatively, distributional approaches resort to the corpus-level co-occurrence statistics of entities. The basic idea is to learn low-dimensional representations of entities to preserve such statistics, so that entities with similar semantic meanings tend to have similar representations. With entity representations, a relation classifier can be learned using the labeled relation instances, which takes entity representations as features and predicts the relation of a pair of entities. To learn entity representations, some approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0019">19</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>] only consider the given text corpus. Despite the unsupervised property, their performance is usually limited due to the lack of supervision&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>]. To learn more effective representations for relation extraction, some other approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>] jointly learn entity representations and relation classifiers using the labeled instances. However, similar to pattern-based approaches, distributional approaches also require considerable amount of relation instances to achieve good performance&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>], which are usually hard to obtain in the weakly-supervised setting.</p>    <p>The pattern-based and the distributional approaches extract relations from different perspectives, which are naturally complementary to each other. Ideally, we would wish to integrate both approaches, so that they can mutually enhance and reduce the reliance on the given relation instances. Towards integrating both approaches, several existing studies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>] try to jointly train a distributional model and a pattern model using the labeled instances. However, the supervision of their frameworks still totally comes from the given relation instances, which is insufficient in the weakly-supervised setting. Therefore, their performance is yet far from satisfaction, and we are seeking an approach that is more robust to the scarcity of seed instances.</p>    <p>In this paper, we propose such an approach called REPEL (Relation Extraction with Pattern-enhanced Embedding Learning) to weakly-supervised relation extraction. Our approach consists of a pattern module and a distributional module (see Fig.&#x00A0;<a class="fig" href="#fig2">2</a>). The pattern module aims at learning a set of reliable textual patterns for relation extraction; while the distributional module tries to learn a relation classifier on entity representations for prediction. Different from existing studies, we follow the co-training&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] strategy and encourage both modules to provide extra supervision for each other, which is expected to complement the limited supervision from the given seed instances (see Fig.&#x00A0;<a class="fig" href="#fig3">3</a>). Specifically, the pattern module acts as a <em>generator</em>, as it can extract some candidate instances based on the discovered reliable patterns; whereas the distributional module is treated as a <em>discriminator</em> to evaluate the quality of each generated instance, that is, whether an instance is reasonable. To encourage the collaboration of both modules, we formulate a joint optimization process, in which we iterate between two sub-processes. In the first sub-process, the discriminator (distributional module) will evaluate the instances generated by the generator (pattern module), and the results serve as extra signals to adjust the generator. In the second sub-process, the generator (pattern module) will in turn generate a set of highly confident instances, which serve as extra training seeds to improve the discriminator (distributional module). During training, we keep iterating between the two sub-processes, so that both modules can be consistently improved. Once the training converges, both modules can be applied to relation extraction, which extract new relation instances from different perspectives. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig2.jpg" class="img-responsive" alt="Figure 2"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">Illustration of the modules. The pattern module aims to learn reliable textual patterns for each relation. The distributional module tries to learn entity representations and a score function to estimate the quality of each instance.</span>     </div>    </figure>    <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig3.jpg" class="img-responsive" alt="Figure 3"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">Comparison with existing integration frameworks. Existing frameworks totally rely on the seed instances to provide supervision. Our framework encourages both modules to provide extra supervision for each other.</span>     </div>    </figure>    </p>    <p>In summary, in this paper we make the following contributions:</p>    <ul class="list-no-style">    <li id="list1" label="&#x2022;">We propose a principled framework to integrate the distributional and pattern-based methods for weakly-supervised relation extraction, which is effective in overcoming the scarcity of seeds.<br/></li>    <li id="list2" label="&#x2022;">We develop a joint optimization algorithm for solving the unified objective, alternating between adjusting the pattern module and improving the distributional module.<br/></li>    <li id="list3" label="&#x2022;">We conduct experiments on two downstream applications over two real-world datasets. Experimental results prove the effectiveness of our framework in the weakly-supervised setting.<br/></li>    </ul>   </section>   <section id="sec-6">    <header>    <div class="title-info">     <h2>      <span class="section-number">2</span> PROBLEM DEFINITION</h2>    </div>    </header>    <p>In this section, we formally define our problem.</p>    <p>    <strong>Entity Name.</strong> An <em>entity name</em> is a string referring to a real-world entity, which usually appears in multiple sentences of a corpus. For example in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>, all strings with purple colors (e.g., Beijing, Bill Gates) are valid entity names.</p>    <p>To extract relations between different entities, a prerequisite is to detect those entity names in text corpora. In this paper, for simplicity, we will not focus on entity name detection. Instead, we will use existing tools to do that. Specifically, we first apply some named entity recognition tools&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0018">18</a>] to the corpus, which are able to detect entity names in text. In practice, many detected entity names can refer to the same entity. For example in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>, &#x201C;<em>Microsoft</em>&#x201D; in sentence 3 and &#x201C;<em>MS</em>&#x201D; in sentence 4 both refer to <em>Microsoft Corporation</em>. For entity names representing the same entity, since they have exactly the same meaning, we may expect to treat them equally instead of treating them independently. Therefore, we further leverage some entity linking tools&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>], which can link synonymous entity names to the same entity in an external knowledge (e.g., Freebase). After entity linking, for each entity, we use a unified id to replace all entity names referring to that entity. For example, we can use the Freebase id of the entity <em>Microsoft Corporation</em> to replace &#x201C;<em>Microsoft</em>&#x201D; and &#x201C;<em>MS</em>&#x201D; in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>.</p>    <p>    <strong>Relation Instance.</strong> A <em>relation instance</em> describes the relation between a pair of entities. Formally, a relation instance is composed of an entity pair (<em>e<sub>h</sub>    </em>, <em>e<sub>t</sub>    </em>) and a relation <em>r</em>, meaning that entity <em>e<sub>h</sub>    </em> and entity <em>e<sub>t</sub>    </em> have the relation <em>r</em>.</p>    <p>Relation instances are ubiquitous. For example in Fig.&#x00A0;<a class="fig" href="#fig1">1</a>     <em>(Beijing, China)</em> with <em>capital of</em>, <em>(Bill Gates, Microsoft)</em> with <em>founder of</em> are both valid relation instances. Extracting such instances from text corpora is an essential task, which has wide applications.</p>    <p>    <strong>Problem Definition.</strong> In this paper, we study weakly-supervised relation extraction. Specifically, given a text corpus <em>D</em> and some target relations <em>R</em>, with each target relation <em>r</em> specified by a set of relation instances <span class="inline-equation"><span class="tex">$\lbrace (e_{h_k}, e_{t_k}, r)\rbrace _{k=1}^{N_r}$</span>    </span>, our goal is to leverage the given instances as seeds and extract more instances from the corpus (Fig.&#x00A0;<a class="fig" href="#fig1">1</a>). Formally, we define our problem as follows:</p>    <p>    <div class="definition" id="enc1">     <Label>Definition 2.1.</Label>     <p>      <strong>(Problem Definition)</strong>      <strong>Given</strong>      <em>a text corpus <em>D</em> and some target relations <em>R</em>, where each target relation <em>r</em> is characterized by a few seed instances </em>      <span class="inline-equation"><span class="tex">$\lbrace (e_{h_k}, e_{t_k}, r)\rbrace _{k=1}^{N_r}$</span>      </span>      <em> or in other words a few seed entity pairs </em>      <span class="inline-equation"><span class="tex">$\lbrace (e_{h_k}, e_{t_k}\rbrace _{k=1}^{N_r}$</span>      </span>      <em>, the weakly-supervised relation extraction task</em>      <strong>aims to</strong>      <em>extract more instances </em>      <span class="inline-equation"><span class="tex">$\lbrace (e_{h_i}, e_{t_i}, r_i)\rbrace _{i=1}^{M}$</span>      </span>      <em> from the corpus. In other words, we aim at discovering more entity pairs </em>      <span class="inline-equation"><span class="tex">$\lbrace (e_{h_i}, e_{t_i})\rbrace _{i=1}^{M_r}$</span>      </span>      <em> under each target relation <em>r</em> &#x2208; <em>R</em>.</em>     </p>    </div>    </p>   </section>   <section id="sec-7">    <header>    <div class="title-info">     <h2>      <span class="section-number">3</span> THE REPEL FRAMEWORK</h2>    </div>    </header>    <section id="sec-8">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Framework Overview</h3>     </div>    </header>    <p>In this section, we introduce our approach to weakly-supervised relation extraction. The major challenge comes from the deficiency of supervision, since we only have a few relation instances as seeds. Therefore, the performance of existing approaches, including the pattern-based&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0044">44</a>] and the distributional approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0039">39</a>], is not satisfactory. Although some studies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] trying to reduce the reliance on seeds by integrating both approaches, they simply employ a joint training framework, which still requires considerable relation instances to train effective models.</p>    <p>To better overcome the challenge of seed scarcity, in this paper we propose a framework called REPEL based on the co-training strategy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>]. Our framework consists of two modules, a pattern module and a distributional module (see Fig.&#x00A0;<a class="fig" href="#fig2">2</a>), which extract relations from different perspectives. The pattern module aims at finding a set of reliable textual patterns for relation extraction. Meanwhile, the distributional module tries to learn entity representations and train a score function, which measures the quality of a relation instance. Different from existing studies, both modules are encouraged to provide extra supervision to each other, which is expected to complement the limited supervision from seed instances (see Fig.&#x00A0;<a class="fig" href="#fig3">3</a>). Specifically, the pattern module is treated as a generator since it can extract some candidate relation instances, and meanwhile the distributional module acts as a discriminator to evaluate each instance. During training, the discriminator evaluates the instances generated by the generator, and the results serve as extra signals to adjust the generator. On the other hand, the generator will in turn generate some highly confident instances, which act as extra seeds to improve the discriminator. We keep iterating between adjusting the pattern module and improving the distributional module. Once the training process converges, both modules can be utilized to discover more instances.</p>    <p>The overall objective is summarized below: <div class="table-responsive" id="eq1">      <div class="display-equation">       <span class="tex mytex">\begin{equation} \max _{P,D} O = \max _{P,D} \lbrace O_p + O_d + \lambda O_i \rbrace . \end{equation} </span>       <br/>       <span class="equation-number">(1)</span>      </div>     </div> In the objective, <em>P</em> represents the parameters of the pattern module, that is, a given number of reliable patterns for each target relation. <em>D</em> denotes the parameters of the distributional module, that is, entity representations and a score function. The objective function consists of three terms. Among them, <em>O<sub>p</sub>     </em> is the objective of the pattern module, in which we leverage the given seed instances for pattern selection. <em>O<sub>d</sub>     </em> is the objective of the distributional module, which learns relevant parameters under the guidance of seed instances. Finally, <em>O<sub>i</sub>     </em> models the interactions of both modules.</p>    <p>Next, we introduce the model details. <strong>Note that</strong> for simplicity, we only consider one relation when introducing the model. To deal with multiple relations, we can simply combine their objectives.</p>    </section>    <section id="sec-9">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Pattern Module</h3>     </div>    </header>    <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig4.jpg" class="img-responsive" alt="Figure 4"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">Illustration of the pattern module. We extract patterns by considering the dependency parsing trees. The pattern reliability is calculated based on the seed entity pairs.</span>     </div>    </figure>    <p>In the pattern module, our goal is to select a given number of the most reliable patterns <em>P</em> for the target relation, and further leverage them to discover more relation instances from corpora.</p>    <p>Following previous studies on pattern based approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>], we define a pattern as the tokens in the shortest dependency path between a pair of entities in a sentence. Fig.&#x00A0;<a class="fig" href="#fig4">4</a> presents an example, in which the pattern of the sentence is <em>[Head] city [Tail]</em>. Given this definition, we can go back to the corpus and extract a pattern for every pair of entities in a sentence, forming a set of candidate patterns and many entity pairs linked to each pattern.</p>    <p>Among all the candidate patterns, we hope to extract the most reliable ones for the target relation. Towards this goal, we leverage the seed relation instances as guidance, and estimate the reliability of a pattern <em>&#x03C0;</em> with the following measurement <em>R</em>(<em>&#x03C0;</em>): <div class="table-responsive" id="eq2">      <div class="display-equation">       <span class="tex mytex">\begin{equation} R(\pi) = \frac{|G(\pi) \cap S_{pair}|}{|G(\pi)|}, \end{equation} </span>       <br/>       <span class="equation-number">(2)</span>      </div>     </div> where <em>G</em>(<em>&#x03C0;</em>) represents all the entity pairs extracted by the pattern <em>&#x03C0;</em>, and <em>S<sub>pair</sub>     </em> is the set of seed entity pairs under the target relation. The numerator of <em>R</em>(<em>&#x03C0;</em>) is the number of seed entity pairs which can be discovered by the pattern <em>&#x03C0;</em>, and the denominator counts all extracted entity pairs. For example in the right part of Fig.&#x00A0;<a class="fig" href="#fig4">4</a>, we focus on the relation <em>capital of</em>, and the pattern <em>[Head] city [Tail]</em> extracts two entity pairs. Among them, the red pair is in the seed set, and therefore the reliability is 1/2. Such definition of <em>R</em>(<em>&#x03C0;</em>) is quite intuitive. Basically, if a pattern can extract many seed entity pairs under the target relation, then it will be considered reliable.</p>    <p>Based on the measurement, we try to select the top-<em>K</em> most reliable patterns, in which <em>K</em> is a given number. Such goal can be achieved by optimizing the following objective function with respect to <em>P</em>: <div class="table-responsive" id="eq3">      <div class="display-equation">       <span class="tex mytex">\begin{equation} O_p = \sum _{\pi \in P} R(\pi), \end{equation} </span>       <br/>       <span class="equation-number">(3)</span>      </div>     </div> where <em>P</em> is the pattern set with size <em>K</em>.</p>    <p>Once the most reliable patterns <em>P</em> are learned for the target relation, we can leverage them to extract new entity pairs under the target relation. Formally, we denote the set of entity pairs extracted by the pattern set <em>P</em> as <em>G</em>(<em>P</em>), which is calculated as follows: <div class="table-responsive" id="eq4">      <div class="display-equation">       <span class="tex mytex">\begin{equation} G(P) = \cup _{\pi \in P} G(\pi) , \end{equation} </span>       <br/>       <span class="equation-number">(4)</span>      </div>     </div> where <em>G</em>(<em>&#x03C0;</em>) is the set of entity pairs extracted by pattern <em>&#x03C0;</em>.</p>    </section>    <section id="sec-10">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Distributional Module</h3>     </div>    </header>    <p>The distributional module of our approach focuses on the global distributional information of entities. Specifically, it aims at learning distributed entity representations from corpora, so that similar entities are likely to have similar representations. Meanwhile, we utilize the given relation instances as seeds to train a score function, which takes entity representations as features to estimate whether a relation instance is reasonable.</p>    <p>To learn entity representations from text corpora, we follow&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>] and build a bipartite network between all the entities and words. The weight between an entity and a word is defined as the number of sentences in which they co-occur. Then for an entity <em>e</em> and a word <em>w</em>, we infer the conditional probability <em>P</em>(<em>w</em>|<em>e</em>) as follows: <div class="table-responsive" id="eq5">      <div class="display-equation">       <span class="tex mytex">\begin{equation} P(w|e)=\frac{\exp (\mathbf {x}_e \cdot \mathbf {c}_w)}{Z}, \end{equation} </span>       <br/>       <span class="equation-number">(5)</span>      </div>     </div> where <strong>x</strong>     <sub>      <em>e</em>     </sub> is the vector representation of entity <em>e</em>, <strong>c</strong>     <sub>      <em>w</em>     </sub> is the embedding vector of word <em>w</em> and <em>Z</em> is a normalization term.</p>    <p>Given the estimated conditional probability <em>p</em>(&#x00B7; |<em>e</em>), we try to minimize its KL divergence from the empirical distribution <em>p</em>&#x2032;(&#x00B7; |<em>e</em>) for every entity <em>e</em>, so that the distributional information can be preserved into the learned entity representations. Specifically, the empirical distribution is defined as <em>p</em>&#x2032;(<em>w</em>|<em>e</em>)&#x221D;<em>n</em>     <sub>      <em>w</em>, <em>e</em>     </sub>, where <em>n</em>     <sub>      <em>w</em>, <em>e</em>     </sub> is the weight of the edge between word <em>w</em> and entity <em>e</em>. After some simplification, we obtain the following objective function: <div class="table-responsive" id="eq6">      <div class="display-equation">       <span class="tex mytex">\begin{equation} O_{text}=\sum _{w,e}n_{w,e}\log P(w|e), \end{equation} </span>       <br/>       <span class="equation-number">(6)</span>      </div>     </div> The above objective function can be efficiently optimized with the negative sampling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0020">20</a>] and edge sampling&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>] techniques. In each epoch, a positive edge and several negative edges are sampled for optimization. For details, readers may refer to&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0033">33</a>].</p>    <p>Meanwhile, we also leverage the given seed instances to learn a score function, which estimates the quality of a instance, that is, how likely an entity pair has the target relation. Following the previous work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0004">4</a>], for an entity pair <em>f</em> = (<em>e<sub>h</sub>     </em>, <em>e<sub>t</sub>     </em>), its score under the target relation is defined as follows: <div class="table-responsive" id="eq7">      <div class="display-equation">       <span class="tex mytex">\begin{equation} L_D(f|r)=-|| x_{e_h} + y_r - x_{e_t} ||_2^2, \end{equation} </span>       <br/>       <span class="equation-number">(7)</span>      </div>     </div> where || &#x00B7; ||<sub>2</sub> is the Euclidean norm of a vector, <em>x<sub>e</sub>     </em> is the representation of entity <em>e</em>, <em>r</em> is the target relation and <em>y<sub>r</sub>     </em> is a parameter vector for the target relation.</p>    <p>Intuitively, we expect a seed entity pair could have larger scores than some randomly sampled pairs under the target relation. Therefore, we adopt the following ranking based objective for training: <div class="table-responsive" id="eq8">      <div class="display-equation">       <span class="tex mytex">\begin{equation} O_{seed} = \sum _{f \in S_{pair}} \sum _{f^{\prime }=(e^{\prime }_h,e^{\prime }_t)}\min \lbrace 1,L_D(f|r) - L_D(f^{\prime }|r)\rbrace . \end{equation} </span>       <br/>       <span class="equation-number">(8)</span>      </div>     </div>     <em>S<sub>pair</sub>     </em> is all seed pairs, <span class="inline-equation"><span class="tex">$e^{\prime }_h$</span>     </span> and <span class="inline-equation"><span class="tex">$e^{\prime }_t$</span>     </span> are randomly sampled entities.</p>    <p>Finally, we integrate Eqn.&#x00A0;<a class="eqn" href="#eq6">6</a> and Eqn.&#x00A0;<a class="eqn" href="#eq8">8</a> as the objective of the distributional module, and we try to optimize it with respect to <em>D</em>. <div class="table-responsive" id="eq9">      <div class="display-equation">       <span class="tex mytex">\begin{equation} O_{d} = O_{text} + \eta O_{seed}, \end{equation} </span>       <br/>       <span class="equation-number">(9)</span>      </div>     </div> where <em>&#x03B7;</em> is used to control the weights of the two parts, <em>D</em> represents all parameters of the distributional module, including entity representations {<em>x<sub>e</sub>     </em>} and the parameter vector <em>y<sub>r</sub>     </em> of the relation.</p>    <p>Once the representations are learned, we can use the score function <em>L<sub>D</sub>     </em> to measure the score of each entity pair under the target relation, and thus discover some highly confident relation instances.</p>    </section>    <section id="sec-11">    <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Modeling the Module Interaction</h3>     </div>    </header>    <p>So far, the supervision of both modules totally comes from the given relation instances, which is insufficient in the weakly-supervised setting. To solve this problem, we follow the co-training strategy&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>], and encourage both modules to provide extra supervision for each other.</p>    <p>Specifically, we introduce the following objective function, and try to maximize it with respect to both of <em>P</em> and <em>D</em>: <div class="table-responsive" id="eq10">      <div class="display-equation">       <span class="tex mytex">\begin{equation} O_i = E_{f \in G(P)}[L_D(f|r)], \end{equation} </span>       <br/>       <span class="equation-number">(10)</span>      </div>     </div> where <em>f</em> &#x2208; <em>G</em>(<em>P</em>) is an entity pair extracted by the reliable pattern set <em>P</em> with <em>G</em>(<em>P</em>) defined in Eqn.&#x00A0;<a class="eqn" href="#eq4">4</a>, <em>L<sub>D</sub>     </em>(<em>f</em>|<em>r</em>) is the score of pair <em>f</em> under the target relation. From the objective function, we see that the selected patterns <em>P</em> acts as a generator, since it generates some candidate entity pairs under the target relation; whereas the distributional module serves as a discriminator, trying to score the generated entity pairs under the target relation. The goal of the objective function is to encourage the agreement of the pattern module and the distributional module. More specifically, we hope that the entity pairs generated by the pattern module can be considered reasonable by the distributional module. The intuition underlying the objective comes from the co-training algorithms&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0003">3</a>], where it has been proved that the error rate of two predictive models can be decreased by minimizing their disagreement&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>].</p>    <p>To intuitively understand how this objective function will improve both modules, let us consider how to optimize with respect to both modules. For the pattern module, to maximize the above objective, the pattern set <em>P</em> should include patterns which are considered reliable by the distributional module. That is, the entity pairs generated by those patterns should obtain large scores from the distributional score function <em>L<sub>D</sub>     </em>. In this way, the distributional module provides extra supervision to estimate the pattern reliability. Meanwhile, for the distributional module, to maximize the objective function, it should assign larger scores to the entity pairs generated by the pattern module. Therefore, the highly confident entity pairs generated by the pattern module serve as extra seeds to help improve the distributional module.</p>    <p>With the above objective function, both modules can tightly interact with each other, and provide extra supervision to overcome the challenge of seed scarcity.</p>    </section>   </section>   <section id="sec-12">    <header>    <div class="title-info">     <h2>      <span class="section-number">4</span> THE JOINT OPTIMIZATION PROBLEM</h2>    </div>    </header>    <p>To optimize the overall objective function (Eqn.&#x00A0;<a class="eqn" href="#eq1">1</a>), we leverage the coordinate gradient descent algorithm&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>], by iterating between two sub-processes. In the first sub-process, we fix the pattern module, and update the distributional module under the guidance of the given seeds and the highly confident instances generated by the pattern module. In the second sub-process, the distributional module is fixed, and we update the selected patterns with the given seed instances and the supervision provided by the distributional module. During training, we keep iterating between the two sub-processes, so that both modules can be consistently improved.</p>    <p>    <strong>1. Optimizing the Distributional Module.</strong> In this step, we fix the selected pattern set <em>P</em> to update the parameters <em>D</em> of the distributional module. Formally, maximizing the objective function with respect to <em>D</em> can be transformed as the following problem: <div class="table-responsive" id="eq11">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \max _{D} \lbrace O_d+\lambda O_i\rbrace = \max _{D}\lbrace O_d + \lambda E_{f \in G(P)}[L_D(f|r)]\rbrace , \end{equation} </span>      <br/>      <span class="equation-number">(11)</span>     </div>    </div> which is a continuous optimization problem. We use the stochastic gradient descent algorithm for optimization. On the one hand, we adjust all parameters <em>D</em> to maximize the <em>O<sub>d</sub>    </em> part. On the other hand, some entity pairs <em>f</em> will be sampled based on the selected patterns <em>P</em>, which are treated as extra instances to update <em>D</em>.</p>    <p>    <strong>2. Optimizing the Pattern Module.</strong> In this this, we fix the parameters <em>D</em> of the distributional module and adjust the reliable pattern set <em>P</em>. Formally, maximizing the objective function with respect to <em>P</em> is equivalent to the following optimization problem: <div class="table-responsive" id="eq12">     <div class="display-equation">      <span class="tex mytex">\begin{equation} \max _{P} \lbrace O_p+\lambda O_i\rbrace =\max _{P}\lbrace \sum _{\pi \in P} \left(R(\pi) + \lambda E_{f \in G(\pi)}[L_D(f|r)] \right) \rbrace , \end{equation} </span>      <br/>      <span class="equation-number">(12)</span>     </div>    </div> which is a discrete optimization problem, with the goal as selecting a given number of patterns <em>P</em> with the largest reliability. The reliability of a pattern <em>&#x03C0;</em> is calculated from two sources: <em>O<sub>p</sub>    </em> and <em>O<sub>i</sub>    </em>. In the <em>O<sub>p</sub>    </em> part, the reliability is measured with <em>R</em>(<em>&#x03C0;</em>) defined in Eqn.&#x00A0;<a class="eqn" href="#eq2">2</a>, which leverages the given seeds for reliability estimation. In the <em>O<sub>i</sub>    </em> part, we utilize the score function <em>L<sub>D</sub>    </em> to score each entity pair <em>f</em> extracted by pattern <em>&#x03C0;</em>, and further average them to obtain another reliability estimation <em>E</em>    <sub>     <em>f</em> &#x2208; <em>G</em>(<em>&#x03C0;</em>)</sub>[<em>L<sub>D</sub>    </em>(<em>f</em>|<em>r</em>)]. Finally, the two estimations are weighted as the overall reliability. In practice, we can first calculate the overall reliability of each pattern, and then select the top-<em>K</em> patterns to form the reliable pattern set <em>P</em>.</p>    <p>Finally, we summarize the optimization algorithm into Alg.&#x00A0;1. Once the training converges, our approach will return a set of discovered reliable patterns from the pattern module and a distributional score function from the distributional module. Both the learned patterns and score function can be leveraged for relation extraction, which extract new instances from different perspectives. Specifically, the learned reliable patterns extract relations from local contexts by matching the contexts with the patterns, which usually have high precision but low recall. This is because for a pair of entities, the local contexts mentioning both entities are usually more reliable for predicting their relations, leading to high precision. However, for many pairs of entities, they may never co-occur in any local contexts, and thus using local contexts can result in low recall. In practice, the learned reliable patterns can be applied to applications such as corpus-level relation extraction (see the details in Sec.&#x00A0;<a class="sec" href="#sec-14">5.1</a>.3 (2)). On the other hand, the learned distributional score function predicts entity relation from corpus-level statistics, leading to relatively low precision but high recall, and is more suitable for tasks like knowledge base completion with text corpora (see the details in Sec.&#x00A0;<a class="sec" href="#sec-14">5.1</a>.3 (1)).</p>    <p>    <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-img1.svg" class="img-responsive" alt="" longdesc=""/>    </p>   </section>   <section id="sec-13">    <header>    <div class="title-info">     <h2>      <span class="section-number">5</span> EXPERIMENT</h2>    </div>    </header>    <p>In this section, we evaluate our approach on two downstream applications: knowledge base completion with text corpora (KBC) and corpus-level relation extraction (RE).</p>    <p>In knowledge base completion with text corpora, the key task is to predict the missing relationships between each pair of entities in knowledge bases. Since some pairs of entities may not co-occur in any sentences in the given corpus, the learned pattern module can not provide information for predicting their relations. Therefore, for KBC we only use the entity representations and score function learned by the distributional module for extraction, and we expect to show that the pattern module can provide extra seeds during training, yielding a more effective distributional module. For corpus-level RE, it aims at predicting the relation of a pair of entities from several sentences mentioning both of them. In this case, the reliable patterns learned by the pattern module can capture the local context information from the sentences. Therefore, we focus on utilizing the learned pattern module for prediction in RE, and we expect to show that the distributional module can enhance the pattern module by providing extra supervision to select reliable patterns.</p>    <section id="sec-14">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> Experiment Setup</h3>     </div>    </header>    <p>     <strong>1. Datasets.</strong> In experiment, we leverage existing NER tool&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>] for entity detection. Since the NER tool can only detect entities of several major types such as location, person and organization, we thus sample 10 common relations&#x00A0;<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a>related to person, location and organization from Freebase&#x00A0;<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a> as our target relations. Then two datasets are constructed based on the selected relations.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">      <span class="table-number">Table 1:</span>      <span class="table-title">Statistics of the Datasets.</span>     </div>     <table class="table">     <thead>       <tr>       <th style="text-align:center;">        <strong>Dataset</strong>       </th>       <th style="text-align:center;">        <strong>Wiki + Freebase</strong>       </th>       <th style="text-align:center;">        <strong>NYT + Freebase</strong>       </th>       </tr>     </thead>      <tbody>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Documents</td>       <td style="text-align:center;">150,000</td>       <td style="text-align:center;">118,664</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Entities</td>       <td style="text-align:center;">92,443</td>       <td style="text-align:center;">23,120</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Candidate Patterns</td>       <td style="text-align:center;">621,782</td>       <td style="text-align:center;">232,892</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Seed Instances per Relation</td>       <td style="text-align:center;">50</td>       <td style="text-align:center;">50</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Relations in KBC</td>       <td style="text-align:center;">10</td>       <td style="text-align:center;">10</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Test Instances in KBC</td>       <td style="text-align:center;">10,734</td>       <td style="text-align:center;">6,094</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Relations in RE</td>       <td style="text-align:center;">5</td>       <td style="text-align:center;">6</td>       </tr>       <tr>       <td style="text-align:center;">        <span class="inline-equation"><span class="tex">$\#$</span>        </span> Test Entity Pairs in RE</td>       <td style="text-align:center;">131</td>       <td style="text-align:center;">222</td>       </tr>      </tbody>     </table>    </div>    <p>(1) <strong>Wiki</strong>: The first 150K articles in Wikipedia&#x00A0;<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a> are used as the corpus. For each target relation, we randomly sample 50 relation instances from Freebase as seeds. In the knowledge base completion task, we select all the above 10 relations as the target relations, and we sample 10,734 extra instances from Freebase for prediction. In the corpus-level relation extraction task, the manually annotated sentences from&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0010">10</a>] are used for evaluation. Among all relations in the annotated sentences, 5 relations&#x00A0;<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a> can be mapped to our selected 10 Freebase relations, and thus we only focus on these 5 relations. There are totally 194 manually annotated sentences and 131 entity pairs related to the relations.</p>    <p>(2) <strong>NYT</strong>: The 118,664 documents from 2013 New York Times news articles. Similar to the Wiki dataset, for each target relation we randomly sample 50 relation instances from Freebase as seeds. In the knowledge base completion task, we select all the above 10 relations as the target relations, and totally 6,094 extra instances are sampled for evaluation. In the corpus-level relation extraction task, we leverage the manually annotated sentences from&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0011">11</a>] for evaluation. Among all relations in the sentences, 6 relations&#x00A0;<a class="fn" href="#fn5" id="foot-fn5"><sup>5</sup></a> can be mapped to the selected 10 Freebase relations, so we focus on these 6 relations. There are totally 322 manually annotated sentences and 222 entity pairs related to the relations.</p>    <p>For each text corpus, we adopt Stanford CoreNLP package&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>]<a class="fn" href="#fn6" id="foot-fn6"><sup>6</sup></a> to do preprocessing. Then we leverage DBpedia Spotlight&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0009">9</a>]<a class="fn" href="#fn7" id="foot-fn7"><sup>7</sup></a> to link the detected entity names to the Freebase.</p>    <p>     <strong>2. Compared Algorithms.</strong> In the knowledge base completion task, we select the following baseline algorithms to compare:</p>    <p>(1) <strong>word2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0020">20</a>]</strong>: A distributional approach for word embedding learning, which can learn entity representations from text corpora. Once the representations are learned, we utilize the seed instances to train a relation classifier (Eqn.&#x00A0;<a class="eqn" href="#eq7">7</a>) for extraction. (2) <strong>TransE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0004">4</a>]</strong>: A distributional approach for knowledge base completion, which only uses the given seed instances for training. (3) <strong>RK&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0036">36</a>]</strong>: A distributional approach for knowledge base completion, which leverages both the text corpus and the given relation instances to learn entity representations. (4) <strong>DPE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0025">25</a>]</strong>: An approach that integrates the distributional and pattern-based methods. It jointly models the distributional information in text corpora, the given relation instances and the textual patterns. (5) <strong>CONV&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0034">34</a>]</strong>: A knowledge base completion approach, which integrates the distributional and pattern-based methods by jointly optimizing the given seed instances and the instances extracted by textual patterns.</p>    <p>In the corpus-level relation extraction task, the following approaches are selected to compare:</p>    <p>(1) <strong>SnowBall&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0001">1</a>]</strong>: A pattern approach for relation extraction, which discovers reliable patterns with the seed instances in a bootstrapping way. (2) <strong>CNN-ATT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>]</strong>: A pattern approach for corpus-level relation extraction. It leverages convolutional neural networks to encode and classify each sentence, and then consolidates the results of different sentences using an attention mechanism. (3) <strong>PCNN-ATT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0016">16</a>]</strong>: A pattern approach for corpus-level relation extraction. Compared with CNN-ATT, it also involves the position embedding for each word and entity. (4) <strong>PathCNN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0045">45</a>]</strong>: A pattern approach for corpus-level relation extraction. For each entity pair, besides sentences mentioning both entities, it also considers some other sentences mentioning only one of them. (5) <strong>LexNET&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"       href="#BibPLXBIB0030">30</a>]</strong>: An approach combining the distributional and pattern-based methods for relation extraction. Formally, it uses a recurrent layer to encode local textual patterns, and then uses the encoding vector together with entity representations for prediction.</p>    <p>For our proposed approach, we consider the following variants:</p>    <p>(1) <strong>REPEL-P</strong>: A variant of our approach with only the pattern module (<em>O<sub>p</sub>     </em>). (2) <strong>REPEL-D</strong>: A variant of our approach with only the distributional module (<em>O<sub>d</sub>     </em>). (3) <strong>REPEL</strong>: Our proposed approach, which encourages the collaboration of both modules during training. Once the training converges, we leverage the entity representations and score function learned by the distributional module for the KBC task; whereas the reliable patterns discovered by the pattern module are used for the RE task.</p>    <p>     <strong>3. Evaluation Setup.</strong> (1) <strong>Knowledge Base Completion:</strong> For each compared algorithm, we first learn entity representations and relation classifiers (or score function for our approach) by using the given text corpus and relation instances. Then the learned representations and classifiers are leveraged for evaluation. Specifically, for each test instance (<em>e<sub>h</sub>     </em>, <em>e<sub>t</sub>     </em>, <em>r</em>), we remove its head entity or tail entity, obtaining two incomplete instances, including (<em>e<sub>h</sub>     </em>, ?, <em>r</em>) and (?, <em>e<sub>t</sub>     </em>, <em>r</em>), and our goal is to select the correct entity from the entity set to fill the incomplete instances. To do that, for each candidate entity in the entity set, we calculate its score by measuring the quality of the formed instance using the relation classifier. Then we sort different entities in the descending order based on their scores, and calculate the rank of the correct entity. Finally, we report the mean value of those ranks (i.e., MR) and also the proportion of the correct entities ranked within top 10 (i.e., Hits@10).</p>    <p>(2) <strong>Corpus-level Relation Extraction:</strong> For each compared algorithm, we first use it to predict the relation expressed in each test sentence. Specifically, for neural network based approaches (PathCNN, CNN-ATT, PCNN-ATT, LexNET), the test sentences can be directly classified based on the learned neural classifiers. For approaches based on textual patterns (Snowball, REPEL, REPEL-P), we first match the local context of the test sentence to a discovered reliable pattern <em>&#x03C0;</em>     <sup>*</sup>, then we classify the sentence based on the relation expressed by pattern <em>&#x03C0;</em>     <sup>*</sup>. To do such matching, we represent each learned reliable pattern and the local patterns of the test sentences with a low-dimensional vector. The pattern vector is calculated by averaging the embeddings of tokens in each pattern, with the token embeddings learned by our approach in Eqn.&#x00A0;<a class="eqn" href="#eq5">5</a>. Once the pattern vectors are obtained, each local pattern in test sentences is matched to its most similar reliable pattern, in which the similarity is measured as the cosine similarity between the pattern vectors. After all test sentences are classified, for each test entity pair, we consolidate the prediction results from the test sentences mentioning both entities, and return the predicted relation together with the confidence score. During consolidate, we either average the prediction results of all test sentences (LexNET, PathCNN, Snowball, REPEL, REPEL-P), or leverage the learned attention mechanism (CNN-ATT, PCNN-ATT). Finally, we sort all test entity pairs in the descending order based on the calculated confidence scores, and compare the ranked list with the ground-truth. Based on the results, we report both the precision at position K (i.e., P@K), recall at position K (i.e., R@K) and the precision-recall curve.</p>    <p>     <strong>4. Parameter Settings.</strong> For all knowledge base completion methods and the distributional module of our approach, we set the dimension of all representations as 100. The number of iterations for TransE, word2vec, RK, DPE are set as 1000, 20, 20, 3B respectively to ensure the convergence. Other parameters are set as the default values suggested in the original papers. For the neural based approaches to corpus-level relation extraction, the dimension of the embedding layer and the hidden layer is set as 100. Other parameters are set as the default values suggested in the original papers. For our proposed approach, the parameter <em>&#x03BB;</em> for controlling the weight of the interaction term is set as 1 by default. For the distributional module, the learning rate is set as 0.01, the parameter <em>&#x03B7;</em> is set as 0.005, the number of training edges in each iteration is set as 3B. For the pattern module, we set the number of reliable patterns <em>K</em> for each relation as 100.</p>    </section>    <section id="sec-15">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Performance Comparison</h3>     </div>    </header>    <p>     <strong>1. Knowledge Base Completion with Text Corpora (KBC).</strong> We present the quantitative results in Table&#x00A0;<a class="tbl" href="#tab2">2</a>, and the hits curve in Fig.&#x00A0;<a class="fig" href="#fig5">5</a>. For the approach only considering the given seed instances (TransE), we see the performance is very limited due to the scarcity of seeds. Along the other line, the approach considering text corpora (word2vec) achieves relatively better results, but are still far from satisfactory, since it ignores the supervision from the seed instances. If we consider both the text corpus and seed instances for entity representation learning (RK), we obtain much better results. Moreover, by further jointly training a pattern model (DPE, CONV), the hits ratio can be further significantly improved.</p>    <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Quantitative results on the KBC task.</span>     </div>     <table class="table">      <thead>       <tr>       <td valign="middle" rowspan="2" style="text-align:center;">        <strong>Algorithm</strong>       </td>       <td colspan="2" style="text-align:center;">        <strong>Wiki + Freebase</strong>        <hr/>       </td>       <td colspan="2" style="text-align:center;">        <strong>NYT + Freebase</strong>        <hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">        <strong>Hits@10</strong>       </td>       <td style="text-align:center;">        <strong>MR</strong>       </td>       <td style="text-align:center;">        <strong>Hits@10</strong>       </td>       <td style="text-align:center;">        <strong>MR</strong>       </td>       </tr>      </thead>      <tbody>       <tr>       <td style="text-align:center;">TransE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0004">4</a>]</td>       <td style="text-align:center;">7.13</td>       <td style="text-align:center;">4328.40</td>       <td style="text-align:center;">15.94</td>       <td style="text-align:center;">3833.47</td>       </tr>       <tr>       <td style="text-align:center;">word2vec&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0020">20</a>]</td>       <td style="text-align:center;">32.12</td>       <td style="text-align:center;">203.53</td>       <td style="text-align:center;">15.56</td>       <td style="text-align:center;">913.04</td>       </tr>       <tr>       <td style="text-align:center;">RK&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0036">36</a>]</td>       <td style="text-align:center;">41.49</td>       <td style="text-align:center;">72.87</td>       <td style="text-align:center;">29.01</td>       <td style="text-align:center;">307.89</td>       </tr>       <tr>       <td style="text-align:center;">DPE&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0025">25</a>]</td>       <td style="text-align:center;">45.45</td>       <td style="text-align:center;">78.87</td>       <td style="text-align:center;">32.47</td>       <td style="text-align:center;">279.99</td>       </tr>       <tr>       <td style="text-align:center;">CONV&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0034">34</a>]</td>       <td style="text-align:center;">46.84</td>       <td style="text-align:center;">139.81</td>       <td style="text-align:center;">31.51</td>       <td style="text-align:center;">903.38</td>       </tr>       <tr>       <td style="text-align:center;">REPEL-D</td>       <td style="text-align:center;">47.49</td>       <td style="text-align:center;">67.28</td>       <td style="text-align:center;">35.79</td>       <td style="text-align:center;">234.23</td>       </tr>       <tr>       <td style="text-align:center;">REPEL</td>       <td style="text-align:center;">        <strong>51.18</strong>       </td>       <td style="text-align:center;">        <strong>62.18</strong>       </td>       <td style="text-align:center;">        <strong>38.98</strong>       </td>       <td style="text-align:center;">        <strong>199.44</strong>       </td>       </tr>      </tbody>     </table>    </div>    <figure id="fig5">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig5.jpg" class="img-responsive" alt="Figure 5"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Hits curves on the KBC task.</span>     </div>    </figure>    <p>For our proposed approach, with only the distributional module (REPEL-D), it already outperforms all the baseline approaches. Compared with DPE, the performance gain of REPEL-D mainly comes from the usage of the score function in Eqn.&#x00A0;<a class="eqn" href="#eq7">7</a>, which can better model different relations. Compared with CONV, REPEL-D achieves better results, as the distributional information in text corpora can be better captured with Eqn.&#x00A0;<a class="eqn" href="#eq6">6</a>. Moreover, by encouraging the collaboration of both modules (REPEL), the results are further significantly improved. This observation demonstrates that the pattern module can indeed help improve the distributional module by providing some highly confident instances.</p>    <p>Overall, our approach achieves quite impressive results on the knowledge base completion task compared with several strong baseline approaches. Also, the pattern module can indeed enhance the distributional module with our co-training framework.</p>    <p>     <strong>2. Corpus-level Relation Extraction (RE).</strong> Next, we show the results on the corpus-level relation extraction task. We present the quantitative results in Table&#x00A0;<a class="tbl" href="#tab3">3</a> and the precision-recall in Fig.&#x00A0;<a class="fig" href="#fig6">6</a>. For the approaches using textual patterns (Snowball), we see the results are quite limited especially on the NYT dataset. This is because it discovers informative patterns in a bootstrapping way, which can lead to the semantic drift problem&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0008">8</a>] and thus harm the performance. For other neural network based pattern approaches (PathCNN, CNN-ATT, PCNN-ATT), although they are proved to be very effective when the given instances are abundant, their performance in the weakly-supervised setting is not satisfactory. The reason is that they typically deploy complicated convolutional layers or recurrent layers in their model, which rely on massive relation instances to tune. However, in our setting, the instances are very limited, leading to their poor performance. For the integration approach (LexNET), although it incorporates the distributional information, the performance is still quite limited especially on the NYT dataset. This is because the joint training framework of LexNET also requires considerable training instances.</p>    <div class="table-responsive" id="tab3">     <div class="table-caption">      <span class="table-number">Table 3:</span>      <span class="table-title">Quantitative results on the RE task.</span>     </div>     <table class="table">     <thead>       <tr>       <td valign="middle" rowspan="2" style="text-align:center;">        <strong>Algorithm</strong>       </td>       <td colspan="4" style="text-align:center;">        <strong>Wiki + Freebase</strong>        <hr/>       </td>       <td colspan="4" style="text-align:center;">        <strong>NYT + Freebase</strong>        <hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">        <strong>P@50</strong>       </td>       <td style="text-align:center;">        <strong>R@50</strong>       </td>       <td style="text-align:center;">        <strong>P@100</strong>       </td>       <td style="text-align:center;">        <strong>R@100</strong>       </td>       <td style="text-align:center;">        <strong>P@50</strong>       </td>       <td style="text-align:center;">        <strong>R@50</strong>       </td>       <td style="text-align:center;">        <strong>P@100</strong>       </td>       <td style="text-align:center;">        <strong>R@100</strong>       </td>       </tr>     </thead>      <tbody>       <tr>       <td style="text-align:center;">Snowball&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0001">1</a>]</td>       <td style="text-align:center;">58.00</td>       <td style="text-align:center;">22.14</td>       <td style="text-align:center;">65.00</td>       <td style="text-align:center;">49.62</td>       <td style="text-align:center;">20.00</td>       <td style="text-align:center;">4.50</td>       <td style="text-align:center;">21.00</td>       <td style="text-align:center;">9.46</td>       </tr>       <tr>       <td style="text-align:center;">CNN-ATT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0016">16</a>]</td>       <td style="text-align:center;">26.00</td>       <td style="text-align:center;">9.92</td>       <td style="text-align:center;">22.00</td>       <td style="text-align:center;">16.79</td>       <td style="text-align:center;">24.00</td>       <td style="text-align:center;">5.41</td>       <td style="text-align:center;">29.00</td>       <td style="text-align:center;">13.06</td>       </tr>       <tr>       <td style="text-align:center;">PCNN-ATT&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0016">16</a>]</td>       <td style="text-align:center;">58.00</td>       <td style="text-align:center;">22.14</td>       <td style="text-align:center;">36.00</td>       <td style="text-align:center;">27.48</td>       <td style="text-align:center;">46.00</td>       <td style="text-align:center;">10.36</td>       <td style="text-align:center;">26.00</td>       <td style="text-align:center;">11.71</td>       </tr>       <tr>       <td style="text-align:center;">PathCNN&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0045">45</a>]</td>       <td style="text-align:center;">36.00</td>       <td style="text-align:center;">13.74</td>       <td style="text-align:center;">38.00</td>       <td style="text-align:center;">29.01</td>       <td style="text-align:center;">42.00</td>       <td style="text-align:center;">9.46</td>       <td style="text-align:center;">26.00</td>       <td style="text-align:center;">11.71</td>       </tr>       <tr>       <td style="text-align:center;">LexNET&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0029">29</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"         href="#BibPLXBIB0030">30</a>]</td>       <td style="text-align:center;">74.00</td>       <td style="text-align:center;">28.24</td>       <td style="text-align:center;">61.00</td>       <td style="text-align:center;">46.56</td>       <td style="text-align:center;">32.00</td>       <td style="text-align:center;">7.21</td>       <td style="text-align:center;">26.00</td>       <td style="text-align:center;">11.71</td>       </tr>       <tr>       <td style="text-align:center;">REPEL-D</td>       <td style="text-align:center;">14.00</td>       <td style="text-align:center;">5.34</td>       <td style="text-align:center;">17.00</td>       <td style="text-align:center;">12.98</td>       <td style="text-align:center;">6.00</td>       <td style="text-align:center;">1.35</td>       <td style="text-align:center;">7.00</td>       <td style="text-align:center;">3.15</td>       </tr>       <tr>       <td style="text-align:center;">REPEL-P</td>       <td style="text-align:center;">64.00</td>       <td style="text-align:center;">24.43</td>       <td style="text-align:center;">70.00</td>       <td style="text-align:center;">53.44</td>       <td style="text-align:center;">32.00</td>       <td style="text-align:center;">7.21</td>       <td style="text-align:center;">33.00</td>       <td style="text-align:center;">14.86</td>       </tr>       <tr>       <td style="text-align:center;">REPEL</td>       <td style="text-align:center;">        <strong>78.00</strong>       </td>       <td style="text-align:center;">        <strong>29.77</strong>       </td>       <td style="text-align:center;">        <strong>76.00</strong>       </td>       <td style="text-align:center;">        <strong>58.02</strong>       </td>       <td style="text-align:center;">        <strong>48.00</strong>       </td>       <td style="text-align:center;">        <strong>10.81</strong>       </td>       <td style="text-align:center;">        <strong>43.00</strong>       </td>       <td style="text-align:center;">        <strong>19.37</strong>       </td>       </tr>      </tbody>     </table>    </div>    <figure id="fig6">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig6.jpg" class="img-responsive" alt="Figure 6"      longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 6:</span>      <span class="figure-title">Precision-Recall curves on the RE task.</span>     </div>    </figure>    <p>For our proposed approach, the performance of the distributional module (REPEL-D) is very bad. This is because each test entity is mentioned in only few test sentences, and thus the learned entity representations are not so effective due to the sparsity of the distributional information. On the other hand, the pattern module (REPEL-P) of our approach achieves surprisingly good results, which are comparable to the neural models. This is because we represent each pattern using the average embedding of tokens in the pattern for pattern matching, where the token embedding is learned from the given text corpus. Although such strategy is very naive compared with the neural encoding methods, it does not involve any extra parameters to learn. In the weakly-supervised setting, the neural methods are usually hard to train due to the large number of parameters, leading to inferior results. Whereas our approach achieves impressive results because of its simplicity. Furthermore, comparing the pattern module (REPEL-P) with the complete framework (REPEL), we see that the complete framework further outperforms the pattern module, which demonstrates that the distributional module can also enhance the pattern module by helping estimate pattern reliability.</p>    <p>Overall, in the weakly-supervised setting, our approach is able to achieve comparable results compared with the neural methods. Besides, the distributional module can indeed improve the pattern module with our co-training framework.</p>    </section>    <section id="sec-16">    <header>     <div class="title-info">      <h3>       <span class="section-number">5.3</span> Performance Analysis</h3>     </div>    </header>    <p>     <strong>1. Performance w.r.t. the Number of Seed Instances.</strong> To overcome the challenge of seed scarcity, our approach encourages both modules to provide extra supervision for each other. In this section, we thoroughly study whether our framework is indeed robust to the scarcity of seed instances. We take the Wiki dataset as an example, and report the performance of different methods under differ number of seed instances. <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">Performance w.r.t. # relation instances. Our approach consistently outperforms the compared algorithms especially when the given seeds are very limited.</span>      </div>     </figure>    </p>    <p>Fig.&#x00A0;<a class="fig" href="#fig7">7</a> presents the results on the KBC and RE tasks. We see that our approach (REPEL) consistently outperforms other approaches (CONV, LexNET) integrating both the distributional and pattern-based methods. Besides, our approach (REPEL) also achieves better results than its variants (REPEL-P, REPEL-D), which deploy only one module. Moreover, we observe that when the given seed instances are quite sufficient, the results of different approaches are pretty close. Whereas under very limited seed instances, our approach (REPEL) significantly outperforms its variants (REPEL-P, REPEL-D) and the baseline approaches (CONV, LexNET). Based on the observation, we see that with the co-training framework, our approach is more robust to seed scarcity compared with existing integration approaches (CONV, LexNET).</p>    <p>     <strong>2. Convergences Analysis.</strong> In our approach, we leverage the coordinate gradient descent algorithm for optimization, alternating between updating the distributional module and improving the pattern module. Next, we examine the optimization algorithm and study whether it converges during training. We take the Wiki dataset as an example, and present the performance of our approach at each iteration. <figure id="fig8">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 8:</span>       <span class="figure-title">Convergence curves of our approach. Our approach quickly converges after several iterations.</span>      </div>     </figure>    </p>    <p>Fig.&#x00A0;<a class="fig" href="#fig8">8</a> presents the results. In both tasks, the performance of our approach is consistently improved at the first several iterations, which shows that both modules can keep improving each other in our framework. Besides, we see that our approach quickly converges after several (3 &#x223C; 4) iterations, which demonstrates the efficiency of the optimization algorithm.</p>    <p>     <strong>3. Performance w.r.t. <em>&#x03BB;</em>.</strong> In our framework, the parameter <em>&#x03BB;</em> controls the weight of the interaction term <em>O<sub>i</sub>     </em> (Eqn.&#x00A0;<a class="eqn" href="#eq10">10</a>). A large <em>&#x03BB;</em> encourages strong interactions of both modules, whereas a small <em>&#x03BB;</em> corresponds to weak interactions. In this part, we study the performance of our approach under different <em>&#x03BB;</em>. We take the Wiki dataset as an example, and report the results on both tasks. <figure id="fig9">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186024/images/www2018-33-fig9.jpg" class="img-responsive" alt="Figure 9"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 9:</span>       <span class="figure-title">Performance w.r.t. <em>&#x03BB;</em>. Encouraging the interaction of the both modules (<em>&#x03BB;</em> > 0) improves the performance.</span>      </div>     </figure>    </p>    <p>Fig.&#x00A0;<a class="fig" href="#fig9">9</a> presents the results. When <em>&#x03BB;</em> is set as 0, meaning that there is no interaction between the modules, we see that the results are quite limited. Then the results are quickly improved as we gradually increase <em>&#x03BB;</em>, which further remain stable in the range (0.5, 1). If we further increase <em>&#x03BB;</em>, the results begin to drop in the knowledge base completion task, as a large <em>&#x03BB;</em> emphasizes too much on the supervision provided by the modules, and thus ignores the supervision from the given seed instances.</p>    <p>     <strong>4. Case Study.</strong> In our co-training framework, both modules will collaborate with each other to overcome the seed scarcity problem. Specifically, the distributional module provides extra signals to select reliable patterns, whereas the pattern module discovers some highly confident instances to improve the distributional module. Next, we show some case study results to intuitively illustrate that both modules can indeed mutually enhance each other.</p>    <div class="table-responsive" id="tab4">     <div class="table-caption">      <span class="table-number">Table 4:</span>      <span class="table-title">The most reliable patterns discovered by our approach. Blue patterns are incorrect ones by human.</span>     </div>     <table class="table">      <tbody>       <tr>       <td colspan="2" style="text-align:center;">        <strong>Relation: people.person.place-of-birth</strong>        <hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">        <strong>REPEL-P</strong>       </td>       <td style="text-align:center;">        <strong>REPEL</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">[Tail] [Head] birthplace</td>       <td style="text-align:center;">[Head] born place [Tail]</td>       </tr>       <tr>       <td style="text-align:center;">[Head] born place [Tail]</td>       <td style="text-align:center;">[Head] born city [Tail]</td>       </tr>       <tr>       <td style="text-align:center;">[Head] father move [Tail]</td>       <td style="text-align:center;">[Tail] [Head] birthplace</td>       </tr>       <tr>       <td style="text-align:center;">[Head] born city [Tail]</td>       <td style="text-align:center;">[Head] born June [Tail]</td>       </tr>       <tr>       <td style="text-align:center;">[Head] born January [Tail]</td>       <td style="text-align:center;">[Head] live return [Tail]</td>       </tr>       <tr>       <td colspan="2" style="text-align:center;">        <strong>Relation: people.person.parents</strong>        <hr/>       </td>       </tr>       <tr>       <td style="text-align:center;">        <strong>REPEL-P</strong>       </td>       <td style="text-align:center;">        <strong>REPEL</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">[Tail] die succeed son [Head]</td>       <td style="text-align:center;">[Tail] die succeed son [Head]</td>       </tr>       <tr>       <td style="text-align:center;">Babur [Tail] [Head]</td>       <td style="text-align:center;">[Head] daughter [Tail]</td>       </tr>       <tr>       <td style="text-align:center;">[Tail] give boy [Head]</td>       <td style="text-align:center;">[Head] son [Tail]</td>       </tr>       <tr>       <td style="text-align:center;">[Head] son [Tail]</td>       <td style="text-align:center;">descendant [Tail] [Head]</td>       </tr>       <tr>       <td style="text-align:center;">have relationship [Head] [Tail]</td>       <td style="text-align:center;">[Tail] marry have son [Head]</td>       </tr>      </tbody>     </table>    </div>    <p>We first present the most reliable path-based patterns (i.e., tokens along the shortest dependency path between two entities) discovered by our approach and its variant on the Wiki dataset in Table&#x00A0;<a class="tbl" href="#tab4">4</a>. Blue patterns are unreliable ones based on the human. Comparing our approach with its variant (REPEL-P), we see that by considering the supervision signals from the distributional module (REPEL), some unreliable patterns can be filtered out from the pattern list, and the patterns discovered by our approach (REPEL) are more reliable. Therefore, the distributional module can indeed help the pattern module for reliable pattern selection.</p>    <div class="table-responsive" id="tab5">     <div class="table-caption">      <span class="table-number">Table 5:</span>      <span class="table-title">Top ranked instances extracted by the reliable patterns. Blue instances are incorrect ones by human.</span>     </div>     <table class="table">      <tbody>       <tr>       <td style="text-align:center;">        <strong>Relation: people.person.nationality</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">(Charles IV of France, France) (Benjamin Franklin, USA)</td>       </tr>       <tr>       <td style="text-align:center;">(Adolf Hitler, German) (Thomas Jefferson, USA) (Pol Pot, Thailand)</td>       </tr>       <tr>       <td style="text-align:center;">        <strong>Relation: location.country.capital</strong>       </td>       </tr>       <tr>       <td style="text-align:center;">(Denmark, Copenhagen) (Vietnam, Ho Chi Minh City)</td>       </tr>       <tr>       <td style="text-align:center;">(Norway, Oslo) (Yugoslavia, Belgrade) (Guinea, Conakry)</td>       </tr>      </tbody>     </table>    </div>    <p>Meanwhile, we also randomly sample some instances extracted by the discovered reliable patterns, and we show them in Table&#x00A0;<a class="tbl" href="#tab5">5</a>, where the blue instances are the incorrect ones by human. From the results, we see that most instances extracted by the reliable patterns are correct and reasonable. Therefore, the pattern module can in turn benefit the distributional module by providing some reasonable relation instances.</p>    </section>   </section>   <section id="sec-17">    <header>    <div class="title-info">     <h2>      <span class="section-number">6</span> RELATED WORK</h2>    </div>    </header>    <p>Our work is related to pattern-based approaches for relation extraction. Given two entities, the pattern-based approaches predict their relation from sentences mentioning both entities. Traditional approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0041">41</a>] try to find some informative textual patterns using the given instances, and utilize the patterns for extraction. However, these approaches ignore the semantic correlations of patterns, and thus suffer from semantic drift&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>]. Recent approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0040">40</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>] address the problem by encoding textual patterns with neural networks. Despite their success, these approaches rely on considerable labeled instances to train effective models, which suffer from the seed scarcity problem in the weakly-supervised setting. Our approach solves the problem by letting the distributional module provide extra supervision.</p>    <p>Our work is also related to the distributional approaches. Typically, these approaches learn entity representations from corpus-level statistics, and meanwhile a relation classifier is trained with the relation instances, which takes entity representations as features for relation prediction. Some approaches learn entity representations from only text corpora&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>]. However, their performance is usually limited due to the lack of supervision. Some other approaches&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0013">13</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0015">15</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0036">36</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0037">37</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0042">42</a>] learn more predictive entity representations by using the given relation instances as supervision, achieving superior results. However, they also require abundant relation instances to learn effective relation classifiers, which are hard to obtain in the weakly-supervised setting. Our approach alleviates the problem by letting the pattern module generate some highly confident instances as extra seeds.</p>    <p>There are also handful studies&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0030">30</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0034">34</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>] trying to integrate the distributional and pattern-based approaches. Typically, they jointly train a distributional model and a pattern model. However, the supervision of each model totally comes from the given relation instances, which is insufficient in the weakly-supervised setting. Our approach solves the seed scarcity problem with a co-training framework, which encourages both models to provide extra supervision for each other.</p>   </section>   <section id="sec-18">    <header>    <div class="title-info">     <h2>      <span class="section-number">7</span> CONCLUSIONS</h2>    </div>    </header>    <p>In this paper, we studied corpus-level relation extraction in the weakly-supervised setting. We proposed a novel co-training framework called REPEL to integrate a pattern module and a distributional module. Our framework encouraged both modules to provide extra supervision for each other, so that they can collaborate to overcome the scarcity of seeds. Experimental results proved the effectiveness of our framework. In the future, we plan to enhance the pattern module by using neural models for pattern encoding.</p>   </section>   <section id="sec-19">    <header>    <div class="title-info">     <h2>Acknowledgments</h2>    </div>    </header>    <p>Research was sponsored in part by U.S. Army Research Lab. under Cooperative Agreement No. W911NF-09-2-0053 (NSCTA), DARPA under Agreement No. W911NF-17-C-0099, National Science Foundation IIS 16-18481, IIS 17-04532, and IIS 17-41317, and also grant 1U54GM114838 awarded by NIGMS through funds provided by the trans-NIH Big Data to Knowledge (BD2K) initiative.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>    <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>    </div>    </header>    <ul class="bibUl">    <li id="BibPLXBIB0001" label="[1]">Eugene Agichtein and Luis Gravano. 2000. Snowball: Extracting relations from large plain-text collections. In <em>      <em>ACM DL&#x2019;00</em></em>. 85&#x2013;94.</li>    <li id="BibPLXBIB0002" label="[2]">Lidong Bing, Sneha Chaudhari, Richard Wang, and William Cohen. 2015. Improving distant supervision for information extraction using label propagation through lists. In <em>      <em>EMNLP&#x2019;15</em></em>. 524&#x2013;529.</li>    <li id="BibPLXBIB0003" label="[3]">Avrim Blum and Tom Mitchell. 1998. Combining labeled and unlabeled data with co-training. In <em>      <em>COLT&#x2019;98</em></em>. 92&#x2013;100.</li>    <li id="BibPLXBIB0004" label="[4]">Antoine Bordes, Nicolas Usunier, Alberto Garcia-Duran, Jason Weston, and Oksana Yakhnenko. 2013. Translating embeddings for modeling multi-relational data. In <em>      <em>NIPS&#x2019;13</em></em>. 2787&#x2013;2795.</li>    <li id="BibPLXBIB0005" label="[5]">Razvan&#x00A0;C Bunescu and Raymond&#x00A0;J Mooney. 2005. A shortest path dependency kernel for relation extraction. In <em>      <em>HLT-EMNLP&#x2019;05</em></em>. 724&#x2013;731.</li>    <li id="BibPLXBIB0006" label="[6]">Olivier Chapelle, Bernhard Scholkopf, and Alexander Zien. 2009. Semi-supervised learning (chapelle, o. et al., eds.; 2006)[book reviews]. <em>      <em>IEEE Transactions on Neural Networks</em></em> 20, 3 (2009), 542&#x2013;542.</li>    <li id="BibPLXBIB0007" label="[7]">Aron Culotta and Jeffrey Sorensen. 2004. Dependency tree kernels for relation extraction.. In <em><em>ACL&#x2019;04</em></em>. 423&#x2013;429.</li>    <li id="BibPLXBIB0008" label="[8]">James&#x00A0;R Curran, Tara Murphy, and Bernhard Scholz. 2007. Minimising semantic drift with mutual exclusion bootstrapping. In <em><em>PACLING&#x2019;07</em></em>. 172&#x2013;180.</li>    <li id="BibPLXBIB0009" label="[9]">Joachim Daiber, Max Jakob, Chris Hokamp, and Pablo&#x00A0;N. Mendes. 2013. Improving Efficiency and Accuracy in Multilingual Entity Extraction. In <em><em>I-SEMANTICS&#x2019;13</em></em>. 121&#x2013;124.</li>    <li id="BibPLXBIB0010" label="[10]">Joe Ellis, Jeremy Getman, Justin Mott, Xuansong Li, Kira Griffitt, Stephanie Strassel, and Jonathan Wright. 2013. Linguistic Resources for 2013 Knowledge Base Population Evaluations.. In <em><em>TAC&#x2019;13</em></em>.</li>    <li id="BibPLXBIB0011" label="[11]">Oren Etzioni, Anthony Fader, Janara Christensen, Stephen Soderland, and Mausam Mausam. 2011. Open Information Extraction: The Second Generation.. In <em><em>IJCAI&#x2019;11</em></em>. 3&#x2013;10.</li>    <li id="BibPLXBIB0012" label="[12]">Raphael Hoffmann, Congle Zhang, Xiao Ling, Luke Zettlemoyer, and Daniel&#x00A0;S Weld. 2011. Knowledge-based weak supervision for information extraction of overlapping relations.. In <em><em>ACL&#x2019;11</em></em>. 541&#x2013;550.</li>    <li id="BibPLXBIB0013" label="[13]">Guoliang Ji, Shizhu He, Liheng Xu, Kang Liu, and Jun Zhao. 2015. Knowledge Graph Embedding via Dynamic Mapping Matrix.. In <em><em>ACL&#x2019;15</em></em>. 687&#x2013;696.</li>    <li id="BibPLXBIB0014" label="[14]">Meng Jiang, Jingbo Shang, Taylor Cassidy, Xiang Ren, Lance&#x00A0;M Kaplan, Timothy&#x00A0;P Hanratty, and Jiawei Han. 2017. MetaPAD: Meta Pattern Discovery from Massive Text Corpora. In <em><em>KDD&#x2019;17</em></em>. 877&#x2013;886.</li>    <li id="BibPLXBIB0015" label="[15]">Yankai Lin, Zhiyuan Liu, Maosong Sun, Yang Liu, and Xuan Zhu. 2015. Learning Entity and Relation Embeddings for Knowledge Graph Completion.. In <em><em>AAAI&#x2019;15</em></em>. AAAI, 2181&#x2013;2187.</li>    <li id="BibPLXBIB0016" label="[16]">Yankai Lin, Shiqi Shen, Zhiyuan Liu, Huanbo Luan, and Maosong Sun. 2016. Neural Relation Extraction with Selective Attention over Instances.. In <em><em>ACL&#x2019;16</em></em>. 2124&#x2013;2133.</li>    <li id="BibPLXBIB0017" label="[17]">Yang Liu, Furu Wei, Sujian Li, Heng Ji, Ming Zhou, and Houfeng Wang. 2015. A dependency-based neural network for relation classification. In <em><em>ACL&#x2019;15</em></em>. 285&#x2013;290.</li>    <li id="BibPLXBIB0018" label="[18]">Christopher&#x00A0;D. Manning, Mihai Surdeanu, John Bauer, Jenny Finkel, Steven&#x00A0;J. Bethard, and David McClosky. 2014. The Stanford CoreNLP Natural Language Processing Toolkit. In <em><em>ACL&#x2019;14 (System Demonstrations)</em></em>. 55&#x2013;60.</li>    <li id="BibPLXBIB0019" label="[19]">Tomas Mikolov, Kai Chen, Greg Corrado, and Jeffrey Dean. 2013. Efficient estimation of word representations in vector space. <em><em>arXiv preprint arXiv:1301.3781</em></em> (2013).</li>    <li id="BibPLXBIB0020" label="[20]">Tomas Mikolov, Ilya Sutskever, Kai Chen, Greg&#x00A0;S Corrado, and Jeff Dean. 2013. Distributed representations of words and phrases and their compositionality. In <em><em>NIPS&#x2019;13</em></em>. MIT Press, 3111&#x2013;3119.</li>    <li id="BibPLXBIB0021" label="[21]">Mike Mintz, Steven Bills, Rion Snow, and Dan Jurafsky. 2009. Distant supervision for relation extraction without labeled data.. In <em><em>ACL-IJCNLP&#x2019;09</em></em>. 1003&#x2013;1011.</li>    <li id="BibPLXBIB0022" label="[22]">Raymond&#x00A0;J Mooney and Razvan&#x00A0;C Bunescu. 2006. Subsequence kernels for relation extraction. In <em><em>NIPS&#x2019;06</em></em>. MIT Press, 171&#x2013;178.</li>    <li id="BibPLXBIB0023" label="[23]">Ndapandula Nakashole, Gerhard Weikum, and Fabian Suchanek. 2012. PATTY: A taxonomy of relational patterns with semantic types. In <em><em>EMNLP&#x2019;12</em></em>. 1135&#x2013;1145.</li>    <li id="BibPLXBIB0024" label="[24]">Jeffrey Pennington, Richard Socher, and Christopher&#x00A0;D Manning. 2014. Glove: Global vectors for word representation.. In <em><em>EMNLP&#x2019;14</em></em>. 1532&#x2013;1543.</li>    <li id="BibPLXBIB0025" label="[25]">Meng Qu, Xiang Ren, and Jiawei Han. 2017. Automatic Synonym Discovery with Knowledge Bases. In <em><em>KDD&#x2019;17</em></em>. 997&#x2013;1005.</li>    <li id="BibPLXBIB0026" label="[26]">Xiang Ren, Zeqiu Wu, Wenqi He, Meng Qu, Clare&#x00A0;R Voss, Heng Ji, Tarek&#x00A0;F Abdelzaher, and Jiawei Han. 2017. CoType: Joint extraction of typed entities and relations with knowledge bases.. In <em><em>WWW&#x2019;17</em></em>. 1015&#x2013;1024.</li>    <li id="BibPLXBIB0027" label="[27]">Sebastian Riedel, Limin Yao, Andrew McCallum, and Benjamin&#x00A0;M Marlin. 2013. Relation Extraction with Matrix Factorization and Universal Schemas.. In <em><em>HLT-NAACL&#x2019;13</em></em>. 74&#x2013;84.</li>    <li id="BibPLXBIB0028" label="[28]">Michael Schmitz, Robert Bart, Stephen Soderland, Oren Etzioni, and others. 2012. Open language learning for information extraction. In <em><em>EMNLP-CoNLL&#x2019;12</em></em>. 523&#x2013;534.</li>    <li id="BibPLXBIB0029" label="[29]">Vered Shwartz and Ido Dagan. 2016. Path-based vs. distributional information in recognizing lexical semantic relations. <em><em>arXiv preprint arXiv:1608.05014</em></em> (2016).</li>    <li id="BibPLXBIB0030" label="[30]">Vered Shwartz, Yoav Goldberg, and Ido Dagan. 2016. Improving hypernymy detection with an integrated path-based and distributional method. In <em><em>ACL&#x2019;16</em></em>. 2389&#x2013;2398.</li>    <li id="BibPLXBIB0031" label="[31]">Rion Snow, Daniel Jurafsky, and Andrew&#x00A0;Y Ng. 2005. Learning syntactic patterns for automatic hypernym discovery.. In <em><em>NIPS&#x2019;05</em></em>. 1297&#x2013;1304.</li>    <li id="BibPLXBIB0032" label="[32]">Jian Tang, Meng Qu, and Qiaozhu Mei. 2015. Pte: Predictive text embedding through large-scale heterogeneous text networks. In <em><em>KDD&#x2019;15</em></em>. 1165&#x2013;1174.</li>    <li id="BibPLXBIB0033" label="[33]">Jian Tang, Meng Qu, Mingzhe Wang, Ming Zhang, Jun Yan, and Qiaozhu Mei. 2015. Line: Large-scale information network embedding. In <em><em>WWW&#x2019;15</em></em>. 1067&#x2013;1077.</li>    <li id="BibPLXBIB0034" label="[34]">Kristina Toutanova, Danqi Chen, Patrick Pantel, Hoifung Poon, Pallavi Choudhury, and Michael Gamon. 2015. Representing Text for Joint Embedding of Text and Knowledge Bases.. In <em><em>EMNLP&#x2019;15</em></em>. 1499&#x2013;1509.</li>    <li id="BibPLXBIB0035" label="[35]">Patrick Verga, Arvind Neelakantan, and Andrew McCallum. 2017. Generalizing to Unseen Entities and Entity Pairs with Row-less Universal Schema. In <em><em>EACL&#x2019;17</em></em>. 613&#x2013;622.</li>    <li id="BibPLXBIB0036" label="[36]">Huazheng Wang, Fei Tian, Bin Gao, Chengjieren Zhu, Jiang Bian, and Tie-Yan Liu. 2016. Solving Verbal Questions in IQ Test by Knowledge-Powered Word Embedding.. In <em><em>EMNLP&#x2019;16</em></em>. 541&#x2013;550.</li>    <li id="BibPLXBIB0037" label="[37]">Zhen Wang, Jianwen Zhang, Jianlin Feng, and Zheng Chen. 2014. Knowledge Graph and Text Jointly Embedding.. In <em><em>EMNLP&#x2019;14</em></em>. 1591&#x2013;1601.</li>    <li id="BibPLXBIB0038" label="[38]">Stephen&#x00A0;J Wright. 2015. Coordinate descent algorithms. <em><em>Mathematical Programming</em></em> 151, 1 (2015), 3&#x2013;34.</li>    <li id="BibPLXBIB0039" label="[39]">Chang Xu, Yalong Bai, Jiang Bian, Bin Gao, Gang Wang, Xiaoguang Liu, and Tie-Yan Liu. 2014. Rc-net: A general framework for incorporating knowledge into word representations. In <em><em>CIKM&#x2019;14</em></em>. 1219&#x2013;1228.</li>    <li id="BibPLXBIB0040" label="[40]">Yan Xu, Lili Mou, Ge Li, Yunchuan Chen, Hao Peng, and Zhi Jin. 2015. Classifying Relations via Long Short Term Memory Networks along Shortest Dependency Paths.. In <em><em>EMNLP&#x2019;15</em></em>. 1785&#x2013;1794.</li>    <li id="BibPLXBIB0041" label="[41]">Mohamed Yahya, Steven Whang, Rahul Gupta, and Alon&#x00A0;Y Halevy. 2014. ReNoun: Fact Extraction for Nominal Attributes.. In <em><em>EMNLP&#x2019;14</em></em>. ACL, 325&#x2013;335.</li>    <li id="BibPLXBIB0042" label="[42]">Bishan Yang, Wen-tau Yih, Xiaodong He, Jianfeng Gao, and Li Deng. 2015. Embedding entities and relations for learning and inference in knowledge bases. In <em><em>ICLR&#x2019;15</em></em>.</li>    <li id="BibPLXBIB0043" label="[43]">Daojian Zeng, Kang Liu, Yubo Chen, and Jun Zhao. 2015. Distant Supervision for Relation Extraction via Piecewise Convolutional Neural Networks.. In <em><em>EMNLP&#x2019;15</em></em>. 1753&#x2013;1762.</li>    <li id="BibPLXBIB0044" label="[44]">Daojian Zeng, Kang Liu, Siwei Lai, Guangyou Zhou, and Jun Zhao. 2014. Relation Classification via Convolutional Deep Neural Network.. In <em><em>COLING&#x2019;14</em></em>. 2335&#x2013;2344.</li>    <li id="BibPLXBIB0045" label="[45]">Wenyuan Zeng, Yankai Lin, Zhiyuan Liu, and Maosong Sun. 2017. Incorporating relation paths in neural relation extraction. In <em><em>EMNLP&#x2019;17</em></em>. 1769&#x2013;1778.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">    <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>    <em>location.country.capital</em>, <em>people.person.parents</em>, <em>people.person.children</em>, <em>location.administrative division.country</em>, <em>people.person.place of birth</em>, <em>location.neighborhood.neighborhood of</em>, <em>people.person.nationality</em>, <em>people.deceased person.place of death</em>, <em>location.location.contains</em>, <em>organization.organization.founders</em>.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>&#x00A0;<a class="link-inline force-break" href="https://developers.google.com/freebase/">https://developers.google.com/freebase/</a>   </p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>&#x00A0;<a class="link-inline force-break" href="https://www.wikipedia.org/">https://www.wikipedia.org/</a>   </p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>    <em>people.person.children</em>,<em>people.person.place of birth</em>,<em>people.person.nationality</em>,<em>people.deceased person.place of death</em>,<em>organization.organization.founders</em>.</p>   <p id="fn5"><a href="#foot-fn5"><sup>5</sup></a>    <em>people.person.children</em>,<em>people.person.nationality</em>,<em>location.location.contains</em>, <em>people.deceased person.place of death</em>,<em>organization.organization.founders</em>, <em>location.administrative division.country</em>.</p>   <p id="fn6"><a href="#foot-fn6"><sup>6</sup></a>&#x00A0;<a class="link-inline force-break" href="http://stanfordnlp.github.io/CoreNLP/">http://stanfordnlp.github.io/CoreNLP/</a>   </p>   <p id="fn7"><a href="#foot-fn7"><sup>7</sup></a>&#x00A0;<a class="link-inline force-break"    href="https://github.com/dbpedia-spotlight/dbpedia-spotlight">https://github.com/dbpedia-spotlight/dbpedia-spotlight</a>   </p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>    <em>WWW' 18, April 23-27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License.<br/>ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186024">https://doi.org/10.1145/3178876.3186024</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div> </body> </html> 
