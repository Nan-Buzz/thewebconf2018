<!DOCTYPE html> <html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">  <head>  <title>CHIMP: Crowdsourcing Human Inputs for Mobile Phones</title>  <!-- Copyright (c) 2010-2015 The MathJax Consortium --><meta http-equiv="Content-Type" content="text/html; charset=UTF-8"></meta>  <meta name="viewport" content="width=device-width; initial-scale=1.0;"></meta>  <meta http-equiv="X-UA-Compatible" content="IE=edge"></meta>  <link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/bootstrap-theme.min.css"/><link media="screen, print" rel="stylesheet"    href="../../../data/dl.acm.org/pubs/lib/css/main.css"/><script src="../../../data/dl.acm.org/pubs/lib/js/jquery.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bootstrap.min.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/bibCit.js" type="text/javascript"></script>  <script src="../../../data/dl.acm.org/pubs/lib/js/divTab.js" type="text/javascript"></script>  <script type="text/javascript"    src="../../../data/dl.acm.org/pubs/lib/js/MathJax.js?config=TeX-AMS_CHTML"></script>  <script type="text/x-mathjax-config">MathJax.Hub.Config({tex2jax: {inlineMath: [['$','$'], ['\\(','\\)']]}});</script>  </head>  <body id="main">  <section class="front-matter">   <section>    <header class="title-info">     <div class="journal-title">     <h1>      <span class="title">CHIMP: Crowdsourcing Human Inputs for Mobile Phones</span>      <br/>      <span class="subTitle"/>     </h1>     </div>    </header>    <div class="authorGroup">     <div class="author">     <span class="givenName">Mario</span>      <span class="surName">Almeida</span>, Polytechnic University of Catalonia     </div>     <div class="author">     <span class="givenName">Muhammad</span>      <span class="surName">Bilal</span>, Universite Catholique de Louvain     </div>     <div class="author">     <span class="givenName">Alessandro</span>      <span class="surName">Finamore</span>, Telefonica Research     </div>     <div class="author">     <span class="givenName">Ilias</span>      <span class="surName">Leontiadis</span>, Telefonica Research     </div>     <div class="author">     <span class="givenName">Yan</span>      <span class="surName">Grunenberger</span>, Telefonica Research     </div>     <div class="author">     <span class="givenName">Matteo</span>      <span class="surName">Varvello</span>, AT&#x0026;T     </div>     <div class="author">     <span class="givenName">Jeremy</span>      <span class="surName">Blackburn</span>, University of Alabama at Birmingham     </div>        </div>    <br/>    <div class="pubInfo">     <p>DOI: <a href="https://doi.org/10.1145/3178876.3186035" target="_blank">https://doi.org/10.1145/3178876.3186035</a>     <br/>WWW '18: <a href="https://doi.org/10.1145/3178876" target="_blank">Proceedings of The Web Conference 2018</a>, Lyon, France, April 2018</p>    </div>    <div class="abstract">     <p>     <small>While developing mobile apps is becoming easier, testing and characterizing their behavior is still hard. On the one hand, the de facto testing tool, called &#x201C;Monkey,&#x201D; scales well due to being based on random inputs, but fails to gather inputs useful in understanding things like user engagement and attention. On the other hand, gathering inputs and data from real users requires distributing instrumented apps, or even phones with pre-installed apps, an expensive and inherently unscaleable task.</small>     </p>     <p>     <small>To address these limitations we present CHIMP, a system that integrates automated tools and large-scale crowdsourced inputs. CHIMP is different from previous approaches in that it runs apps in a virtualized mobile environment that thousands of users all over the world can access via a standard Web browser. CHIMP is thus able to gather the full range of real-user inputs, detailed run-time traces of apps, and network traffic.</small>     </p>     <p>     <small>We thus describe CHIMP&#x0027;s design and demonstrate the efficiency of our approach by testing thousands of apps via thousands of crowdsourced users. We calibrate CHIMP with a large-scale campaign to understand how users approach app testing tasks. Finally, we show how CHIMP can be used to improve both traditional app testing tasks, as well as more novel tasks such as building a traffic classifier on encrypted network flows.</small>     </p>    </div>    <div class="classifications">     <div class="AcmReferenceFormat">     <p>      <small>       <span style="font-weight:bold;">ACM Reference Format:</span>       <br/>       Mario Almeida, Muhammad Bilal, Alessandro Finamore, Ilias Leontiadis, Yan Grunenberger, Matteo Varvello, Jeremy Blackburn. 2018. CHIMP: Crowdsourcing Human Inputs for Mobile Phones. In <em>WWW 2018: The 2018 Web Conference,</em>       <em>April 23&#x2013;27, 2018,</em>       <em> Lyon, France. ACM, New York, NY, USA</em> 11 Pages. <a href="https://doi.org/10.1145/3178876.3186035" class="link-inline force-break"        target="_blank">https://doi.org/10.1145/3178876.3186035</a></small>     </p>     </div>    </div>   </section>  </section>  <section class="body">   <section id="sec-2">    <header>     <div class="title-info">     <h2>      <span class="section-number">1</span> Introduction</h2>     </div>    </header>    <p>While developing apps has become easier, testing and characterizing them remains challenging because of a dearth of tools for large-scale testing and measurement of mobile apps. The de facto standard app testing technique is to use &#x201C;monkeys&#x201D;&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>]. A monkey is a simple tool that performs random (partially configurable) inputs, operating under the assumption that a million monkeys tapping on a million touch screens will eventually expose faulty code.</p>    <p>While this might be true, there are several issues. First, prior work has shown that monkeys are not well suited for certain types of inputs&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>], e.g., filling out forms. Second, monkeys&#x2019; inputs do not reflect those of actual app users. Figure&#x00A0;<a class="fig" href="#fig1">1</a> visually shows this issue when testing &#x201C;frozen bubble&#x201D;, an Android game, via both real users and monkeys. While real users focus on the part of the screen where game play actually happens, monkeys make no distinction and spread their efforts across the entire UI. While this might have some advantages if exploring all code paths is the desired outcome, it is very much a problem when looking for, e.g., the way an apps&#x2019; users access the network, understanding how users will navigate through options/menus, or evaluating a change in functionality/UI. <figure id="fig1">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig1.jpg" class="img-responsive" alt="Figure 1"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 1:</span>      <span class="figure-title">Human input (100 users) vs monkeys when playing frozen-bubbles.</span>     </div>     </figure>    </p>    <p>For this reason, we still need humans to test applications, both in industry and research. Unfortunately, while large-scale human testing is (mostly) achievable for giants of industry (e.g., Apple, Google, Facebook, Microsoft, and Amazon), many smaller developers do not have thousands of users to A/B test with, or control over app delivery mechanisms (i.e., app stores)&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>]. Indeed, even solutions proposed by app store operators have their own limitations (e.g., in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>] only owned apps can be tested and users must install them). Further, the research literature is littered with examples where authors spend many hours manually running apps to better understand a variety of issues&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0024">24</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0032">32</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0035">35</a>].</p>    <p>In this paper we present <SmallCap>CHIMP</SmallCap>, a flexible Android app testing system that enables quick collection of human inputs for mobile apps. <SmallCap>CHIMP</SmallCap> runs apps on a server, streaming them to a browser for real users to interact with. While users test apps, <SmallCap>CHIMP</SmallCap> collects a wide range of data (user interactions, network traffic, runtime traces, performance, etc.) as well as explicit user feedback. &#x201C;Experimenters&#x201D; (e.g., app developers or researchers) can use <SmallCap>CHIMP</SmallCap> with apps they want to test and specify the data they want to collect via <em>campaigns</em>. <SmallCap>CHIMP</SmallCap> offers integration with CrowdFlower &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0014">14</a>], along with user validation techniques, to quickly provide large, trust-worthy datasets. For example, the human input behind Figure&#x00A0;<a class="fig" href="#fig1">1</a> was obtained from 100 CrowdFlower users in just a couple of hours.</p>    <p>We present the design (&#x00A7;<a class="sec" href="#sec-4">3</a>) and evaluation (&#x00A7;<a class="sec" href="#sec-9">4</a>) of <SmallCap>CHIMP</SmallCap> with thousands of apps and users. We analyze user interactions via a calibration campaign to design more useful campaigns (&#x00A7;<a class="sec" href="#sec-10">5</a>). We then demonstrate two use cases for <SmallCap>CHIMP</SmallCap> showing that integrating users into the testing loop can improve code coverage by up to 25% (&#x00A7;<a class="sec" href="#sec-13">6</a>), and that the three times increase in network traffic volume it generates can be used to build an app classification model which achieves f1 scores over 0.9 in some cases. Finally, we discuss our findings, <SmallCap>CHIMP</SmallCap>&#x2019;s limitations, and conclude (&#x00A7;<a class="sec" href="#sec-17">8</a>).</p>   </section>   <section id="sec-3">    <header>     <div class="title-info">     <h2>      <span class="section-number">2</span> Background &#x0026; Related Work</h2>     </div>    </header>    <p>While <SmallCap>CHIMP</SmallCap> draws heavily from the automated testing literature, it is designed to meet the needs of researchers in a wide variety of contexts. In this section, we provide background on the state of the art in automated testing tools as well as an overview of other app behavior measurement techniques and applications.</p>    <p>     <strong>Automated App Testing:</strong> Automated testing can be considered a search problem where the objective is to &#x201C;explore&#x201D; the largest possible set of app functionalities within a defined time span. Such an exploration is usually measured in terms of <em>code coverage</em>, i.e., the number of lines of code in the target app that the test exercises. [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] reviews the state of the art, evaluating 14 testing tools grouped into 3 categories: <em>random</em>, <em>model based</em>, and <em>systematic</em>.</p>    <p>     <em>Random</em> tools&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0038">38</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0050">50</a>] are best exemplified by the official Android monkey&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0005">5</a>]. They amount to a blind search through the app being tested. Random testing tools are reasonably easy to use and often provide pretty good coverage.</p>    <p>     <em>Model based</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0020">20</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0048">48</a>] tools view mobile apps as a finite automata where user actions trigger transitions between states. Models can be extracted considering the sequence of function calls (call graph model - CGM), the user interface layout, and interaction between components (interface model - IM). After the model is built, testing corresponds to exploring the space of the state machine, terminating when all state transitions have been discovered.</p>    <p>     <em>Systematic</em> exploration tools&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0009">9</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0045">45</a>] are more complicated and use things like evolutionary algorithms in an attempt to produce inputs that improve code coverage. For example, EvoDroid&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0028">28</a>] uses the IM as &#x201C;genes&#x201D; and the CGM as space to be explored while a fitness function optimizes code coverage and guides the exploration.</p>    <p>All these tools have strengths and weaknesses, but ultimately&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] finds no tool to be superior; indeed, monkeys often beat more sophisticated tools in terms of code coverage. They share however an important limitation: they are &#x201C;stress test&#x201D; tools only. Since no real human input is synthesized, no information regarding actual human behavior is collected (i.e., how users react to a user interface), nor if users consider the app performing correctly. There are also tasks that might either be easier for, or even <em>require</em>, real humans, e.g., login screens, forms, games, etc.</p>    <p>There are a few services that can bring humans into the app testing loop, similar to <SmallCap>CHIMP</SmallCap>. The two most popular services are offered by Amazon&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0003">3</a>] and Google&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0017">17</a>], and integrated into their app stores. <SmallCap>CHIMP</SmallCap> is different in a few ways though. First, they are meant to be used by an app&#x0027;s developer exclusively, and therefore not good candidates for large scale app analysis. Second, they either require the developer to provide a mailing list or make a version of the app available for open beta testing in their stores. Finally, these tools are mostly oriented towards testing app compatibility with different devices and do not provide any additional access or low level information (e.g., traffic dumps, instrumentation, etc.) which researchers struggle to capture at scale. Like <SmallCap>CHIMP</SmallCap>, Appetize&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0008">8</a>] provides streaming of mobile devices to browsers. However, its focus is not measurements and experimentation, for example, it does not provide any mechanism to acquire users or to analyze apps at scale. There are other human-based services occupying the same general space as <SmallCap>CHIMP</SmallCap> by having in-house app testers with real devices, but scalability and pricing (e.g., [<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0044">44</a>] charges <font style="normal">&#x0024;</font>99 per user testing session) make it prohibitive for large scale app analysis.</p>    <p>     <strong>Crowdsourced Systems</strong> Although not quite an automated testing tool, Varvello et al.&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>] built EYEORG, a platform for crowdsourcing web quality of experience measurements. EYEORG presents paid crowdsourced workers with interactive videos of web page crawls, allowing users to provide judgments on performance in a controlled, yet scalable environment. <SmallCap>CHIMP</SmallCap> is similar to EYEORG in spirit but it provides an orthogonal service. Nevertheless, <SmallCap>CHIMP</SmallCap>&#x2019;s evaluation follows the validation methodology laid out by&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0046">46</a>], which includes using engagement estimation, and control questions (&#x00A7;<a class="sec" href="#sec-10">5</a>).</p>    <p>Nikravesh et al.&#x00A0;built Mobilizer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>], a platform for performing network measurements in a mobile environment that also leverages crowdsourcing. The key insight of Mobilyzer is that the idea of a &#x201C;killer app&#x201D; that can reach enough user penetration to be of meaningful use is not very realistic. Mobilyzer is delivered as a combination of library and service. Experimenters can design and issue experiments to gain a view of network conditions across all Mobilyzer devices. They demonstrate ease of development (&#x201C;about an hour&#x201D; for a 3rd party app&#x0027;s developers to integrate Mobilyzer) and demonstrate its effectiveness by conducting crowdsourced measurements of mobile Web performance and Video QoE.</p>    <p>While obviously related, Mobilyzer and <SmallCap>CHIMP</SmallCap> have with fundamentally different objectives. While Mobilyzer provides a previously unseen global view of the mobile network, CHIMP focuses more on app and user behavior, giving researchers and developers a different view of the mobile environment.</p>    <p>     <SmallCap>CHIMP</SmallCap> is designed to complement state of the art tools, and to offer a flexible platform to collect a rich set of data targeting human behavior specifically. For app developers, it means that they can quickly A-B test design and algorithmic choices before releasing an app to an app-store. For the research community, it means it is now possible to run experiments on apps the researcher has no control over. Indeed, the impetus for building <SmallCap>CHIMP</SmallCap> was our frustration as researchers when trying to collect mobile app interaction data for even dozens of users, let alone hundreds or thousands.</p>   </section>   <section id="sec-4">    <header>     <div class="title-info">     <h2>      <span class="section-number">3</span> Chimp</h2>     </div>    </header>    <p>     <SmallCap>CHIMP</SmallCap> is made available as a web application, and users interact with it via the <em>web-client</em>. Within the web-client, they interact with an Android <em>virtual phone</em>, where mouse clicks and drags get translated into taps and swipes.</p>    <p>     <SmallCap>CHIMP</SmallCap> integrates several technologies behind the scenes to achieve its goal of providing insights on mobile apps via both objective measurements (e.g., application runtime analysis, network traffic, permissions) and feedback from users. Further, to meet many of our goals <SmallCap>CHIMP</SmallCap> must scale reasonably well and be flexible enough to support different types of analysis, number of users, and apps. This necessitates addressing a few challenges we discuss in this section: 1)&#x00A0;web-client workflow, i.e., how to organize and present tests to users, 2)&#x00A0;selecting an effective means of virtualizing a phone for users, 3)&#x00A0;supporting a multi-user experimentation platform, and 4)&#x00A0;collecting useful data for experimenters. <figure id="fig2">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig2.jpg" class="img-responsive" alt="Figure 2"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 2:</span>      <span class="figure-title">       <SmallCap>CHIMP</SmallCap>&#x2019;s architecture: timeline a user session (top), system composition (bottom).</span>     </div>     </figure>    </p>    <section id="sec-5">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.1</span> Web-client Workflow</h3>     </div>     </header>     <p>Figure&#x00A0;<a class="fig" href="#fig2">2</a> sketches <SmallCap>CHIMP</SmallCap>&#x2019;s architecture. As an illustrative example, the figure includes a screenshot of what a user sees when interacting with the YouTube app. In particular, the webpage presented to the user is composed of a <em>streaming area</em> replicating the content of the virtual phone&#x0027;s display (dashed area in the figure), and a <em>control area</em> allowing the user to issue specific commands to the system (zoomed area in the figure).</p>     <p>There are a number of actions that must be taken before the user can actually interact with an app, which are illustrated in Figure&#x00A0;<a class="fig" href="#fig2">2</a> (top) with a timeline, along with the associated system actions (bottom). In the following, we use this visual aid to describe the status of the user&#x0027;s interactions with <SmallCap>CHIMP</SmallCap>.</p>     <p>We define a <em>session</em> as the entire set of actions a user takes in <SmallCap>CHIMP</SmallCap>. A session starts when the user visits <SmallCap>CHIMP</SmallCap>&#x2019;s homepage and presses the <tt>&#x003C;Take a test&#x003E;</tt> button. A welcome message is presented, including a form to collect demographic data. While the user is busy filling out the form, <SmallCap>CHIMP</SmallCap> prepares a virtual phone (&#x00A7;<a class="sec" href="#sec-6">3.2</a>) and installs the first app. When the virtualized environment is ready, users can press <tt>&#x003C;Launch&#x003E;</tt>. They are then presented with a set of instructions on how to use <SmallCap>CHIMP</SmallCap>. Once the instructions are dismissed, users can interact with their first app.</p>     <p>Since <SmallCap>CHIMP</SmallCap> allows users to interact with different apps, we partition each session into multiple <em>steps</em>; one per app. The control area lets users navigate through steps via the <tt>&#x003C;Next app&#x003E;</tt> and <tt>&#x003C;Finish&#x003E;</tt> buttons. Specifically, users interact with the current app until clicking one of these two buttons. After clicking one of these two buttons, an <em>experience feedback questionaire</em> is presented.</p>     <p>If the user pressed <tt>&#x003C;Next app&#x003E;</tt>, the previous app is removed from the virtual phone and a new app is installed. After filling out the experience feedback form, users can interact with the new app. If instead the user opted for <tt>&#x003C;Finish&#x003E;</tt>, the virtual phone is shut down and the session terminates after the app experience feedback form is completed. The control area also shows a cumulative session duration and overall step progress (&#x223C; 4 minutes and 7 apps in Figure&#x00A0;<a class="fig" href="#fig2">2</a>). Multiple user sessions with the same setup are logically grouped into a <em>campaign</em> (details discussed in &#x00A7;<a class="sec" href="#sec-7">3.3</a>).</p>    </section>    <section id="sec-6">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.2</span> Android Virtual Phone</h3>     </div>     </header>     <p>The core of <SmallCap>CHIMP</SmallCap> is built around the virtualization of (Android) mobile devices. This is accomplished by instrumenting virtual machines (VM) running the Android operating system to provide the user with an Android Virtual Phone (AVP). To maximize app compatibility, <SmallCap>CHIMP</SmallCap> should be able to 1)&#x00A0;execute ARM instructions (to support apps that use native binaries that target it), 2)&#x00A0;support OpenGL (especially for games), and 3)&#x00A0;offer fluid interactivity. We evaluated several existing solutions and discuss the lessons learned.</p>     <p>     <strong>Android VM:</strong> The first obvious solution is the Android emulator (Emu) which comes in two flavors: Emu-ARM and Emu-x86. Emu-ARM refers to the original Android emulator which implemented the ARM instruction set in software. Emu-ARM is known to suffer huge performance penalties when running on x86 architectures&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0022">22</a>], and is often replaced by Emu-x86. Notice that, despite the name, Emu-x86 is actually a VM hosting a build of Android targeting the x86 instruction set.</p>     <p>While Emu-x86 speeds up app execution, it introduces the problem of translating instructions for native binaries that target ARM. Android apps are mostly built in Java which (in theory) does not target specific hardware at all, but they <em>can</em> make use of native code. Since ARM dominates the mobile landscape, most build systems compile to ARM native code by default, leaving x86 support as optional. The real-world implication of this is that native x86 binaries cannot be assumed present in apps. This means that while Emu-x86 is a good solution when <em>developing</em> apps, it does not fit our needs. To deal with running ARM code on x86 chipsets, Intel developed <em>houdini</em> which performs on the fly translation of ARM to x86 instructions. Fortunately, the community-driven port of Android for x86 architectures (Android-x86&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0006">6</a>]) has native support for houdini&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0007">7</a>]. We opted to directly use QEMU, a popular open source hardware emulator and virtualizer. Using QEMU directly gives us fine-grained control over VM RAM allocation, CPU core usage, and supports output streaming via websockets.</p>     <p>While QEMU takes care of most hardware virtualization tasks, particular attention is needed for OpenGL. QEMU supports <em>virglrenderer</em>, a virtual GPU that provides the Android guest VM with access to the host&#x0027;s GPU for hardware accelerated graphics (i.e., OpenGL support), however it is unusable out of the box. Integrating virglrenderer required us to recompile the Android-x86 image and QEMU to enable GTK library support. Next, since QEMU emulates a VESA-compliant VGA output device, we modified the VGA BIOS to support WVGA resolutions found in mobile devices (e.g., 400x800).</p>     <p>As reported in Figure&#x00A0;<a class="fig" href="#fig2">2</a>, each user session is associated with a separate QEMU/Android-x86 VM, which corresponds to one <em>virtual phone</em>. Overall, <SmallCap>CHIMP</SmallCap>&#x2019;s virtualization is composed of two technologies: 1)&#x00A0;Android-x86 to execute the Android operating system and apps and 2)&#x00A0;QEMU to virtualize the phone hardware.</p>     <p>     <strong>Streaming:</strong> QEMU natively supports Spice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0041">41</a>] and VNC (an implementation of RFB&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0036">36</a>]), two popular streaming technologies. Additionally, streaming can also be done via XSpice&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0042">42</a>], a variant of Spice which uses an X11 server. We experimented with all three options (results not reported for brevity), by creating an HTML5 client handling the streaming from the virtual phone, i.e., the streaming area in Figure&#x00A0;<a class="fig" href="#fig2">2</a>. In the end, we opted for VNC which offered a more fluid experience throughout our tests.</p>    </section>    <section id="sec-7">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.3</span> Experimentation Platform</h3>     </div>     </header>     <p>While the AVP is a necessary component for implementing <SmallCap>CHIMP</SmallCap>, it is not sufficient. <SmallCap>CHIMP</SmallCap> must allow experimenters to specify campaigns and dynamically launch and manage AVPs for users.</p>     <p>     <strong>Campaigns:</strong> As previously mentioned, user sessions are grouped into campaigns. A campaign is a set of parameters that includes the target number of users, the set of apps to test, how many apps per user, if apps should be presented to users in a random or pre-set order, the amount of time to be spent on each app, and what data to collect (&#x00A7;<a class="sec" href="#sec-8">3.4</a>). The instructions for the campaign and interstitial feedback forms are also customizable via <tt>JSON</tt>.</p>     <p>     <strong>Orchestration:</strong> The different components of <SmallCap>CHIMP</SmallCap> are orchestrated via a <em>controller</em> and <em>scheduler</em> (see Figure&#x00A0;<a class="fig" href="#fig2">2</a>). The controller tracks events issued by the web-client control area, and triggers <em>job</em> scheduling. E.g., when clicking the <tt>&#x003C;Next app&#x003E;</tt> button the controller schedules jobs to retrieve data from the virtual phone and the web-client, and to prepare the virtual phone for the next app. The controller is a multi-process, multi-threaded, and stateless Ruby on Rails application served from Puma&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0034">34</a>] webservers which run behind an Nginx&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0030">30</a>] reverse proxy on the server.</p>     <p>We use Sidekiq&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0040">40</a>] as our job processing framework, which in turn uses Redis to back job queues. As pictured in Figure&#x00A0;<a class="fig" href="#fig2">2</a>, <SmallCap>CHIMP</SmallCap>&#x2019;s scheduler makes use of 3 queues with priorities <em>high</em>, <em>default</em>, and <em>low</em>. The high priority queue is reserved for system critical jobs that have tight scheduling deadlines (e.g., extracting code coverage metrics over time). The default queue handles jobs related to user experience, e.g., virtual phone booting/shutdown and app installation. Low priority jobs handle things reclaiming resources from timed-out sessions and post-processing of campaigns for reporting.</p>     <p>Jobs themselves interact with AVPs via the Android Debug Bridge (<em>ADB</em>), and are often chained together to perform more complex operations. E.g., booting a virtual phone is done by chaining four jobs: 1)&#x00A0;replicating an Android image and booting it via QEMU, 2)&#x00A0;making the VM accessible to the user (i.e., opening ports and configuring mappings), 3)&#x00A0;enabling measurements (i.e., setting up communications between the AVP and requested data collection modules), and 4)&#x00A0;scheduling an app installation.</p>    </section>    <section id="sec-8">     <header>     <div class="title-info">      <h3>       <span class="section-number">3.4</span> Data Collection Modules</h3>     </div>     </header>     <p>     <SmallCap>CHIMP</SmallCap> allows the collection of data via different modules. Since <SmallCap>CHIMP</SmallCap> collects data from human participants<a class="fn" href="#fn1" id="foot-fn1"><sup>1</sup></a> we make sure to: 1)&#x00A0;collect only the minimum data required for the system to achieve its goals, 2)&#x00A0;anonymize any sensitive data, and 3)&#x00A0;inform the users of which data is collected prior to each session. Although we intend to expand the type of data collected in the future, for now <SmallCap>CHIMP</SmallCap> supports collecting six types of data.</p>     <p>     <strong>User interactions:</strong> The web-client collects mouse events from the streaming area, as well as information on browser focus.<a class="fn" href="#fn2" id="foot-fn2"><sup>2</sup></a>This is achieved by instrumenting the web-client with JavaScript attached to the HTML5 VNC streaming area. This data is crucial to understand user behavior (e.g., identifying where users click as in Figure&#x00A0;<a class="fig" href="#fig1">1</a>), to validate the quality of the crowdsourced workers (&#x00A7;<a class="sec" href="#sec-10">5</a>), or to reproduce crashes by replaying inputs. The web-client uploads user interaction data at the end of each session step (Figure&#x00A0;<a class="fig" href="#fig2">2</a>).</p>     <p>     <strong>User feedback:</strong> While the virtual phone is booting, the users are asked to provide some demographic feedback (age, gender, country, and mobile app expertise). At the end of each session step users are asked to provide some feedback on their experience using the app, i.e., if the app crashed or required login, give a rating on &#x201C;fun&#x201D; and &#x201C;speed&#x201D; of the app (on a scale of 1 to 3), and place the app in one of several pre-defined categories (e.g., social, multimedia, or game).</p>     <p>     <strong>App data:</strong> At the end of each session step, <SmallCap>CHIMP</SmallCap> retrieves the execution history of the app (<tt>logcat</tt> on the virtual phone), to identify exceptions or crashes (if any). Similarly, <tt>dumpsys</tt> is used to collect app&#x0027;s resource consumption (CPU, memory, disk I/O, network), interactions with the operating system, permissions, sensor usage, etc. App meta-data (e.g., number of downloads, category) as well as app structure (e.g., classes, methods) are retrieved via static analysis of apps downloaded from the Google Play store.</p>     <p>     <strong>Runtime data:</strong>&#x00A0; The app runtime execution (&#x00A7;<a class="sec" href="#sec-13">6</a>) is captured via method tracing. Additionally, EMMA&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0016">16</a>], the defacto standard of code coverage analysis, is also supported for opensource apps. It also allows running automated monkeys whose inputs can be used in conjunction with humans.</p>     <p>     <strong>Network data:</strong>     <SmallCap>CHIMP</SmallCap> uses <em>tcpdump</em> to collect raw pcap files and polls Android&#x0027;s <tt>/proc/net</tt> where open network sockets are listed along with their UID.<a class="fn" href="#fn3" id="foot-fn3"><sup>3</sup></a>This solution requires calibration to capture ephemeral flows (i.e., very short lived connections). In our tests, only 1% of the flows were shorter than 100&#x00A0;ms, so we consider a 50&#x00A0;ms polling frequency as a conservative choice. We further map UIDs to the app&#x0027;s package name using the <tt>dumpsys</tt> information.</p>     <p>     <strong>System data:</strong> A module that monitors both the whole system health, as well as the connectivity toward each individual user web-client and the associated virtual phone. In particular, the web-client runs HTTP pings&#x00A0;every 5s towards <SmallCap>CHIMP</SmallCap>&#x2019;s webserver, and collects frame buffer updates from the streaming area. The system records also the current workload (e.g., virtual phones running, memory available, etc.), as well as the time-line progress of each individual user session using a combination of <tt>collectd</tt>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0013">13</a>] for collection of system metrics and <tt>graphana</tt>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0019">19</a>] for visualization.</p>    </section>   </section>   <section id="sec-9">    <header>     <div class="title-info">     <h2>      <span class="section-number">4</span> Implementation</h2>     </div>    </header>    <p>This section describes the details of <SmallCap>CHIMP</SmallCap>&#x2019;s implementation. We start by briefly reporting on the alternative choices we contemplated. Next, we evaluate our prototype in terms of resource consumption and user experience. Finally, we discuss its limitations.</p>    <div class="table-responsive" id="tab1">     <div class="table-caption">     <span class="table-number">Table 1:</span>     <span class="table-title">Summary of <SmallCap>CHIMP</SmallCap>&#x2019;s campaigns. NB: CrowdFlower charges an additional 20% processing fee.</span>     </div>     <table class="table">     <thead>      <tr>       <th style="text-align:right;"/>       <th style="text-align:center;">        <strong># Users</strong>       </th>       <th style="text-align:center;">        <strong>Duration</strong>       </th>       <th style="text-align:center;">        <strong>User pay</strong>       </th>       <th style="text-align:center;">        <strong># Apps</strong>       </th>       <th style="text-align:center;">        <strong># Steps</strong>       </th>      </tr>     </thead>     <tbody>      <tr>       <td style="text-align:right;">        <strong>Automation</strong>       </td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">1 day</td>       <td style="text-align:center;">-</td>       <td style="text-align:center;">18,010</td>       <td style="text-align:center;">100</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>Discovery</strong>       </td>       <td style="text-align:center;">1,000</td>       <td style="text-align:center;">1.25 days</td>       <td style="text-align:center;"><font style="normal">&#x0024;</font>120</td>       <td style="text-align:center;">1,000</td>       <td style="text-align:center;">7</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>Calibration</strong>       </td>       <td style="text-align:center;">100</td>       <td style="text-align:center;">3 hour</td>       <td style="text-align:center;"><font style="normal">&#x0024;</font>12</td>       <td style="text-align:center;">7</td>       <td style="text-align:center;">7</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>Code Coverage</strong>       </td>       <td style="text-align:center;">1,000</td>       <td style="text-align:center;">1.4 days</td>       <td style="text-align:center;"><font style="normal">&#x0024;</font>120</td>       <td style="text-align:center;">59</td>       <td style="text-align:center;">4</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>Trace Coverage</strong>       </td>       <td style="text-align:center;">500</td>       <td style="text-align:center;">1 day</td>       <td style="text-align:center;"><font style="normal">&#x0024;</font>60</td>       <td style="text-align:center;">55</td>       <td style="text-align:center;">4</td>      </tr>      <tr>       <td style="text-align:right;">        <strong>Traffic Classification</strong>       </td>       <td style="text-align:center;">500</td>       <td style="text-align:center;">22 hours</td>       <td style="text-align:center;"><font style="normal">&#x0024;</font>60</td>       <td style="text-align:center;">76</td>       <td style="text-align:center;">4</td>      </tr>     </tbody>     </table>    </div>    <p>     <strong>Setup:</strong> We deployed <SmallCap>CHIMP</SmallCap> on a server with a Dual Intel Xeon CPU E5-2697v3 (2.60GHz), with 128&#x00A0;GB of RAM, and a single 7,200&#x00A0;RPM hard disk with 130MB/s write throughput. To control the Android VMs we used the default ADB implementation with automated device detection, which is limited to a predefined range of only 15 ports. While this could be trivially extended by having <SmallCap>CHIMP</SmallCap> managing the adb connections, we aim to scale <SmallCap>CHIMP</SmallCap> horizontally by adding smaller on-demand backend machines and so the following evaluation operates within this limit.</p>    <p>For Android VMs we used a customized Android-x86 image (4.4-RC1, although we also support 6.0), which requires around 1.6&#x00A0;GB after installing when using QEMU&#x0027;s <em>qcow2</em> image format&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0025">25</a>]. A copy of the image is made for each user; we experimented storing these images both in disk and in volatile memory, i.e., a <em>ramdisk</em>. For both setups we benchmarked each of QEMU&#x0027;s caching policies, but in the following we compare only the two best performing strategies: writeback caching when using disk, and no caching when storing images in RAM (using tmpfs, a RAM-based file system).</p>    <p>To evaluate <SmallCap>CHIMP</SmallCap>&#x2019;s implementation, we cast our net wide by crawling the top 500 apps per category in the Google Play store, retrieving a total of 18,787 unique apps. For users, we use both &#x201C;synthetic&#x201D; and human users. We created synthetic users with <em>Selenium</em>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0039">39</a>], a framework for automating web browser interaction, while we recruited 1K real users on CrowdFlower. We leverage synthetic users to cover the full set of apps (at no cost) while stressing our implementation, i.e., we launch up to 15 simultaneous synthetic users. We leverage real users to test a subset of apps (1K randomly selected, non-crashing apps) and to collect explicit feedback on system performance. A summary of these two campaigns appears in Table&#x00A0;<a class="tbl" href="#tab1">1</a> as &#x201C;Automation&#x201D; and &#x201C;Discovery,&#x201D; respectively. <figure id="fig3">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig3.jpg" class="img-responsive" alt="Figure 3"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 3:</span>      <span class="figure-title">CPU, disk, and RAM utilization as a function of number of concurrent users. Rows are HDD and RAM based disks, respectively.</span>     </div>     </figure>     <figure id="fig4">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig4.jpg" class="img-responsive" alt="Figure 4"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 4:</span>      <span class="figure-title">CDF of mean question scores per app for 1,000 tested apps.</span>     </div>     </figure>     <figure id="fig5">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig5.jpg" class="img-responsive" alt="Figure 5"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 5:</span>      <span class="figure-title">Boxplots of time spent on each app.</span>     </div>     </figure>     <figure id="fig6">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig6.jpg" class="img-responsive" alt="Figure 6"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 6:</span>      <span class="figure-title">CDF of actions (clicks and move) per user.</span>     </div>     </figure>     <strong>App Compatibility:</strong> The first major question we aim to answer is simply how many <em>real</em> apps does <SmallCap>CHIMP</SmallCap> support? Using the app data collection module we find that while 13,374 of the apps in the campaign installed fine, 474 failed to install, and 4,939 failed to be brought to the foreground of the AVP (i.e., the app did not successfully launch). Failing to install indicates that the app binary is broken, incompatible with hardware, or it takes too long to install (we enforce an upper bound of 40s for app installation). Failing to be brought to the foreground is often a sign of a crashing app. Regardless, 71% of apps were successfully brought to the foreground, indicating that <SmallCap>CHIMP</SmallCap> has pretty good support for real-world apps (compare to the &#x201C;official&#x201D; mechanism&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0047">47</a>] for running Android apps in a browser which supports 58.7% of tested apps&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0026">26</a>]).</p>    <p>     <strong>Resource utilization:</strong> Next, to get an idea of how our design choices play out in terms of resource usage, Figure&#x00A0;<a class="fig" href="#fig3">3</a> plots the distribution of CPU, disk, and RAM utilization as recorded by <SmallCap>CHIMP</SmallCap> during our synthetic user campaign. Notice that for disk, the utilization corresponds to throughput of the disk. Our server has plenty of CPU available to sustain the workload, e.g., in the worst case (15 concurrent users), we see a maximum of 30% CPU utilization. Instead, <SmallCap>CHIMP</SmallCap> is limited by disk I/O, e.g., up to 100% disk utilization (130MB/s) with only 4 concurrent VMs and a physical disk. tmpfs alleviates this contention and allows <SmallCap>CHIMP</SmallCap> to scale up to 15 concurrent VMs. Here, RAM usage is linear since resource consumption is driven by the size of Android images (previously on disk) and the RAM allocated to the VM itself. We saw a maximum of 10 and 7 Mbps network transmission and reception rates, respectively.</p>    <p>Based on the analysis above, <SmallCap>CHIMP</SmallCap> uses tmpfs and the controller (&#x00A7;<a class="sec" href="#sec-7">3.3</a>) ensures that system load remains below configurable thresholds. This means that new users might be put on hold when attempting to start a new session if not enough resources are available. Such functionality is key when regulating the user load from CrowdFlower since it provides no API to request a user arrival rate. However, jobs can be started, paused, and stopped at the job owner&#x0027;s discretion and we engineered the <SmallCap>CHIMP</SmallCap> controller to regulate system workload by using this &#x201C;trick.&#x201D;</p>    <p>     <strong>Service latency:</strong> To benchmark <SmallCap>CHIMP</SmallCap>&#x2019;s performance we collected the boot and app install times for the synthetic users using tmpfs. To summarize, on average, when a user starts a new session he faces a waiting time of about 36 seconds: 21 seconds to boot (at least 12s less than from disk) and on average 15 seconds to install and launch apps. While the boot time is stable, installing an app varies with the app size, taking between 10 seconds for small apps (a few hundred KB) to 35 second for large apps (hundreds of MB).</p>    <p>     <strong>User experience:</strong> Here, we see how &#x201C;fast&#x201D; app interaction felt to users and if real users were able to trigger any crashes that the synthetic users did not detect. Next, we also investigate how many users had previously used each app, how fun they thought each app was, and whether or not they saw ads within the app.</p>    <p>Figure&#x00A0;<a class="fig" href="#fig4">4</a> plots the Cumulative Distribution Function (CDF) of the mean question scores reported by users, per app, for the 1,000 Google Play Store apps tested. Binary questions (i.e., yes/no) are on the left while questions scored on a scale of 1 to 3 are on the right. From the left plot, we can see that most of the apps were new to our users (90% of apps tested had a mean &#x201C;tried before&#x201D; score less than 0.5), that most users reported most apps as <em>not</em> having ads/requiring a login. While we see no crashes reported for most of the apps, about 15% of apps have a majority of users claiming crashes. From the plot on the right, we see that users found the apps a bit boring with about 50% receiving a mean score less than 2.0. Regardless, we see that users felt that <SmallCap>CHIMP</SmallCap> was providing at least a &#x201C;decent&#x201D; app experience: over 60% of apps receive at least a 2.0 mean score with respect to speed. Unfortunately, we could not reach a conclusive correlation between system metrics and user&#x0027;s reported app experience. This is a complicated subject since multiple factors need to be considered, e.g.., user connectivity, apps characteristics (e.g., unresponsive apps due to bad design, resource consumption, and/or CDN content policy retrieval), etc. We leave a more careful characterization for future work.</p>    <p>     <strong>Limitations:</strong> While <SmallCap>CHIMP</SmallCap> does support many of the top apps on the Google Play Store, despite its flexible design, there are some limitations. The most obvious is due to the lack of a physical mobile device which limits testing with respect to sensors, mobility, debugging for specific device types or multi-gesture touches. Additionally, its virtualization targets Android specifically, and does not support iOS or Windows Mobile. Next, there are some apps for which <SmallCap>CHIMP</SmallCap> is a sub-par platform. For example, certain types of apps (e.g., background services or boring apps) might not stimulate users enough for them to provide meaningful interactions. Second, there are many apps whose usage requires an account or only becomes useful with some sort of network effect (e.g., Facebook). Although we could have created accounts that were automatically included in AVPs, not only it would require manual work, but it might also violate service provider terms of services, while not necessarily representing reality. That said, we expressly instruct users to never enter personal information into apps.</p>    <p>Finally, there are some types of experimentation that <SmallCap>CHIMP</SmallCap> is just not well suited for. For example, understanding mobile network conditions is better left to tools like Mobilyzer&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0031">31</a>]. Additionally, certain experiments may be more longitudinal in nature, while <SmallCap>CHIMP</SmallCap>&#x2019;s AVPs are bound to a session that will eventually time out.</p>   </section>   <section id="sec-10">    <header>     <div class="title-info">     <h2>      <span class="section-number">5</span> Campaign Calibration</h2>     </div>    </header>    <p>Apart from the engineering challenge to build <SmallCap>CHIMP</SmallCap>, a more fundamental challenge is dealing with real people. To collect meaningful data, we need to engineer campaigns people will be happy to take, e.g., select the right number of apps per session. We also need to validate input to avoid random clickers or distracted users.</p>    <p>In this section, we use <SmallCap>CHIMP</SmallCap> to get insights on how campaigns should be structured. Accordingly, we run a &#x201C;calibration&#x201D; campaign (Table&#x00A0;<a class="tbl" href="#tab1">1</a>) with sessions containing 7 steps, i.e., 6 apps plus a &#x201C;control&#x201D; one (see &#x00A7;<a class="sec" href="#sec-12">5.2</a> for details), presented in random order. The apps we test are: 1)&#x00A0;adobe sketchbook, an app to draw and paint images, 2)&#x00A0;divideandconquer (an open source game), 3)&#x00A0;frozenbubble (an open source game), 4)&#x00A0;pou (a popular, closed source game), 5)&#x00A0;youtube, the most popular app to watch videos, and 6) buzzfeed, a news aggregator app. These apps were chosen relatively arbitrarily with the goal of having a fairly diverse and popular set of apps.</p>    <p>The calibration campaign consists of 100 CrowdFlower users. We request only users that are &#x201C;historically trustworthy&#x201D; which comes at the cost of longer recruitment time (3 hours at a cost of <font style="normal">&#x0024;</font>12). Users exhibit roughly a 75/25% male/female gender split, and they are located in 30&#x00A0;countries (Venezuela being the most popular). We do not enforce a minimum number of steps that users should take, leaving them free to skip steps or even finish without having tested all 7 apps. Similarly, we do not impose any minimum time a user should spend on a given step, the goal being to estimate how much work each user is willing to do &#x201C;naturally.&#x201D;</p>    <p>In the remainder of this section, we first investigate user behavior while interacting with <SmallCap>CHIMP</SmallCap>&#x2019;s website, with the goal of identifying guidelines for future measurement campaigns (&#x00A7;<a class="sec" href="#sec-11">5.1</a>). Then, we compare techniques for discarding unreliable responses (&#x00A7;<a class="sec" href="#sec-12">5.2</a>).</p>    <section id="sec-11">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.1</span> User Behavior</h3>     </div>     </header>     <p>We start by investigating how users navigate through a session. Although not shown due to space limitations, 80% of the users interact with all 7 apps and only 7% of the users interact with a single app. Regardless of their progress in a session, 100% of the users click the <tt>&#x003C;Finish&#x003E;</tt> button and redeem their completion code (required for payment). It is worth reporting that we reached these (high) utilization numbers through various iterations on the website&#x0027;s design. Specifically, the addition of a progress bar (Figure&#x00A0;<a class="fig" href="#fig2">2</a>) generated an impressive 40% improvement.</p>     <p>Next, we investigate how much time users spend per app (Figure&#x00A0;<a class="fig" href="#fig5">5</a>). Note that we subtract any time the user spent interacting with a other browser tabs. Users tend to spend a decreasing amount of time on subsequent apps, e.g., the median decreases from 105s (first app) down to 30s (last 3 apps). The third app is not an exception to this trend but rather an artifact of our methodology. While regular apps are placed randomly, we opted to show a control app (&#x00A7;<a class="sec" href="#sec-12">5.2</a>) in the middle of the session to increase the chance of users testing it. The control app is very simple, it is thus realistic that users spend little time on it.</p>     <p>Finally, we quantify how much users interact with apps via mouse movements and clicks. Figure&#x00A0;<a class="fig" href="#fig6">6</a> shows the CDF of the number of mouse clicks/movements per user restricted to the virtual phone area. Overall, the figure shows about 10% of &#x201C;inactive&#x201D; users, i.e., users with neither clicks nor movements. These users, as well as users with very few clicks, quickly navigate through <SmallCap>CHIMP</SmallCap> to redeem a payment, e.g., none of them test the 7 apps.</p>     <p>Based on the analysis above, we choose to structure our future campaigns with four apps (three plus control). Our rationale is twofold: 1)&#x00A0;maximize the number of users who will complete a full session and 2)&#x00A0;increase the chance the user will spend meaningful time on an app. Apps are presented in random ordered to make sure they get comparable amount of user time.</p>    </section>    <section id="sec-12">     <header>     <div class="title-info">      <h3>       <span class="section-number">5.2</span> Crowdsourcing and Response Validation</h3>     </div>     </header>     <p>No standardized methodology exists to determine the quality of a crowdsourced user. <SmallCap>CHIMP</SmallCap> leverages CrowdFlower&#x0027;s help to select &#x201C;historically trustworthy&#x201D; users. We also draw inspiration from the validation methodology in&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0046">46</a>] as it dealt with similar issues (&#x00A7;<a class="sec" href="#sec-3">2</a>).</p>     <p>Specifically, <SmallCap>CHIMP</SmallCap> leverages &#x201C;engagement&#x201D; as an indication of user quality. We define engagement by the amount of mouse clicks and movements detected in the virtual phone area. To avoid setting arbitrary thresholds, we discard users who never interact with the phone area, i.e., about 10% of the users (Figure&#x00A0;<a class="fig" href="#fig6">6</a>). Clearly, users with a single click should also be discarded but we leverage additional filtering techniques to better capture such users.</p>     <p>We also use <em>control questions</em>, i.e., questions to which the answer is known a priori&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0021">21</a>], to identify low quality users. We developed an Android app that simply asks participants to press three numbered buttons in either ascending or descending order. Failure to produce any of the requested order, or to even press a button, is considered as a sign of a low quality user. We implemented the control as an Android app, rather than a simple question within the interstitial forms, since it is a bit more realistic. In fact, users need to interact with it in the same way as they do with other apps and they might face a similar frustration due to app loading time.</p>     <p>Only 3% of the users <em>fail</em> the control, which indicates it was well designed; however, 22% of users <em>skip</em> the control app. Users failing the control are labeled as low quality and discarded (we still pay these users). Users that skipped it are given a second chance for redemption: they are not discarded if they show high level of engagement with other apps. Specifically, we require such users to have at least 100 clicks across the entire session. This filtering rule introduces an additional 11% of users that are labeled as low quality, and fully captures low engagement users discussed above. The other campaigns in this paper exhibited the same rate of attrition.</p>     <p>To summarize, we use a &#x201C;control&#x201D; Android app of our own design to identify low quality users within a <SmallCap>CHIMP</SmallCap>&#x2019;s campaign. We also use engagement, i.e., a minimum of 1 mouse click in the virtual phone area, to spot users that quickly navigate through <SmallCap>CHIMP</SmallCap> to just redeem their payment. Finally, we combine the two rules for users that skipped the control: we require at least 100 mouse clicks not to discard the user. <figure id="fig7">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig7.jpg" class="img-responsive" alt="Figure 7"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 7:</span>       <span class="figure-title">CDF of method coverage achieved by humans, monkeys, and <SmallCap>CHIMP</SmallCap>&#x2019;s combination of both.</span>      </div>     </figure>     </p>    </section>   </section>   <section id="sec-13">    <header>     <div class="title-info">     <h2>      <span class="section-number">6</span> App Runtime Analysis</h2>     </div>    </header>    <p>We now show how <SmallCap>CHIMP</SmallCap>&#x2019;s advanced runtime analysis can be used to characterize app behavior using UI interactions. More specifically, we use this module to calculate the amount of an app&#x0027;s code triggered by both human and monkey interaction. While we expect humans to perform better for usability tests with specific tasks (e.g., buying a product in a shopping app) and for bypassing known monkey limitations (e.g., login screens, forms, and timing events in interactive apps), we expect monkeys to perform better in exploring overall app code, mostly due to their higher input frequency.</p>    <p>Code coverage is a widely used metric in UI automation literature&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>] to compare different exploration approaches. Unfortunately, research literature tends to focus on opensource applications, while <SmallCap>CHIMP</SmallCap> must also operate with closed-source apps, often with orders of magnitude more complexity. This limitation is mostly due to the ease of analyzing open-source apps&#x2019; code and the existence of coverage instrumentation tools such as <tt>EMMA</tt>&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0016">16</a>]. Standard build tools (which require source code) can be used to run unit tests and calculate coverage using <tt>EMMA</tt>. Unfortunately, to get the coverage of a third party app or to use <tt>EMMA</tt> with the monkey, modifications to the app&#x0027;s code are required. Nevertheless, we analyzed 59 apps used in Choudhary et al.&#x2019;s&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] benchmark, which in turn are taken from previous literature&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0002">2</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0004">4</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0011">11</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0027">27</a>], and tested them with real users (<em>Code Coverage</em> in Table&#x00A0;<a class="tbl" href="#tab1">1</a>). We used a similar experiment methodology as&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] but with ten, 6 minute ([<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0012">12</a>] claims no significant coverage improvement after 5 minutes) monkey runs for each app; humans interacted with the app for 84s on average. We found that humans improved coverage over the monkey for around 40% of apps, while the combination of human and monkey interactions improved coverage for over 60% of apps. Unfortunately, upon closer inspection we noticed that these apps are very simple (a median of 44.5 classes in their main packages) with very limited interactivity (a median of only 4 Activities<a class="fn" href="#fn4" id="foot-fn4"><sup>4</sup></a>), and at least 2 apps were completely non-interactive. <figure id="fig8">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig8.jpg" class="img-responsive" alt="Figure 8"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 8:</span>      <span class="figure-title">Jaccard similarity indexes between human and monkey runs, per category.</span>     </div>     </figure>    </p>    <p>To address these limitations, <SmallCap>CHIMP</SmallCap> uses a different approach. In Android, each app runs on a dedicated VM that opens a debugger port using Java&#x0027;s Debug Wire Protocol (JWDP) managed by the Dalvik Debug Monitor (DDM). As long as the device is set as debuggable (<tt>ro.debuggable</tt> property), it is possible to activate <em>method tracing</em> capabilities. To this end, we implemented our own tool to open a connection directly to the VM&#x0027;s debugger (using an Android platform library called ddmlib&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0001">1</a>]) and collect traces on a given app. After collecting the app traces, we parse the output of <tt>dmtracedump</tt>, which generates call-stack diagrams from traces, to retrieve the runtime method invocations of the app. Additionally, we also decompile the original app, retrieving its method signatures, including argument and return types (to address method overloading). Matching the class and method signatures of the traces to those extracted from the app binary allows us to calculate the class and method coverage of the run. We note that while this method allows us to calculate coverage of apps without source code access, it does have shortcomings, i.e., it cannot provide code coverage based on lines of code, and thus coverage numbers might not perfectly map to those produced by <tt>EMMA</tt> (e.g., lines of code per method tend to follow a skewed distribution).</p>    <p>To test our tracing mechanisms with popular apps, we collected the top 100 most downloaded apps from Google&#x0027;s Play store. From these, we filtered those requiring login or unavailable hardware (e.g., apps using the camera or the device speakers), ending up with 55 apps (<em>Trace Coverage</em> in Table&#x00A0;<a class="tbl" href="#tab1">1</a>). We note that humans could be given login accounts, but we decided to perform a fair comparison with the monkeys. These apps have a median of 11,532 classes, 73,958 methods, and 39 activities (up to 256 activities); more than one order of magnitude higher than the open-source app set.</p>    <p>Contrary to our initial expectations, humans performed well in regards to code coverage compared to monkeys (Figure&#x00A0;<a class="fig" href="#fig7">7</a>). Humans&#x2019; median coverage outperformed monkeys for 63.6% of app categories, and by combining both humans and monkeys, we can improve coverage by up to 25%. We also observe that individual humans and monkeys covered code tend to be quite different, with a similarity lower than 0.5 for 59.1% of the categories (Figure&#x00A0;<a class="fig" href="#fig8">8</a>).</p>    <p>The main take-away is two fold. First, <SmallCap>CHIMP</SmallCap> can effectively integrate and combine the benefits of different UI interaction tools with competing code exploration techniques. Second, <SmallCap>CHIMP</SmallCap>&#x2019;s tracing can be useful to benchmark different UI automation techniques with the benefit of being able to test real, unmodified market applications. Furthermore, its tracing capabilities provide us with a better understanding of app behavior and its inclusion motivates the use of <SmallCap>CHIMP</SmallCap> for other purposes, e.g., extending recent malware detection tools&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0029">29</a>] to use runtime analysis instead of performing static analysis only. <SmallCap>CHIMP</SmallCap>&#x2019;s traces could also be used to train better UI automation tools that interact more like real users.</p>   </section>   <section id="sec-14">    <header>     <div class="title-info">     <h2>      <span class="section-number">7</span> App Classification</h2>     </div>    </header>    <p>In this section, we showcase how to take advantage of the network data collected by <SmallCap>CHIMP</SmallCap> (pcap files and ground truth collected by polling <tt>/proc/net</tt>) to create a per-app traffic classifier. Many solutions in the literature propose using HTTP meta-data like hostname and user-agent&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0010">10</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0043">43</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0049">49</a>], but such approaches are stymied by the increasing adoption of HTTPS. Instead, inspired by recent work&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0023">23</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0033">33</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"     href="#BibPLXBIB0051">51</a>], we use <SmallCap>CHIMP</SmallCap> to create &#x201C;app signatures&#x201D;, i.e., derive a set of network transport level features (packet sizes, handshake characteristics, hostnames, etc.) that uniquely identify each app. We first investigate apps based on their network traffic (&#x00A7;<a class="sec" href="#sec-15">7.1</a>). We then use this characterization to compare real users with respect to monkeys. Next, we use the knowledge gained to build a traffic classifier and evaluate its effectiveness (&#x00A7;<a class="sec" href="#sec-16">7.2</a>). <figure id="fig9">     <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig9.jpg" class="img-responsive" alt="Figure 9"       longdesc=""/>     <div class="figure-caption">      <span class="figure-number">Figure 9:</span>      <span class="figure-title">Comparing per-app number of flows in &#x201C;traffic classification&#x201D; campaign.</span>     </div>     </figure>    </p>    <section id="sec-15">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.1</span> Traffic Characterization</h3>     </div>     </header>     <p>The used dataset was collected from a campaign with 75 apps, 4 steps per session presented in random order, and 500 users who spent about 100s per app (&#x201C;Traffic Classification&#x201D; in Table&#x00A0;<a class="tbl" href="#tab1">1</a>). Pcap files are processed using Tstat &#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0015">15</a>], an open source passive traffic flow analyzer. Tstat generates per-flow logs, i.e., it rebuilds TCP connections based on the exchanged packets, and for each connection reports basic information, e.g., (srcIP, srcPort, dstIP, dstPort) tuples, time of creation, duration, general stats (total bytes/pkts, RTT, TCP handshake duration, TLS handshake duration, etc.), and metadata (hostname for HTTP requests, TLS Server Name Indication, etc.)</p>     <p>Figure&#x00A0;<a class="fig" href="#fig9">9</a>&#x00A0;(left) shows the distribution of the number of flows per app as boxplot. To make things a bit more readable, we put apps into one of four groups based on their category from the Google Play Store: games (10 apps), social and video players (21), news and education (21), and lifestyle/shopping/sports (24). We first note how each group has a median of about 300 flows. That said, while the distribution of three of the groups is heavy tailed, apps in the games group tend to have less traffic.</p>     <div class="table-responsive" id="tab2">     <div class="table-caption">      <span class="table-number">Table 2:</span>      <span class="table-title">Top-10 apps with respect to number of flows, and prediction accuracy of a Random Forest model.</span>     </div>     <table class="table">      <thead>       <tr>        <th style="text-align:left;">        <strong>Appname</strong>        </th>        <th style="text-align:center;">        <strong>Categ.</strong>        </th>        <th style="text-align:center;">        <strong>Downl.&#x00A0;(M)</strong>        </th>        <th colspan="2" style="text-align:center;">        <strong>Train&#x00A0;(%)</strong>        <hr/>        </th>        <th style="text-align:center;">        <strong>Prec.</strong>        </th>        <th style="text-align:center;">        <strong>Recall</strong>        </th>       </tr>      </thead>      <tbody>       <tr>        <td style="text-align:left;">com.bambuna.podcastaddict</td>        <td style="text-align:center;">news</td>        <td style="text-align:center;">5-10</td>        <td style="text-align:right;">1,184</td>        <td style="text-align:right;">(15.6)</td>        <td style="text-align:center;">95.0</td>        <td style="text-align:center;">93.9</td>       </tr>       <tr>        <td style="text-align:left;">com.quvideo.xiaoying</td>        <td style="text-align:center;">video</td>        <td style="text-align:center;">100-500</td>        <td style="text-align:right;">972</td>        <td style="text-align:right;">(12.8)</td>        <td style="text-align:center;">82.4</td>        <td style="text-align:center;">87.2</td>       </tr>       <tr>        <td style="text-align:left;">com.zeptolab.ctr.ads</td>        <td style="text-align:center;">game</td>        <td style="text-align:center;">100-500</td>        <td style="text-align:right;">844</td>        <td style="text-align:right;">(11.1)</td>        <td style="text-align:center;">88.5</td>        <td style="text-align:center;">87.7</td>       </tr>       <tr>        <td style="text-align:left;">it.pinenuts.rassegnastampa</td>        <td style="text-align:center;">news</td>        <td style="text-align:center;">1-5</td>        <td style="text-align:right;">799</td>        <td style="text-align:right;">(10.5)</td>        <td style="text-align:center;">78.5</td>        <td style="text-align:center;">82.9</td>       </tr>       <tr>        <td style="text-align:left;">com.mobilonia.appdater</td>        <td style="text-align:center;">news</td>        <td style="text-align:center;">1-5</td>        <td style="text-align:right;">748</td>        <td style="text-align:right;">(9.8)</td>        <td style="text-align:center;">79.3</td>        <td style="text-align:center;">73.4</td>       </tr>       <tr>        <td style="text-align:left;">com.miniinthebox.android</td>        <td style="text-align:center;">lifestyle</td>        <td style="text-align:center;">1-5</td>        <td style="text-align:right;">690</td>        <td style="text-align:right;">(9.1)</td>        <td style="text-align:center;">97.9</td>        <td style="text-align:center;">95.8</td>       </tr>       <tr>        <td style="text-align:left;">com.Love.Collage.Photo.Frames</td>        <td style="text-align:center;">lifestyle</td>        <td style="text-align:center;">5-10</td>        <td style="text-align:right;">668</td>        <td style="text-align:right;">(8.8)</td>        <td style="text-align:center;">62.6</td>        <td style="text-align:center;">69.3</td>       </tr>       <tr>        <td style="text-align:left;">com.eisterhues_media_2</td>        <td style="text-align:center;">sports</td>        <td style="text-align:center;">1-5</td>        <td style="text-align:right;">648</td>        <td style="text-align:right;">(8.5)</td>        <td style="text-align:center;">87.6</td>        <td style="text-align:center;">81.3</td>       </tr>       <tr>        <td style="text-align:left;">com.topps.kick</td>        <td style="text-align:center;">sports</td>        <td style="text-align:center;">1-5</td>        <td style="text-align:right;">563</td>        <td style="text-align:right;">(7.4)</td>        <td style="text-align:center;">96.6</td>        <td style="text-align:center;">96.8</td>       </tr>       <tr>        <td style="text-align:left;">com.mcdonalds.android</td>        <td style="text-align:center;">lifestyle</td>        <td style="text-align:center;">1-5</td>        <td style="text-align:right;">447</td>        <td style="text-align:right;">(5.9)</td>        <td style="text-align:center;">92.5</td>        <td style="text-align:center;">88.9</td>       </tr>      </tbody>     </table>     </div>     <p>As expected&#x00A0;[<a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0018">18</a>, <a class="bib" data-trigger="hover" data-toggle="popover" data-placement="top"      href="#BibPLXBIB0037">37</a>], we find traffic to be predominantly HTTPS. Most app groups have over 70% of their traffic encrypted, with news and education apps having the least (&#x223C; 60%). We additionally find that SNI is not used in &#x223C; 28% of the flows. SNI is a TLS extension that lets the client indicate, during the TLS handshake, the server&#x0027;s hostname it needs to talk to, and is commonly used by deep packet inspection solutions. This indicates that <em>SNI-based traffic classification has limited applicability to mobile traffic</em>.</p>     <p>Next, have monkeys run the same 75 apps. The monkeys perform 10 runs of 6&#x00A0;min each for each app, i.e., we intentionally set up monkeys to run longer than the aggregated workload done by <SmallCap>CHIMP</SmallCap> users. Figure&#x00A0;<a class="fig" href="#fig9">9</a>&#x00A0;(right) summarizes the network flow analysis from monkey generated traffic. Despite the favorable set up, monkeys produce only 30% of the traffic volume of real users. In particular, we note how the games group has the least number of flows. Indeed, the random nature of monkey input is unsuitable for mimicking human behavior for this category of apps.</p>    </section>    <section id="sec-16">     <header>     <div class="title-info">      <h3>       <span class="section-number">7.2</span> App Classifier</h3>     </div>     </header>     <p>     <strong>Dataset Preparation:</strong> Our goal is to leverage per-flow and -packet features to train a model for each app. An accurate model cannot be built for apps with little traffic. When ranking apps by their number of flows, the top-10, top-20, and top-30 apps represent 31%, 51%, and 66% of the overall traffic, respectively, and over 400 flows. We thus limit experimentation to the top-30 apps since they capture the majority of traffic in our dataset.</p>     <p>The <em>number of features</em> for the model is another important parameter. In our case, we use the per-flow stats provided by Tstat, but enabled reporting of per-packet level information. We thus need to decide how many packets should be used per-flow. We find that &#x223C; 80% of flows carry less than 10 packets (detailed analysis omitted due to space limitation) which we adopt as a threshold when extracting features. By coupling per-packet features with those provided by Tstat, we end up with 127 features for each flow. Note this seemingly large number of features is because metrics are reported separately for each direction. E.g., we extract the size of the first 10 <em>outgoing</em> packets and 10 <em>incoming</em> packets separately. Finally, each flow in the Tstat logs is mapped to the ground truth provided by the polling of <tt>/proc/net</tt>. <figure id="fig10">      <img src="../../../data/deliveryimages.acm.org/10.1145/3190000/3186035/images/www2018-44-fig10.jpg" class="img-responsive" alt="Figure 10"       longdesc=""/>      <div class="figure-caption">       <span class="figure-number">Figure 10:</span>       <span class="figure-title">Average per-app f1 score when increasing the number of target classes.</span>      </div>     </figure>      <strong>Algorithm Tuning:</strong> We use the Random Forest algorithm, which combines multiple Decision Trees to mitigate over fitting. We configured the algorithm to use 50 trees (we do not observe improvements in prediction quality with larger values). We consider a 30% split, i.e., 70% of the samples for each app are used for training and the rest for testing We leave the training set unbalanced, while we balance the test set to make use of the standard prediction metrics <em>Precision</em>, <em>Recall</em>, and <em>f1 score</em> (also known as f-measure). Finally, for each scenario below, we take 10 random 30/70 splits and average the prediction indexes across the 10 tests.</p>     <p>     <strong>Modeling:</strong> Table&#x00A0;<a class="tbl" href="#tab2">2</a> lists the top-10 apps in our dataset and the accuracy when classifying them. These apps are popular (most of them have over 1M downloads), and span several different categories. The model achieves Precision and Recall above 70% for most apps.</p>     <p>Further stressing the classifier, Figure&#x00A0;<a class="fig" href="#fig10">10</a> compares accuracy when classifying 10, 20, and 30 apps, respectively. While we plot only the f1 score for readability, results for Precision and Recall are similar. As expected, the accuracy decreases when increasing the number of apps, but the drop is minimal. On average, it moves from 87% when classifying the top-10 to 78% when classifying the top-30. Given the unbalanced nature of our dataset, increasing the number of apps reduces the size of the testing set to keep it balanced across apps, but results indicate this effect does not strongly impact overall accuracy, i.e., the model indeed &#x201C;fingerprints&#x201D; each app.</p>    </section>   </section>   <section id="sec-17">    <header>     <div class="title-info">     <h2>      <span class="section-number">8</span> Conclusion</h2>     </div>    </header>    <p>In this paper we presented <SmallCap>CHIMP</SmallCap>, a system that enables large scale, <em>human</em> testing of mobile apps. We described in detail the virtual phone environment via which users can interact with Android from their browser, as well as the experimentation platform and data collection modules that make <SmallCap>CHIMP</SmallCap> a complete system for large-scale app testing with real humans and UI automation tools. Although <SmallCap>CHIMP</SmallCap> has some limitations with respect to input (e.g., multi-touch gestures) and hardware (e.g., sensors), an interesting research direction would be to perform a comprehensive study of how user interactions differ when using real mobile devices.</p>    <p>     <SmallCap>CHIMP</SmallCap> achieves its scale in part by integrating with paid crowdworker services like CrowdFlower. After evaluating <SmallCap>CHIMP</SmallCap>, we performed a system calibration that resulted in guidelines for designing and executing <SmallCap>CHIMP</SmallCap> experiments. Next, we used <SmallCap>CHIMP</SmallCap>&#x2019;s advanced runtime tracing mechanisms to compare humans to the &#x201C;monkey&#x201D; UI automation tool, both in terms of code coverage and similarity. We have found that <SmallCap>CHIMP</SmallCap> successfully leverages the wisdom of the crowd. Its users outperformed the monkey for over 60% of the tested app categories, while <SmallCap>CHIMP</SmallCap>&#x2019;s combined coverage (monkey and human) improved for the majority of apps, with coverage increasing by up to 25%. Finally, we used <SmallCap>CHIMP</SmallCap> to capture network traffic generated by apps with the goal of building a traffic classifier. We found that while monkey inputs were insufficient for generating usable traffic (up to 3 times less traffic volume), a random forest classifier built using <SmallCap>CHIMP</SmallCap> generated data could reach f1 scores of above 0.9. Overall, <SmallCap>CHIMP</SmallCap> shows both the <em>feasibility</em> and <em>applicability</em> of keeping humans in the app testing loop.</p>   </section>  </section>  <section class="back-matter">   <section id="ref-001">    <header>     <div class="title-info">     <h2 class="page-brake-head">REFERENCES</h2>     </div>    </header>    <ul class="bibUl">     <li id="BibPLXBIB0001" label="[1]">2017. DDMlib: APIs for talking with Dalvik VM. (2017). <a class="link-inline force-break"      href="https://mvnrepository.com/artifact/com.android.ddmlib/ddmlib">https://mvnrepository.com/artifact/com.android.ddmlib/ddmlib</a>.</li>     <li id="BibPLXBIB0002" label="[2]">Domenico Amalfitano, Anna&#x00A0;Rita Fasolino, Porfirio Tramontana, Salvatore De&#x00A0;Carmine, and Atif&#x00A0;M. Memon. 2012. Using GUI Ripping for Automated Testing of Android Applications. In <em>      <em>Proceedings of the 27th IEEE/ACM International Conference on Automated Software Engineering</em>     </em>(<em>ASE 2012</em>). ACM, New York, NY, USA, 258&#x2013;261. <a class="link-inline force-break" href="https://doi.org/10.1145/2351676.2351717"      target="_blank">https://doi.org/10.1145/2351676.2351717</a></li>     <li id="BibPLXBIB0003" label="[3]">Amazon. 2016. Live App Testing. (2016). <a class="link-inline force-break"      href="https://developer.amazon.com/live-app-testing">https://developer.amazon.com/live-app-testing</a>.</li>     <li id="BibPLXBIB0004" label="[4]">Saswat Anand, Mayur Naik, Mary&#x00A0;Jean Harrold, and Hongseok Yang. 2012. Automated Concolic Testing of Smartphone Apps. In <em>      <em>Proceedings of the ACM SIGSOFT 20th International Symposium on the Foundations of Software Engineering</em>     </em>(<em>FSE &#x2019;12</em>). ACM, New York, NY, USA, 59:1&#x2013;59:11. <a class="link-inline force-break" href="https://doi.org/10.1145/2393596.2393666"      target="_blank">https://doi.org/10.1145/2393596.2393666</a></li>     <li id="BibPLXBIB0005" label="[5]">Android. 2017. UI/Application Exerciser Monkey. (2017). <a class="link-inline force-break"      href="https://developer.android.com/studio/test/monkey.html">https://developer.android.com/studio/test/monkey.html</a>.</li>     <li id="BibPLXBIB0006" label="[6]">Android-x86. 2017. Android-x86 Open Source Project Announcement. (2017). <a class="link-inline force-break" href="http://www.android-x86.org/">http://www.android-x86.org/</a>.</li>     <li id="BibPLXBIB0007" label="[7]">Android-x86. 2017. Houdini Source Tree. (2017). <a class="link-inline force-break"      href="https://sourceforge.net/p/android-x86/vendor_intel_houdini/ci/kitkat-x86/tree/">https://sourceforge.net/p/android-x86/vendor_intel_houdini/ci/kitkat-x86/tree/</a>.</li>     <li id="BibPLXBIB0008" label="[8]">Appetize.io. 2017. Appetize.io. (2017). <a class="link-inline force-break" href="https://appetize.io/">https://appetize.io/</a>.</li>     <li id="BibPLXBIB0009" label="[9]">Tanzirul Azim and Iulian Neamtiu. 2013. Targeted and Depth-first Exploration for Systematic Testing of Android Apps. In <em>      <em>Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &#x0026; Applications</em>     </em>(<em>OOPSLA &#x2019;13</em>). ACM, New York, NY, USA, 641&#x2013;660. <a class="link-inline force-break" href="https://doi.org/10.1145/2509136.2509549"      target="_blank">https://doi.org/10.1145/2509136.2509549</a></li>     <li id="BibPLXBIB0010" label="[10]">Pedro Casas, Pierdomenico Fiadino, and Arian B&#x00E4;r. 2014. Understanding HTTP Traffic and CDN Behavior from the Eyes of a Mobile ISP. In <em>      <em>Proc. Passive and Active Measurement (PAM)</em>     </em>.</li>     <li id="BibPLXBIB0011" label="[11]">Wontae Choi, George Necula, and Koushik Sen. 2013. Guided GUI Testing of Android Apps with Minimal Restart and Approximate Learning. In <em>      <em>Proceedings of the 2013 ACM SIGPLAN International Conference on Object Oriented Programming Systems Languages &#x0026; Applications</em>     </em>(<em>OOPSLA &#x2019;13</em>). ACM, New York, NY, USA, 623&#x2013;640. <a class="link-inline force-break" href="https://doi.org/10.1145/2509136.2509552"      target="_blank">https://doi.org/10.1145/2509136.2509552</a></li>     <li id="BibPLXBIB0012" label="[12]">S.&#x00A0;R. Choudhary, A. Gorla, and A. Orso. 2015. Automated Test Input Generation for Android: Are We There Yet? (E). In <em>      <em>Automated Software Engineering (ASE), 2015 30th IEEE/ACM International Conference on</em>     </em>. 429&#x2013;440. <a class="link-inline force-break" href="https://doi.org/10.1109/ASE.2015.89"      target="_blank">https://doi.org/10.1109/ASE.2015.89</a></li>     <li id="BibPLXBIB0013" label="[13]">Collectd. 2017. The system statistics collection daemon. (2017). <a class="link-inline force-break" href="https://collectd.org/">https://collectd.org/</a>.</li>     <li id="BibPLXBIB0014" label="[14]">Crowdflower. 2017. Crowdsourcing platform. (2017). <a class="link-inline force-break" href="https://www.crowdflower.com/">https://www.crowdflower.com/</a>.</li>     <li id="BibPLXBIB0015" label="[15]">Telecommunication Networks Group&#x00A0;Politecnico di Torino. 2017. Tstat website. (2017). <a class="link-inline force-break" href="http://tstat.polito.it">http://tstat.polito.it</a>.</li>     <li id="BibPLXBIB0016" label="[16]">EMMA. 2017. EMMA: a free Java code coverage tool. (2017). <a class="link-inline force-break" href="http://emma.sourceforge.net/">http://emma.sourceforge.net/</a>.</li>     <li id="BibPLXBIB0017" label="[17]">Google. 2017. Firebase Test Lab for Android. (2017). <a class="link-inline force-break"      href="https://firebase.google.com/docs/test-lab/">https://firebase.google.com/docs/test-lab/</a>.</li>     <li id="BibPLXBIB0018" label="[18]">Google. 2017. HTTPS at Google &#x2013; Google Transparency Report. (2017). <a class="link-inline force-break"      href="https://www.google.com/transparencyreport/https">https://www.google.com/transparencyreport/https</a>.</li>     <li id="BibPLXBIB0019" label="[19]">Grafana. 2017. The open platform for analytics and monitoring. (2017). <a class="link-inline force-break" href="https://grafana.com/">https://grafana.com/</a>.</li>     <li id="BibPLXBIB0020" label="[20]">Shuai Hao, Bin Liu, Suman Nath, William&#x00A0;G.J. Halfond, and Ramesh Govindan. 2014. PUMA: Programmable UI-automation for Large-scale Dynamic Analysis of Mobile Apps. In <em>      <em>Proceedings of the 12th Annual International Conference on Mobile Systems, Applications, and Services</em>     </em>(<em>MobiSys &#x2019;14</em>). ACM, New York, NY, USA, 204&#x2013;217. <a class="link-inline force-break" href="https://doi.org/10.1145/2594368.2594390"      target="_blank">https://doi.org/10.1145/2594368.2594390</a></li>     <li id="BibPLXBIB0021" label="[21]">Tobias Hossfeld, Christian Keimel, Matthias Hirth, Bruno Gardlo, Julian Habigt, Klaus Diepold, and Phuoc Tran-Gia. 2014. Best Practices for QoE Crowdtesting: QoE Assessment With Crowdsourcing. <em>      <em>Trans. Multi.</em>     </em>16, 2 (Feb. 2014).</li>     <li id="BibPLXBIB0022" label="[22]">Gael&#x00A0;H. (Intel). 2013. Performance Results for Android Emulators - with and without Intel HAXM. (2013). <a class="link-inline force-break" href="https://goo.gl/D6rUf2">https://goo.gl/D6rUf2</a>.</li>     <li id="BibPLXBIB0023" label="[23]">Maciej Korczynski and Andrzej Duda. 2014. Markov chain fingerprinting to classify encrypted traffic. In <em>      <em>Proc. IEEE INFOCOM</em>     </em>.</li>     <li id="BibPLXBIB0024" label="[24]">Christophe Leung, Jingjing Ren, David Choffnes, and Christo Wilson. 2016. Should You Use the App for That?: Comparing the Privacy Implications of App- and Web-based Online Services. In <em>      <em>Proceedings of the 2016 ACM on Internet Measurement Conference</em>     </em>(<em>IMC &#x2019;16</em>). ACM, New York, NY, USA, 365&#x2013;372. <a class="link-inline force-break" href="https://doi.org/10.1145/2987443.2987456"      target="_blank">https://doi.org/10.1145/2987443.2987456</a></li>     <li id="BibPLXBIB0025" label="[25]">Linux-KVM. 2017. Qcow2 image format. (2017). <a class="link-inline force-break" href="https://www.linux-kvm.org/page/Qcow2">https://www.linux-kvm.org/page/Qcow2</a>.</li>     <li id="BibPLXBIB0026" label="[26]">ARC Welder Official&#x00A0;Compatibility List. 2017. goo.gl/Q0fy3m. (2017).</li>     <li id="BibPLXBIB0027" label="[27]">Aravind Machiry, Rohan Tahiliani, and Mayur Naik. 2013. Dynodroid: An Input Generation System for Android Apps. In <em>      <em>Proceedings of the 2013 9th Joint Meeting on Foundations of Software Engineering</em>     </em>(<em>ESEC/FSE 2013</em>). ACM, New York, NY, USA, 224&#x2013;234. <a class="link-inline force-break" href="https://doi.org/10.1145/2491411.2491450"      target="_blank">https://doi.org/10.1145/2491411.2491450</a></li>     <li id="BibPLXBIB0028" label="[28]">Riyadh Mahmood, Nariman Mirzaei, and Sam Malek. 2014. EvoDroid: Segmented Evolutionary Testing of Android Apps. In <em>      <em>Proceedings of the 22Nd ACM SIGSOFT International Symposium on Foundations of Software Engineering</em>     </em>(<em>FSE 2014</em>). ACM, New York, NY, USA, 599&#x2013;609. <a class="link-inline force-break" href="https://doi.org/10.1145/2635868.2635896"      target="_blank">https://doi.org/10.1145/2635868.2635896</a></li>     <li id="BibPLXBIB0029" label="[29]">Enrico Mariconti, Lucky Onwuzurike, Panagiotis Andriotis, Emiliano De&#x00A0;Cristofaro, Gordon Ross, and Gianluca Stringhini. 2016. MAMADROID: Detecting Android Malware by Building Markov Chains of Behavioral Models. <em>      <em>arXiv preprint arXiv:1612.04433</em>     </em>(2016).</li>     <li id="BibPLXBIB0030" label="[30]">NGINX. 2017. High Performance Load Balancer, Web Server, &#x0026; Reverse Proxy. (2017). <a class="link-inline force-break" href="https://www.nginx.com/">https://www.nginx.com/</a>.</li>     <li id="BibPLXBIB0031" label="[31]">Ashkan Nikravesh, Hongyi Yao, Shichang Xu, David Choffnes, and Z.&#x00A0;Morley Mao. 2015. Mobilyzer: An Open Platform for Controllable Mobile Network Measurements. In <em>      <em>Proc. ACM MobiSys</em>     </em>. 389&#x2013;404.</li>     <li id="BibPLXBIB0032" label="[32]">Lucky Onwuzurike and Emiliano De&#x00A0;Cristofaro. 2015. Danger is My Middle Name: Experimenting with SSL Vulnerabilities in Android Apps. In <em>      <em>Proceedings of the 8th ACM Conference on Security &#x0026; Privacy in Wireless and Mobile Networks</em>     </em>(<em>WiSec &#x2019;15</em>). ACM, New York, NY, USA, Article 15, 6&#x00A0;pages. <a class="link-inline force-break" href="https://doi.org/10.1145/2766498.2766522"      target="_blank">https://doi.org/10.1145/2766498.2766522</a></li>     <li id="BibPLXBIB0033" label="[33]">Andriy Panchenko, Fabian Lanze, Jan Pennekamp, Thomas Engel, Andreas Zinnen, Martin Henze, and Klaus Wehrle. 2016. Website Fingerprinting at Internet Scale. In <em>      <em>Proc. Network &#x0026; Distributed System Security Symposium (NDSS)</em>     </em>.</li>     <li id="BibPLXBIB0034" label="[34]">Puma. 2017. A Modern, Concurrent Web Server for Ruby. (2017). <a class="link-inline force-break" href="http://puma.io/">http://puma.io/</a>.</li>     <li id="BibPLXBIB0035" label="[35]">Jingjing Ren, Ashwin Rao, Martina Lindorfer, Arnaud Legout, and David Choffnes. 2016. ReCon: Revealing and Controlling PII Leaks in Mobile Network Traffic. In <em>      <em>Proceedings of the 14th Annual International Conference on Mobile Systems, Applications, and Services</em>     </em>(<em>MobiSys &#x2019;16</em>). ACM, New York, NY, USA, 361&#x2013;374. <a class="link-inline force-break" href="https://doi.org/10.1145/2906388.2906392"      target="_blank">https://doi.org/10.1145/2906388.2906392</a></li>     <li id="BibPLXBIB0036" label="[36]">T Richardson and J Levine. 2011. The Remote Framebuffer Protocol. (2011). <a class="link-inline force-break" href="https://tools.ietf.org/html/rfc6143">https://tools.ietf.org/html/rfc6143</a>.</li>     <li id="BibPLXBIB0037" label="[37]">Global Internet&#x00A0;Phenomena Sandvine. 2016. Spotlight: Encrypted Internet Traffic. (2016). <a class="link-inline force-break"      href="https://www.sandvine.com/trends/encryption.html">https://www.sandvine.com/trends/encryption.html</a>.</li>     <li id="BibPLXBIB0038" label="[38]">Raimondas Sasnauskas and John Regehr. 2014. Intent Fuzzer: Crafting Intents of Death. In <em>      <em>Proceedings of the 2014 Joint International Workshop on Dynamic Analysis (WODA) and Software and System Performance Testing, Debugging, and Analytics (PERTEA)</em>     </em>(<em>WODA+PERTEA 2014</em>). ACM, New York, NY, USA, 1&#x2013;5. <a class="link-inline force-break" href="https://doi.org/10.1145/2632168.2632169"      target="_blank">https://doi.org/10.1145/2632168.2632169</a></li>     <li id="BibPLXBIB0039" label="[39]">Selenium. 2017. Web Browser Automation. (2017). <a class="link-inline force-break" href="http://www.seleniumhq.org/">http://www.seleniumhq.org/</a>.</li>     <li id="BibPLXBIB0040" label="[40]">Sidekiq. 2017. Simple, efficient job processing for Ruby. (2017). <a class="link-inline force-break" href="http://sidekiq.org">http://sidekiq.org</a>.</li>     <li id="BibPLXBIB0041" label="[41]">Spice. 2017. Spice. (2017). <a class="link-inline force-break" href="https://www.spice-space.org/">https://www.spice-space.org/</a>.</li>     <li id="BibPLXBIB0042" label="[42]">Spice. 2017. Xspice. (2017). <a class="link-inline force-break" href="https://www.spice-space.org/xspice.html">https://www.spice-space.org/xspice.html</a>.</li>     <li id="BibPLXBIB0043" label="[43]">Alok Tongaonkar, Shuaifu Dai, Antonio Nucci, and Dawn Song. 2013. Understanding Mobile App Usage Patterns Using In-app Advertisements. In <em>      <em>Proc. Passive and Active Measurement (PAM)</em>     </em>.</li>     <li id="BibPLXBIB0044" label="[44]">UserTesting. 2017. User Experience Research Platform. (2017). <a class="link-inline force-break" href="https://www.usertesting.com/">https://www.usertesting.com/</a>.</li>     <li id="BibPLXBIB0045" label="[45]">Heila van&#x00A0;der Merwe, Brink van&#x00A0;der Merwe, and Willem Visser. 2012. Verifying Android Applications Using Java PathFinder. <em>      <em>SIGSOFT Softw. Eng. Notes</em>     </em>37, 6 (Nov. 2012), 1&#x2013;5. <a class="link-inline force-break" href="https://doi.org/10.1145/2382756.2382797"      target="_blank">https://doi.org/10.1145/2382756.2382797</a></li>     <li id="BibPLXBIB0046" label="[46]">Matteo Varvello, Jeremy Blackburn, David Naylor, and Kostantina Papagiannaki. 2016. EYEORG: A Platform For Crowdsourcing Web Quality Of Experience Measurements. In <em>      <em>Proceedings of the 12th International on Conference on emerging Networking EXperiments and Technologies</em>     </em>. ACM, 399&#x2013;412.</li>     <li id="BibPLXBIB0047" label="[47]">Google Chrome&#x00A0;ARC Welder. 2017. <a class="link-inline force-break" href="https://goo.gl/Rohk1E">https://goo.gl/Rohk1E</a>. (2017).</li>     <li id="BibPLXBIB0048" label="[48]">Wei Yang, Mukul&#x00A0;R. Prasad, and Tao Xie. 2013. A Grey-Box Approach for Automated GUI-Model Generation of Mobile Applications. In <em>      <em>Fundamental Approaches to Software Engineering</em>     </em>, Vittorio Cortellessaand D&#x00E1;niel Varr&#x00F3;<em>(Eds.). Number 7793 in Lecture Notes in Computer Science</em>. Springer Berlin Heidelberg, 250&#x2013;265. <a class="link-inline force-break"      href="http://link.springer.com/chapter/10.1007/978-3-642-37057-1_19"      target="_blank">http://link.springer.com/chapter/10.1007/978-3-642-37057-1_19</a>DOI: 10.1007/978-3-642-37057-1_19.</li>     <li id="BibPLXBIB0049" label="[49]">Yao, Hongyi and Ranjan, Gyan and Tongaonkar, Alok and Liao, Yong and Mao, Zhuoqing Morley. 2015. SAMPLES: Self Adaptive Mining of Persistent LExical Snippets for Classifying Mobile Application Traffic. In <em>      <em>Proc. ACM MobiCom</em>     </em>.</li>     <li id="BibPLXBIB0050" label="[50]">Hui Ye, Shaoyin Cheng, Lanbo Zhang, and Fan Jiang. 2013. DroidFuzzer: Fuzzing the Android Apps with Intent-Filter Tag. In <em>      <em>Proceedings of International Conference on Advances in Mobile Computing &#x0026; Multimedia</em>     </em>(<em>MoMM &#x2019;13</em>). ACM, New York, NY, USA, 68:68&#x2013;68:74. <a class="link-inline force-break" href="https://doi.org/10.1145/2536853.2536881"      target="_blank">https://doi.org/10.1145/2536853.2536881</a></li>     <li id="BibPLXBIB0051" label="[51]">Ting-Fang Yen, Yinglian Xie, Fang Yu, Roger&#x00A0;P. Yu, and Martin Abadi. 2012. Host Fingerprinting and Tracking on the Web: Privacy and Security Implications. In <em>      <em>Proc. Network &#x0026; Distributed System Security Symposium (NDSS)</em>     </em>.</li>    </ul>   </section>  </section>  <section id="foot-001" class="footnote">   <header>    <div class="title-info">     <h2>FOOTNOTE</h2>    </div>   </header>   <p id="fn1"><a href="#foot-fn1"><sup>1</sup></a>All data was collected in compliance with our institutional ethics guidelines.</p>   <p id="fn2"><a href="#foot-fn2"><sup>2</sup></a>We do not monitor activity on other browser tabs nor raw keyboard inputs.</p>   <p id="fn3"><a href="#foot-fn3"><sup>3</sup></a>NB: in Android each app is given a different numerical user id (UID).</p>   <p id="fn4"><a href="#foot-fn4"><sup>4</sup></a>Activities are responsible for displaying the interactive windows presented to users.</p>   <div class="bibStrip">    <p>This paper is published under the Creative Commons Attribution 4.0 International (CC-BY&#x00A0;4.0) license. Authors reserve their rights to disseminate the work on their personal and corporate Web sites with the appropriate attribution.</p>    <p>     <em>WWW 2018, April 23&#x2013;27, 2018, Lyon, France</em>    </p>    <p>&#x00A9; 2018; IW3C2 (International World Wide Web Conference Committee), published under Creative Commons CC-BY&#x00A0;4.0 License. ACM ISBN 978-1-4503-5639-8/18/04.<br/>DOI: <a class="link-inline force-break" target="_blank"     href="https://doi.org/10.1145/3178876.3186035">https://doi.org/10.1145/3178876.3186035</a>    </p>   </div>  </section>  <div class="pubHistory">   <p/>  </div>  </body> </html> 
